---
title: Data Manipulation (DML)
date: 2025-10-14
tags: [sql, dml, insert, update, delete, merge, upsert, transactions, data-engineering, foundation-basics]
status: active
learning_phase: "Foundation (Basics)"
---

# Data Manipulation (DML)

## Overview

**DML (Data Manipulation Language)** consists of SQL commands used to manipulate data within existing database tables. These are the core operations data engineers use daily to insert, update, delete, and merge data.

**Core DML Commands:**
- INSERT - Add new rows
- UPDATE - Modify existing rows
- DELETE - Remove rows
- MERGE/UPSERT - Insert or update (depending on existence)

---

## INSERT - Adding Data

### Basic INSERT

```sql
-- Insert a single row
INSERT INTO customers (customer_id, name, email, signup_date)
VALUES (1, 'John Doe', 'john@example.com', '2024-01-15');

-- Insert multiple rows
INSERT INTO customers (customer_id, name, email, signup_date)
VALUES
    (2, 'Jane Smith', 'jane@example.com', '2024-01-16'),
    (3, 'Bob Johnson', 'bob@example.com', '2024-01-17'),
    (4, 'Alice Brown', 'alice@example.com', '2024-01-18');
```

### INSERT with All Columns

```sql
-- If inserting all columns in order, column list is optional
INSERT INTO customers
VALUES (5, 'Charlie Wilson', 'charlie@example.com', '2024-01-19');
```

### INSERT with DEFAULT values

```sql
CREATE TABLE orders (
    order_id INT PRIMARY KEY,
    customer_id INT,
    order_date DATE DEFAULT CURRENT_DATE,
    status VARCHAR(20) DEFAULT 'pending'
);

-- Use DEFAULT keyword or omit column
INSERT INTO orders (order_id, customer_id)
VALUES (1, 100);  -- order_date and status will use defaults

-- Explicitly specify DEFAULT
INSERT INTO orders (order_id, customer_id, order_date, status)
VALUES (2, 101, DEFAULT, DEFAULT);
```

### INSERT from SELECT (ETL Pattern)

```sql
-- Copy data from one table to another
INSERT INTO customers_archive (customer_id, name, email, archived_date)
SELECT customer_id, name, email, CURRENT_DATE
FROM customers
WHERE signup_date < '2020-01-01';

-- Insert aggregated data
INSERT INTO daily_sales_summary (sale_date, total_orders, total_revenue)
SELECT
    DATE(order_timestamp) AS sale_date,
    COUNT(*) AS total_orders,
    SUM(amount) AS total_revenue
FROM orders
WHERE DATE(order_timestamp) = CURRENT_DATE - INTERVAL '1' DAY
GROUP BY DATE(order_timestamp);
```

### INSERT with ON CONFLICT (PostgreSQL/SQLite)

```sql
-- Ignore conflicts
INSERT INTO products (product_id, name, price)
VALUES (1, 'Widget', 19.99)
ON CONFLICT (product_id) DO NOTHING;

-- Update on conflict (UPSERT)
INSERT INTO products (product_id, name, price)
VALUES (1, 'Widget', 24.99)
ON CONFLICT (product_id)
DO UPDATE SET
    name = EXCLUDED.name,
    price = EXCLUDED.price,
    updated_at = CURRENT_TIMESTAMP;
```

### Bulk INSERT Best Practices

```sql
-- ✅ GOOD: Batch inserts (much faster)
INSERT INTO customers (customer_id, name, email)
VALUES
    (1, 'User1', 'user1@example.com'),
    (2, 'User2', 'user2@example.com'),
    (3, 'User3', 'user3@example.com'),
    -- ... thousands of rows ...
    (10000, 'User10000', 'user10000@example.com');

-- ❌ BAD: Individual inserts (very slow)
INSERT INTO customers VALUES (1, 'User1', 'user1@example.com');
INSERT INTO customers VALUES (2, 'User2', 'user2@example.com');
INSERT INTO customers VALUES (3, 'User3', 'user3@example.com');
-- ... 10,000 individual statements
```

**Performance Tips:**
- Batch inserts into single statements (1000-5000 rows per batch)
- Use transactions for multiple batches
- Disable indexes temporarily for very large loads
- Use COPY/LOAD DATA for massive bulk inserts

---

## UPDATE - Modifying Data

### Basic UPDATE

```sql
-- Update a single column
UPDATE customers
SET email = 'newemail@example.com'
WHERE customer_id = 1;

-- Update multiple columns
UPDATE customers
SET
    email = 'updated@example.com',
    phone = '555-1234',
    updated_at = CURRENT_TIMESTAMP
WHERE customer_id = 1;

-- Update all rows (dangerous without WHERE!)
UPDATE products
SET is_active = TRUE;
```

### UPDATE with Expressions

```sql
-- Increment values
UPDATE products
SET stock_quantity = stock_quantity + 100
WHERE product_id = 50;

-- Calculate new values
UPDATE orders
SET
    discount_amount = total_amount * 0.10,
    final_amount = total_amount - (total_amount * 0.10)
WHERE order_date >= '2024-01-01';

-- String concatenation
UPDATE customers
SET full_name = first_name || ' ' || last_name
WHERE full_name IS NULL;
```

### UPDATE with CASE

```sql
-- Conditional updates
UPDATE products
SET price_tier = CASE
    WHEN price < 10 THEN 'budget'
    WHEN price BETWEEN 10 AND 50 THEN 'standard'
    WHEN price > 50 THEN 'premium'
    ELSE 'unknown'
END;

-- Multiple columns with different conditions
UPDATE customers
SET
    segment = CASE
        WHEN total_spent > 10000 THEN 'VIP'
        WHEN total_spent > 1000 THEN 'Premium'
        ELSE 'Standard'
    END,
    discount_rate = CASE
        WHEN total_spent > 10000 THEN 0.20
        WHEN total_spent > 1000 THEN 0.10
        ELSE 0.05
    END
WHERE last_purchase_date >= '2024-01-01';
```

### UPDATE from JOIN/Subquery

```sql
-- Update based on data from another table (PostgreSQL syntax)
UPDATE orders o
SET customer_name = c.name
FROM customers c
WHERE o.customer_id = c.customer_id;

-- Update using subquery
UPDATE products
SET category_name = (
    SELECT category_name
    FROM categories
    WHERE categories.category_id = products.category_id
)
WHERE category_id IS NOT NULL;

-- Update with aggregated values
UPDATE customers c
SET total_orders = (
    SELECT COUNT(*)
    FROM orders o
    WHERE o.customer_id = c.customer_id
);
```

### UPDATE Best Practices

```sql
-- ✅ GOOD: Always use WHERE clause (unless intentional)
UPDATE customers
SET status = 'inactive'
WHERE last_login_date < CURRENT_DATE - INTERVAL '1' YEAR;

-- ⚠️ DANGEROUS: Updates ALL rows
UPDATE customers
SET status = 'inactive';

-- ✅ GOOD: Test with SELECT first
SELECT *
FROM customers
WHERE last_login_date < CURRENT_DATE - INTERVAL '1' YEAR;
-- Verify results, then run UPDATE

-- ✅ GOOD: Limit scope in production
UPDATE customers
SET status = 'inactive'
WHERE last_login_date < CURRENT_DATE - INTERVAL '1' YEAR
  AND status != 'inactive'  -- Only update if needed
LIMIT 1000;  -- Process in batches
```

---

## DELETE - Removing Data

### Basic DELETE

```sql
-- Delete specific rows
DELETE FROM customers
WHERE customer_id = 1;

-- Delete with multiple conditions
DELETE FROM orders
WHERE order_status = 'cancelled'
  AND order_date < '2020-01-01';

-- Delete all rows (dangerous!)
DELETE FROM temporary_data;
```

### DELETE with Subquery

```sql
-- Delete based on data from another table
DELETE FROM orders
WHERE customer_id IN (
    SELECT customer_id
    FROM customers
    WHERE status = 'deleted'
);

-- Delete duplicates (keep oldest)
DELETE FROM customers
WHERE customer_id NOT IN (
    SELECT MIN(customer_id)
    FROM customers
    GROUP BY email
);
```

### DELETE with JOIN

```sql
-- PostgreSQL syntax
DELETE FROM orders o
USING customers c
WHERE o.customer_id = c.customer_id
  AND c.status = 'deleted';

-- MySQL syntax
DELETE o
FROM orders o
JOIN customers c ON o.customer_id = c.customer_id
WHERE c.status = 'deleted';
```

### TRUNCATE vs DELETE

```sql
-- DELETE: Row-by-row removal, can rollback, triggers fire
DELETE FROM staging_table;

-- TRUNCATE: Fast, removes all rows, resets auto-increment
TRUNCATE TABLE staging_table;

-- TRUNCATE with CASCADE (PostgreSQL)
TRUNCATE TABLE parent_table CASCADE;
```

**Comparison:**

| Feature | DELETE | TRUNCATE |
|---------|--------|----------|
| **Speed** | Slower | Much faster |
| **WHERE clause** | Yes | No (all rows) |
| **Rollback** | Yes | No (usually) |
| **Triggers** | Fires | Doesn't fire |
| **Auto-increment** | Preserved | Reset |
| **Logging** | Full | Minimal |

### DELETE Best Practices

```sql
-- ✅ GOOD: Archive before deleting
INSERT INTO orders_archive
SELECT * FROM orders
WHERE order_date < '2020-01-01';

DELETE FROM orders
WHERE order_date < '2020-01-01';

-- ✅ GOOD: Soft delete instead of hard delete
UPDATE customers
SET is_deleted = TRUE, deleted_at = CURRENT_TIMESTAMP
WHERE customer_id = 1;

-- Query only active records
SELECT * FROM customers WHERE is_deleted = FALSE;

-- ✅ GOOD: Delete in batches for large tables
DELETE FROM logs
WHERE log_date < '2023-01-01'
LIMIT 10000;
-- Repeat until no more rows to delete

-- ✅ GOOD: Test with SELECT first
SELECT COUNT(*)
FROM orders
WHERE order_status = 'cancelled' AND order_date < '2020-01-01';
-- Verify count, then run DELETE
```

---

## MERGE / UPSERT - Insert or Update

### MERGE (SQL Standard)

```sql
-- MERGE syntax (SQL Server, Oracle, PostgreSQL 15+)
MERGE INTO target_table AS target
USING source_table AS source
ON target.id = source.id
WHEN MATCHED THEN
    UPDATE SET
        target.name = source.name,
        target.value = source.value,
        target.updated_at = CURRENT_TIMESTAMP
WHEN NOT MATCHED THEN
    INSERT (id, name, value, created_at)
    VALUES (source.id, source.name, source.value, CURRENT_TIMESTAMP);
```

### PostgreSQL UPSERT (ON CONFLICT)

```sql
-- Insert or update if conflict
INSERT INTO products (product_id, name, price, stock)
VALUES (1, 'Widget', 19.99, 100)
ON CONFLICT (product_id)
DO UPDATE SET
    name = EXCLUDED.name,
    price = EXCLUDED.price,
    stock = products.stock + EXCLUDED.stock,  -- Can reference old value
    updated_at = CURRENT_TIMESTAMP;

-- Multiple rows UPSERT
INSERT INTO inventory (product_id, warehouse_id, quantity)
VALUES
    (1, 'WH01', 50),
    (2, 'WH01', 30),
    (3, 'WH01', 100)
ON CONFLICT (product_id, warehouse_id)
DO UPDATE SET
    quantity = inventory.quantity + EXCLUDED.quantity,
    last_updated = CURRENT_TIMESTAMP;
```

### MySQL UPSERT (ON DUPLICATE KEY)

```sql
-- Insert or update if duplicate key
INSERT INTO products (product_id, name, price, stock)
VALUES (1, 'Widget', 19.99, 100)
ON DUPLICATE KEY UPDATE
    name = VALUES(name),
    price = VALUES(price),
    stock = stock + VALUES(stock),
    updated_at = NOW();

-- Bulk UPSERT
INSERT INTO daily_metrics (metric_date, metric_name, value)
VALUES
    ('2024-01-01', 'sales', 1000),
    ('2024-01-01', 'orders', 50),
    ('2024-01-02', 'sales', 1200)
ON DUPLICATE KEY UPDATE
    value = VALUES(value),
    updated_at = NOW();
```

### UPSERT with Subquery

```sql
-- Load staging data into main table
INSERT INTO customers (customer_id, name, email, segment)
SELECT customer_id, name, email, segment
FROM staging_customers
ON CONFLICT (customer_id)
DO UPDATE SET
    name = EXCLUDED.name,
    email = EXCLUDED.email,
    segment = EXCLUDED.segment,
    updated_at = CURRENT_TIMESTAMP;
```

### Common UPSERT Patterns

```sql
-- Pattern 1: Increment if exists, insert if new
INSERT INTO page_views (page_url, view_count, last_viewed)
VALUES ('/home', 1, CURRENT_TIMESTAMP)
ON CONFLICT (page_url)
DO UPDATE SET
    view_count = page_views.view_count + 1,
    last_viewed = CURRENT_TIMESTAMP;

-- Pattern 2: Update only if newer
INSERT INTO product_prices (product_id, price, price_date)
VALUES (1, 29.99, '2024-01-15')
ON CONFLICT (product_id)
DO UPDATE SET
    price = EXCLUDED.price,
    price_date = EXCLUDED.price_date
WHERE EXCLUDED.price_date > product_prices.price_date;

-- Pattern 3: Conditional UPSERT
INSERT INTO customer_stats (customer_id, order_count, total_spent)
VALUES (100, 1, 50.00)
ON CONFLICT (customer_id)
DO UPDATE SET
    order_count = customer_stats.order_count + EXCLUDED.order_count,
    total_spent = customer_stats.total_spent + EXCLUDED.total_spent
WHERE customer_stats.status = 'active';  -- Only update active customers
```

---

## Transactions and ACID Properties

### What is a Transaction?

A **transaction** is a sequence of SQL operations treated as a single unit of work. Either all operations succeed (commit) or all fail (rollback).

### ACID Properties

| Property | Description | Example |
|----------|-------------|---------|
| **Atomicity** | All or nothing | Transfer money: debit AND credit both succeed or both fail |
| **Consistency** | Valid state always | Balance never goes negative |
| **Isolation** | Transactions don't interfere | Two people can't book the same seat |
| **Durability** | Changes persist | After commit, data survives crash |

### Basic Transaction Syntax

```sql
-- Start transaction
BEGIN;  -- or START TRANSACTION;

-- Perform operations
UPDATE accounts SET balance = balance - 100 WHERE account_id = 1;
UPDATE accounts SET balance = balance + 100 WHERE account_id = 2;

-- Check if everything looks good
SELECT * FROM accounts WHERE account_id IN (1, 2);

-- Commit changes (make permanent)
COMMIT;

-- OR rollback if there's an issue
ROLLBACK;
```

### Transaction Example: Bank Transfer

```sql
BEGIN;

-- Debit from account 1
UPDATE accounts
SET balance = balance - 500
WHERE account_id = 1
  AND balance >= 500;  -- Ensure sufficient funds

-- Check if update succeeded (1 row affected)
-- If 0 rows, means insufficient funds

-- Credit to account 2
UPDATE accounts
SET balance = balance + 500
WHERE account_id = 2;

-- Log the transfer
INSERT INTO transaction_log (from_account, to_account, amount, timestamp)
VALUES (1, 2, 500, CURRENT_TIMESTAMP);

-- If all succeeded, commit
COMMIT;

-- If any failed, rollback
-- ROLLBACK;
```

### Savepoints (Partial Rollback)

```sql
BEGIN;

-- Initial operations
INSERT INTO orders (order_id, customer_id, amount)
VALUES (1, 100, 250.00);

-- Create savepoint
SAVEPOINT after_order_insert;

-- More operations
INSERT INTO order_items (order_id, product_id, quantity)
VALUES (1, 501, 2);

-- Oops, error! Rollback to savepoint
ROLLBACK TO SAVEPOINT after_order_insert;

-- Order insert is preserved, order_items is rolled back

-- Insert corrected order items
INSERT INTO order_items (order_id, product_id, quantity)
VALUES (1, 502, 1);

-- Commit everything
COMMIT;
```

### Transaction Isolation Levels

```sql
-- Set isolation level for session
SET TRANSACTION ISOLATION LEVEL READ COMMITTED;
-- Options: READ UNCOMMITTED, READ COMMITTED, REPEATABLE READ, SERIALIZABLE

BEGIN;
-- Transaction operations
COMMIT;
```

**Isolation Levels:**

| Level | Dirty Read | Non-Repeatable Read | Phantom Read |
|-------|-----------|-------------------|--------------|
| **READ UNCOMMITTED** | Yes | Yes | Yes |
| **READ COMMITTED** | No | Yes | Yes |
| **REPEATABLE READ** | No | No | Yes |
| **SERIALIZABLE** | No | No | No |

---

## ETL/ELT Patterns with DML

### Pattern 1: Full Refresh

```sql
BEGIN;

-- Truncate target table
TRUNCATE TABLE target_table;

-- Load fresh data from source
INSERT INTO target_table
SELECT * FROM source_table;

COMMIT;
```

### Pattern 2: Incremental Load (Append)

```sql
-- Load only new records
INSERT INTO target_table
SELECT *
FROM source_table
WHERE created_date >= (
    SELECT COALESCE(MAX(created_date), '1900-01-01')
    FROM target_table
);
```

### Pattern 3: Incremental Load (Upsert)

```sql
-- Insert new, update changed
INSERT INTO target_table
SELECT * FROM source_table
ON CONFLICT (id)
DO UPDATE SET
    column1 = EXCLUDED.column1,
    column2 = EXCLUDED.column2,
    updated_at = CURRENT_TIMESTAMP;
```

### Pattern 4: Staging Table Pattern

```sql
BEGIN;

-- 1. Load data into staging table
TRUNCATE TABLE staging_customers;

INSERT INTO staging_customers
SELECT * FROM external_source;

-- 2. Data quality checks
DELETE FROM staging_customers
WHERE email IS NULL OR email NOT LIKE '%@%';

-- 3. Merge staging into production
MERGE INTO customers AS target
USING staging_customers AS source
ON target.customer_id = source.customer_id
WHEN MATCHED THEN
    UPDATE SET
        name = source.name,
        email = source.email,
        updated_at = CURRENT_TIMESTAMP
WHEN NOT MATCHED THEN
    INSERT (customer_id, name, email, created_at)
    VALUES (source.customer_id, source.name, source.email, CURRENT_TIMESTAMP);

COMMIT;
```

### Pattern 5: Slowly Changing Dimension Type 2

```sql
BEGIN;

-- Close expired records
UPDATE dim_customer
SET
    expiration_date = CURRENT_DATE - INTERVAL '1' DAY,
    is_current = FALSE
WHERE customer_id IN (
    SELECT customer_id FROM staging_customers
)
AND is_current = TRUE;

-- Insert new versions
INSERT INTO dim_customer (customer_id, name, segment, effective_date, is_current)
SELECT
    customer_id,
    name,
    segment,
    CURRENT_DATE,
    TRUE
FROM staging_customers;

COMMIT;
```

### Pattern 6: Delete-Insert Pattern

```sql
BEGIN;

-- Delete existing records for reload period
DELETE FROM fact_sales
WHERE sale_date >= '2024-01-01' AND sale_date < '2024-02-01';

-- Insert fresh data for that period
INSERT INTO fact_sales
SELECT *
FROM staging_sales
WHERE sale_date >= '2024-01-01' AND sale_date < '2024-02-01';

COMMIT;
```

---

## Performance Optimization

### Batch Processing

```sql
-- Process in batches to avoid locking entire table
DO $$
DECLARE
    batch_size INT := 10000;
    rows_affected INT := 1;
BEGIN
    WHILE rows_affected > 0 LOOP
        UPDATE orders
        SET status = 'archived'
        WHERE order_id IN (
            SELECT order_id
            FROM orders
            WHERE order_date < '2020-01-01'
              AND status != 'archived'
            LIMIT batch_size
        );

        GET DIAGNOSTICS rows_affected = ROW_COUNT;

        COMMIT;

        -- Optional: small delay to reduce system load
        PERFORM pg_sleep(0.1);
    END LOOP;
END $$;
```

### Disable Indexes for Bulk Loads

```sql
-- Drop indexes
DROP INDEX idx_customers_email;
DROP INDEX idx_customers_signup_date;

-- Bulk insert
INSERT INTO customers
SELECT * FROM massive_source_table;

-- Recreate indexes
CREATE INDEX idx_customers_email ON customers(email);
CREATE INDEX idx_customers_signup_date ON customers(signup_date);
```

### Use COPY for Large Loads

```sql
-- PostgreSQL: Much faster than INSERT for bulk loads
COPY customers (customer_id, name, email)
FROM '/path/to/data.csv'
WITH (FORMAT CSV, HEADER);

-- MySQL equivalent
LOAD DATA INFILE '/path/to/data.csv'
INTO TABLE customers
FIELDS TERMINATED BY ','
LINES TERMINATED BY '\n'
IGNORE 1 ROWS;
```

---

## Best Practices Checklist

### ✅ Do's

1. **Use transactions** for related operations
2. **Test with SELECT** before UPDATE/DELETE
3. **Use WHERE clauses** (unless intentionally affecting all rows)
4. **Batch large operations** to avoid locks
5. **Archive before deleting** critical data
6. **Use UPSERT** for idempotent pipelines
7. **Add timestamps** (created_at, updated_at)
8. **Handle errors** and rollback when necessary

### ❌ Don'ts

1. **Don't UPDATE/DELETE without WHERE** (unless intentional)
2. **Don't use individual INSERTs** for bulk data
3. **Don't forget to COMMIT** transactions
4. **Don't use long-running transactions** (blocks other operations)
5. **Don't hard delete** without backup/archive
6. **Don't trust data** - validate before DML
7. **Don't ignore return values** (rows affected)
8. **Don't skip testing** on non-production first

---

## Common Pitfalls

### Pitfall 1: Forgetting WHERE Clause

```sql
-- ❌ DISASTER: Updates ALL customers
UPDATE customers SET email = 'test@example.com';

-- ✅ CORRECT: Update specific customer
UPDATE customers SET email = 'test@example.com'
WHERE customer_id = 1;
```

### Pitfall 2: Not Using Transactions

```sql
-- ❌ BAD: If second UPDATE fails, first succeeds (inconsistent state)
UPDATE inventory SET quantity = quantity - 10 WHERE product_id = 1;
UPDATE orders SET status = 'shipped' WHERE order_id = 100;

-- ✅ GOOD: Both succeed or both fail
BEGIN;
UPDATE inventory SET quantity = quantity - 10 WHERE product_id = 1;
UPDATE orders SET status = 'shipped' WHERE order_id = 100;
COMMIT;
```

### Pitfall 3: Locking Large Tables

```sql
-- ❌ BAD: Locks entire table for long time
UPDATE large_table SET status = 'processed' WHERE process_date < '2024-01-01';

-- ✅ GOOD: Process in smaller batches
UPDATE large_table SET status = 'processed'
WHERE id IN (
    SELECT id FROM large_table
    WHERE process_date < '2024-01-01' AND status != 'processed'
    LIMIT 10000
);
```

---

## Related Concepts

- [[Data Definition Language (DDL)]] - Creating and altering tables
- [[SQL Basics - Quick Reference]] - Foundation SQL
- [[Data Modeling & Table Design]] - Schema design
- [[Query Optimization Techniques]] - Performance tuning

---

## Key Takeaways

1. **INSERT** adds data, **UPDATE** modifies data, **DELETE** removes data
2. **MERGE/UPSERT** combines insert and update logic (idempotent)
3. **Transactions** ensure data integrity with ACID properties
4. **Batch operations** are more efficient than row-by-row
5. **Test with SELECT** before UPDATE/DELETE
6. **Use staging tables** for complex ETL workflows
7. **Soft deletes** are often better than hard deletes
8. **Always use WHERE** unless intentionally affecting all rows
