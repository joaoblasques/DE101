---
title: Data Quality & Validation
date: 2025-10-14
tags: [sql, data-quality, validation, data-cleaning, constraints, data-engineering, data-governance, architecture-performance]
status: active
learning_phase: "Architecture & Performance (Engineering)"
---

# Data Quality & Validation

## Overview

Data quality is fundamental to reliable analytics and decision-making. This guide covers SQL techniques for validating, cleaning, and maintaining high-quality data in your data pipelines.

---

## Data Quality Dimensions

### The Six Dimensions

| Dimension | Description | SQL Check |
|-----------|-------------|-----------|
| **Completeness** | No missing critical data | `IS NULL` checks |
| **Validity** | Data conforms to format/rules | Constraints, regex |
| **Accuracy** | Data reflects reality | Business rules, ranges |
| **Consistency** | No contradictions | Cross-field validation |
| **Uniqueness** | No unwanted duplicates | `DISTINCT`, `GROUP BY` |
| **Timeliness** | Data is current | Date checks |

---

## NULL Handling and Validation

### Detecting NULLs

```sql
-- Count NULL values per column
SELECT
    COUNT(*) AS total_rows,
    COUNT(email) AS email_count,
    COUNT(*) - COUNT(email) AS email_nulls,
    (COUNT(*) - COUNT(email)) * 100.0 / COUNT(*) AS email_null_pct
FROM customers;

-- Find rows with any NULL in critical columns
SELECT *
FROM orders
WHERE customer_id IS NULL
   OR order_date IS NULL
   OR total_amount IS NULL;

-- Comprehensive NULL report
SELECT
    'customer_id' AS column_name,
    COUNT(*) - COUNT(customer_id) AS null_count,
    (COUNT(*) - COUNT(customer_id)) * 100.0 / COUNT(*) AS null_pct
FROM customers
UNION ALL
SELECT 'email', COUNT(*) - COUNT(email), (COUNT(*) - COUNT(email)) * 100.0 / COUNT(*) FROM customers
UNION ALL
SELECT 'phone', COUNT(*) - COUNT(phone), (COUNT(*) - COUNT(phone)) * 100.0 / COUNT(*) FROM customers;
```

### NULL Substitution

```sql
-- Replace NULL with default value
SELECT
    customer_id,
    COALESCE(email, 'no-email@placeholder.com') AS email,
    COALESCE(phone, 'UNKNOWN') AS phone,
    COALESCE(discount_rate, 0.0) AS discount_rate
FROM customers;

-- Use first non-NULL value from multiple columns
SELECT
    customer_id,
    COALESCE(mobile_phone, home_phone, work_phone, 'NO PHONE') AS contact_phone
FROM customers;

-- NULLIF - Convert specific values to NULL
SELECT
    customer_id,
    NULLIF(email, '') AS email,  -- Empty string becomes NULL
    NULLIF(age, 0) AS age         -- Zero becomes NULL
FROM customers;
```

---

## Duplicate Detection

### Finding Exact Duplicates

```sql
-- Find duplicate rows (all columns)
SELECT
    customer_id,
    email,
    COUNT(*) AS duplicate_count
FROM customers
GROUP BY customer_id, email
HAVING COUNT(*) > 1;

-- Find duplicates on specific column(s)
SELECT
    email,
    COUNT(*) AS duplicate_count,
    STRING_AGG(CAST(customer_id AS VARCHAR), ', ') AS customer_ids
FROM customers
GROUP BY email
HAVING COUNT(*) > 1;
```

### Finding Similar Duplicates

```sql
-- Find similar names (case-insensitive, whitespace ignored)
SELECT
    c1.customer_id AS id1,
    c2.customer_id AS id2,
    c1.name AS name1,
    c2.name AS name2
FROM customers c1
JOIN customers c2 ON
    LOWER(TRIM(c1.name)) = LOWER(TRIM(c2.name))
    AND c1.customer_id < c2.customer_id;

-- Fuzzy matching with Levenshtein (PostgreSQL)
SELECT
    c1.customer_id AS id1,
    c2.customer_id AS id2,
    c1.email AS email1,
    c2.email AS email2,
    LEVENSHTEIN(c1.email, c2.email) AS edit_distance
FROM customers c1
JOIN customers c2 ON
    c1.customer_id < c2.customer_id
    AND LEVENSHTEIN(c1.email, c2.email) <= 3;
```

### Removing Duplicates

```sql
-- Keep first occurrence (by customer_id)
DELETE FROM customers
WHERE customer_id NOT IN (
    SELECT MIN(customer_id)
    FROM customers
    GROUP BY email
);

-- Using window function (PostgreSQL)
DELETE FROM customers
WHERE customer_id IN (
    SELECT customer_id
    FROM (
        SELECT
            customer_id,
            ROW_NUMBER() OVER (PARTITION BY email ORDER BY created_at) AS rn
        FROM customers
    ) t
    WHERE rn > 1
);

-- Create clean table without duplicates
CREATE TABLE customers_clean AS
SELECT DISTINCT ON (email)
    *
FROM customers
ORDER BY email, created_at DESC;  -- Keep most recent
```

---

## Data Format Validation

### Email Validation

```sql
-- Basic email format check
SELECT *
FROM customers
WHERE email IS NOT NULL
  AND email NOT LIKE '%_@__%.__%';  -- Basic pattern

-- More robust with regex (PostgreSQL)
SELECT *
FROM customers
WHERE email IS NOT NULL
  AND email !~ '^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$';

-- Common email issues
SELECT
    email,
    CASE
        WHEN email IS NULL THEN 'NULL email'
        WHEN email = '' THEN 'Empty string'
        WHEN email NOT LIKE '%@%' THEN 'Missing @'
        WHEN email LIKE '% %' THEN 'Contains space'
        WHEN email LIKE '%..%' THEN 'Consecutive dots'
        WHEN LOWER(email) != email THEN 'Mixed case'
        ELSE 'Valid'
    END AS validation_status
FROM customers;
```

### Phone Number Validation

```sql
-- US phone number validation
SELECT *
FROM customers
WHERE phone IS NOT NULL
  AND LENGTH(REGEXP_REPLACE(phone, '[^0-9]', '', 'g')) NOT IN (10, 11);

-- Clean and validate phone numbers
SELECT
    customer_id,
    phone AS original_phone,
    REGEXP_REPLACE(phone, '[^0-9]', '', 'g') AS digits_only,
    LENGTH(REGEXP_REPLACE(phone, '[^0-9]', '', 'g')) AS digit_count,
    CASE
        WHEN LENGTH(REGEXP_REPLACE(phone, '[^0-9]', '', 'g')) = 10 THEN 'Valid'
        WHEN LENGTH(REGEXP_REPLACE(phone, '[^0-9]', '', 'g')) = 11 THEN 'Valid (with country code)'
        ELSE 'Invalid'
    END AS validation_status
FROM customers;
```

### Date Validation

```sql
-- Find invalid or suspicious dates
SELECT *
FROM orders
WHERE order_date IS NULL
   OR order_date > CURRENT_DATE  -- Future date
   OR order_date < '2000-01-01'  -- Too old
   OR order_date > ship_date;     -- Logical inconsistency

-- Date range validation
SELECT
    order_id,
    order_date,
    ship_date,
    delivery_date,
    CASE
        WHEN order_date IS NULL THEN 'Missing order date'
        WHEN ship_date < order_date THEN 'Shipped before ordered'
        WHEN delivery_date < ship_date THEN 'Delivered before shipped'
        WHEN delivery_date - order_date > 365 THEN 'Delivery took over 1 year'
        ELSE 'Valid'
    END AS validation_status
FROM orders;
```

---

## Range and Boundary Checks

### Numeric Range Validation

```sql
-- Find values outside expected range
SELECT *
FROM products
WHERE price < 0
   OR price > 100000
   OR discount_pct < 0
   OR discount_pct > 100;

-- Statistical outlier detection (values beyond 3 std deviations)
WITH stats AS (
    SELECT
        AVG(price) AS mean_price,
        STDDEV(price) AS std_price
    FROM products
)
SELECT p.*
FROM products p, stats
WHERE p.price < stats.mean_price - (3 * stats.std_price)
   OR p.price > stats.mean_price + (3 * stats.std_price);
```

### Text Length Validation

```sql
-- Find strings that are too short or too long
SELECT *
FROM customers
WHERE LENGTH(name) < 2              -- Too short
   OR LENGTH(name) > 100            -- Too long
   OR LENGTH(email) > 255           -- Exceeds typical email length
   OR LENGTH(TRIM(name)) = 0;       -- Empty after trim
```

---

## Cross-Field Validation

### Logical Consistency

```sql
-- Check for logical inconsistencies
SELECT *
FROM employees
WHERE hire_date > termination_date
   OR birth_date > hire_date
   OR AGE(birth_date) < INTERVAL '16 years';  -- Under legal working age

-- Product validation
SELECT *
FROM products
WHERE (sale_price > list_price)     -- Sale price higher than list
   OR (cost > list_price)           -- Cost exceeds price
   OR (discount_amount > list_price); -- Discount exceeds price
```

### Referential Integrity

```sql
-- Find orphaned records (no matching foreign key)
SELECT o.*
FROM orders o
LEFT JOIN customers c ON o.customer_id = c.customer_id
WHERE c.customer_id IS NULL;

-- Find orders without line items
SELECT o.*
FROM orders o
LEFT JOIN order_items oi ON o.order_id = oi.order_id
WHERE oi.order_id IS NULL;

-- Comprehensive referential integrity check
SELECT
    'orders -> customers' AS relationship,
    COUNT(*) AS orphan_count
FROM orders o
LEFT JOIN customers c ON o.customer_id = c.customer_id
WHERE c.customer_id IS NULL
UNION ALL
SELECT
    'order_items -> orders',
    COUNT(*)
FROM order_items oi
LEFT JOIN orders o ON oi.order_id = o.order_id
WHERE o.order_id IS NULL;
```

---

## Data Quality Scoring

### Completeness Score

```sql
-- Calculate completeness score per customer
SELECT
    customer_id,
    (CASE WHEN name IS NOT NULL THEN 1 ELSE 0 END +
     CASE WHEN email IS NOT NULL THEN 1 ELSE 0 END +
     CASE WHEN phone IS NOT NULL THEN 1 ELSE 0 END +
     CASE WHEN address IS NOT NULL THEN 1 ELSE 0 END +
     CASE WHEN city IS NOT NULL THEN 1 ELSE 0 END) * 100.0 / 5 AS completeness_pct
FROM customers;
```

### Validity Score

```sql
-- Calculate validity score
WITH validation AS (
    SELECT
        customer_id,
        CASE WHEN email ~ '^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$' THEN 1 ELSE 0 END AS email_valid,
        CASE WHEN LENGTH(REGEXP_REPLACE(phone, '[^0-9]', '', 'g')) IN (10, 11) THEN 1 ELSE 0 END AS phone_valid,
        CASE WHEN LENGTH(name) >= 2 THEN 1 ELSE 0 END AS name_valid,
        CASE WHEN birth_date <= CURRENT_DATE - INTERVAL '18 years' THEN 1 ELSE 0 END AS age_valid
    FROM customers
)
SELECT
    customer_id,
    (email_valid + phone_valid + name_valid + age_valid) * 100.0 / 4 AS validity_pct
FROM validation;
```

### Comprehensive Data Quality Report

```sql
WITH quality_metrics AS (
    SELECT
        COUNT(*) AS total_records,
        -- Completeness
        COUNT(email) * 100.0 / COUNT(*) AS email_completeness,
        COUNT(phone) * 100.0 / COUNT(*) AS phone_completeness,
        -- Validity
        SUM(CASE WHEN email ~ '^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$' THEN 1 ELSE 0 END) * 100.0 /
            NULLIF(COUNT(email), 0) AS email_validity,
        -- Uniqueness
        COUNT(DISTINCT email) * 100.0 / NULLIF(COUNT(email), 0) AS email_uniqueness,
        -- Accuracy (age range)
        SUM(CASE WHEN AGE(birth_date) BETWEEN INTERVAL '18 years' AND INTERVAL '120 years' THEN 1 ELSE 0 END) * 100.0 /
            NULLIF(COUNT(birth_date), 0) AS age_accuracy
    FROM customers
)
SELECT
    'Total Records' AS metric,
    total_records AS value,
    NULL AS percentage
FROM quality_metrics
UNION ALL
SELECT 'Email Completeness', NULL, email_completeness FROM quality_metrics
UNION ALL
SELECT 'Phone Completeness', NULL, phone_completeness FROM quality_metrics
UNION ALL
SELECT 'Email Validity', NULL, email_validity FROM quality_metrics
UNION ALL
SELECT 'Email Uniqueness', NULL, email_uniqueness FROM quality_metrics
UNION ALL
SELECT 'Age Accuracy', NULL, age_accuracy FROM quality_metrics;
```

---

## Data Profiling

### Column Statistics

```sql
-- Profile a numeric column
SELECT
    'price' AS column_name,
    COUNT(*) AS total_count,
    COUNT(price) AS non_null_count,
    COUNT(*) - COUNT(price) AS null_count,
    MIN(price) AS min_value,
    MAX(price) AS max_value,
    AVG(price) AS mean_value,
    PERCENTILE_CONT(0.50) WITHIN GROUP (ORDER BY price) AS median_value,
    STDDEV(price) AS std_dev,
    COUNT(DISTINCT price) AS distinct_count
FROM products;

-- Profile a text column
SELECT
    'name' AS column_name,
    COUNT(*) AS total_count,
    COUNT(name) AS non_null_count,
    COUNT(DISTINCT name) AS distinct_count,
    MIN(LENGTH(name)) AS min_length,
    MAX(LENGTH(name)) AS max_length,
    AVG(LENGTH(name)) AS avg_length
FROM customers;
```

### Value Distribution

```sql
-- Top values and their frequencies
SELECT
    status,
    COUNT(*) AS frequency,
    COUNT(*) * 100.0 / SUM(COUNT(*)) OVER () AS percentage,
    STRING_AGG(order_id::TEXT, ', ') AS sample_order_ids
FROM orders
GROUP BY status
ORDER BY frequency DESC
LIMIT 10;

-- Value distribution buckets
SELECT
    WIDTH_BUCKET(price, 0, 1000, 10) AS price_bucket,
    COUNT(*) AS product_count,
    MIN(price) AS bucket_min,
    MAX(price) AS bucket_max
FROM products
WHERE price BETWEEN 0 AND 1000
GROUP BY WIDTH_BUCKET(price, 0, 1000, 10)
ORDER BY price_bucket;
```

---

## Data Cleaning Patterns

### Pattern 1: Standardize Text

```sql
-- Standardize email addresses
UPDATE customers
SET email = LOWER(TRIM(email))
WHERE email IS NOT NULL;

-- Standardize names (title case)
UPDATE customers
SET name = INITCAP(TRIM(name))
WHERE name IS NOT NULL;

-- Remove extra whitespace
UPDATE customers
SET address = REGEXP_REPLACE(TRIM(address), '\s+', ' ', 'g')
WHERE address IS NOT NULL;
```

### Pattern 2: Fix Data Types

```sql
-- Convert string to proper date
UPDATE orders
SET order_date = TO_DATE(order_date_string, 'MM/DD/YYYY')
WHERE order_date IS NULL AND order_date_string IS NOT NULL;

-- Parse numeric from string
UPDATE products
SET price = CAST(REGEXP_REPLACE(price_string, '[^0-9.]', '', 'g') AS DECIMAL(10,2))
WHERE price IS NULL AND price_string IS NOT NULL;
```

### Pattern 3: Handle Missing Data

```sql
-- Fill missing values with defaults
UPDATE customers
SET
    country = COALESCE(country, 'USA'),
    discount_rate = COALESCE(discount_rate, 0.0),
    is_active = COALESCE(is_active, TRUE)
WHERE country IS NULL OR discount_rate IS NULL OR is_active IS NULL;

-- Fill missing values with median/mode
WITH median_price AS (
    SELECT PERCENTILE_CONT(0.50) WITHIN GROUP (ORDER BY price) AS median
    FROM products
    WHERE price IS NOT NULL
)
UPDATE products
SET price = (SELECT median FROM median_price)
WHERE price IS NULL;
```

### Pattern 4: Deduplicate

```sql
-- Keep most recent duplicate
WITH duplicates AS (
    SELECT
        customer_id,
        ROW_NUMBER() OVER (PARTITION BY email ORDER BY updated_at DESC) AS rn
    FROM customers
)
DELETE FROM customers
WHERE customer_id IN (
    SELECT customer_id FROM duplicates WHERE rn > 1
);
```

---

## Implementing Data Quality Checks

### Create Data Quality Table

```sql
CREATE TABLE data_quality_checks (
    check_id SERIAL PRIMARY KEY,
    table_name VARCHAR(100),
    check_name VARCHAR(200),
    check_query TEXT,
    threshold_value NUMERIC,
    check_timestamp TIMESTAMP DEFAULT NOW(),
    result_count INTEGER,
    passed BOOLEAN,
    details TEXT
);
```

### Run Quality Check

```sql
-- Example: Check for NULL emails
WITH quality_check AS (
    SELECT COUNT(*) AS null_email_count
    FROM customers
    WHERE email IS NULL
)
INSERT INTO data_quality_checks (
    table_name,
    check_name,
    check_query,
    threshold_value,
    result_count,
    passed,
    details
)
SELECT
    'customers',
    'NULL email check',
    'SELECT COUNT(*) FROM customers WHERE email IS NULL',
    0,  -- Threshold: 0 NULLs allowed
    null_email_count,
    null_email_count = 0,
    CASE
        WHEN null_email_count = 0 THEN 'All emails present'
        ELSE null_email_count || ' customers missing email'
    END
FROM quality_check;
```

### Automated Quality Report

```sql
-- Run all checks and generate report
WITH recent_checks AS (
    SELECT
        table_name,
        check_name,
        check_timestamp,
        result_count,
        passed,
        details
    FROM data_quality_checks
    WHERE check_timestamp >= NOW() - INTERVAL '1 day'
)
SELECT
    table_name,
    COUNT(*) AS total_checks,
    SUM(CASE WHEN passed THEN 1 ELSE 0 END) AS passed_checks,
    SUM(CASE WHEN NOT passed THEN 1 ELSE 0 END) AS failed_checks,
    ROUND(SUM(CASE WHEN passed THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) AS pass_rate
FROM recent_checks
GROUP BY table_name
ORDER BY pass_rate;
```

---

## Best Practices

### ✅ Do's

1. **Validate at ingestion** - Catch issues early in pipeline
2. **Document quality rules** - Clear definitions and thresholds
3. **Monitor trends** - Track quality over time
4. **Set realistic thresholds** - Not all data can be 100% perfect
5. **Prioritize critical fields** - Focus on business-critical data
6. **Automate checks** - Schedule regular quality assessments
7. **Log quality issues** - Maintain audit trail
8. **Clean incrementally** - Don't try to fix everything at once

### ❌ Don'ts

1. **Don't delete data without backup** - Archive before cleanup
2. **Don't assume data types** - Validate and convert explicitly
3. **Don't ignore business context** - What's "valid" depends on use case
4. **Don't over-clean** - Sometimes NULL has meaning
5. **Don't trust external data** - Always validate
6. **Don't skip profiling** - Understand data before cleaning
7. **Don't hard-code thresholds** - Make them configurable

---

## Quick Reference Checklist

```
CHECK TYPE          VALIDATION                         SQL PATTERN
──────────────────────────────────────────────────────────────────────
Completeness       No NULLs in critical fields        IS NULL
Uniqueness         No duplicates                      GROUP BY HAVING COUNT(*) > 1
Validity           Correct format/range               REGEXP, BETWEEN, IN
Consistency        No contradictions                  date1 < date2, JOIN checks
Accuracy           Within expected range              Statistical outliers
Timeliness         Data is current                    Date comparisons
Referential        Foreign keys exist                 LEFT JOIN IS NULL
```

---

## Related Concepts

- [[Data Definition Language (DDL)]] - Constraints for data quality
- [[String Manipulation & Pattern Matching]] - Text validation
- [[Date & Time Operations]] - Date validation
- [[Data Manipulation (DML)]] - Data cleaning operations

---

## Key Takeaways

1. **Validate early** in the data pipeline (at ingestion)
2. **Use constraints** (NOT NULL, CHECK, FOREIGN KEY) to prevent bad data
3. **Profile data first** - Understand before cleaning
4. **Automate quality checks** - Schedule regular assessments
5. **Track metrics over time** - Monitor quality trends
6. **Document quality rules** - Clear, business-aligned definitions
7. **Balance quality and practicality** - Not all data needs to be perfect
8. **Maintain audit trail** - Log all quality checks and data cleaning operations
