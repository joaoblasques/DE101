---
title: dbt Models & Materializations
date: 2025-10-17
tags: [dbt, models, materializations, incremental, snapshots, jinja, data-transformation, advanced-dbt]
status: active
learning_phase: "Advanced Queries (Analytical)"
source: https://docs.getdbt.com/docs/build/models
---

**Created:** 2025-10-17
**Last Updated:** 2025-10-17
**Status:** âœ… Complete

---

## Overview

Models are the building blocks of dbt projects - each model is a SQL SELECT statement that transforms raw data into analytics-ready tables and views. Understanding materializations (how models persist in your warehouse) and model dependencies is crucial for building efficient, maintainable data pipelines.

**Key Value Proposition:** Models with proper materializations enable you to balance query performance, warehouse costs, and data freshness based on your specific use case.

---

## 3.1 Models - The Foundation

### What is a Model?

**Definition:** A model is a .sql file containing a single SELECT statement that defines a data transformation. Each model creates one object (table, view, or incremental table) in your data warehouse.

```sql
-- models/staging/stg_customers.sql
-- This is a complete model!

SELECT
    id AS customer_id,
    UPPER(TRIM(first_name)) AS first_name,
    UPPER(TRIM(last_name)) AS last_name,
    LOWER(TRIM(email)) AS email,
    created_at,
    updated_at
FROM {{ source('raw', 'customers') }}
WHERE deleted_at IS NULL
```

**Key Characteristics:**
- One model = one .sql file = one database object
- Pure SELECT statements (no DDL required)
- Models reference other models via `{{ ref() }}`
- Models reference source tables via `{{ source() }}`

---

### Model Layers - Organizing Transformations

dbt projects typically follow a layered architecture:

```
Staging â†’ Intermediate â†’ Marts
(Clean)   (Join/Enrich)  (Business Logic)
```

#### Staging Layer

**Purpose:** Light transformations on source data - renaming, type casting, basic cleaning.

**Characteristics:**
- 1:1 relationship with source tables
- Minimal business logic
- Typically materialized as views
- Column renaming for consistency
- Data type standardization

```sql
-- models/staging/stripe/stg_stripe_payments.sql

WITH source AS (
    SELECT * FROM {{ source('stripe', 'payments') }}
),

cleaned AS (
    SELECT
        id AS payment_id,
        order_id,
        payment_method,
        -- Convert cents to dollars
        amount / 100.0 AS amount_usd,
        -- Standardize status values
        CASE
            WHEN status = 'successful' THEN 'success'
            WHEN status = 'failed' THEN 'fail'
            ELSE status
        END AS payment_status,
        created_at,
        _fivetran_synced AS source_synced_at
    FROM source
)

SELECT * FROM cleaned
```

**Naming Convention:** `stg_<source>_<entity>.sql`

---

#### Intermediate Layer

**Purpose:** Complex joins, enrichment, and business logic that doesn't fit in staging or marts.

**Characteristics:**
- Joins between staging models
- Complex calculations
- Window functions
- Typically ephemeral or views
- Not exposed to end users

```sql
-- models/intermediate/orders/int_orders_with_payments.sql

WITH orders AS (
    SELECT * FROM {{ ref('stg_orders') }}
),

payments AS (
    SELECT * FROM {{ ref('stg_stripe_payments') }}
),

order_payments AS (
    SELECT
        o.order_id,
        o.customer_id,
        o.order_date,
        o.status AS order_status,
        p.payment_id,
        p.payment_method,
        p.amount_usd,
        p.payment_status,
        -- Calculate running total per order
        SUM(p.amount_usd) OVER (
            PARTITION BY o.order_id
            ORDER BY p.created_at
        ) AS cumulative_payment_amount
    FROM orders o
    LEFT JOIN payments p
        ON o.order_id = p.order_id
)

SELECT * FROM order_payments
```

**Naming Convention:** `int_<entity>_<verb>.sql`

---

#### Marts Layer

**Purpose:** Business-defined entities ready for reporting and analytics.

**Characteristics:**
- Denormalized for query performance
- Business metrics and KPIs
- Typically materialized as tables
- Optimized for BI tool consumption
- Well-documented and tested

```sql
-- models/marts/finance/customer_lifetime_value.sql

WITH customers AS (
    SELECT * FROM {{ ref('stg_customers') }}
),

orders AS (
    SELECT * FROM {{ ref('int_orders_with_payments') }}
),

customer_orders AS (
    SELECT
        c.customer_id,
        c.first_name,
        c.last_name,
        c.email,
        c.created_at AS customer_created_at,
        COUNT(DISTINCT o.order_id) AS total_orders,
        SUM(o.amount_usd) AS lifetime_value,
        MIN(o.order_date) AS first_order_date,
        MAX(o.order_date) AS most_recent_order_date,
        MAX(o.order_date) - MIN(o.order_date) AS customer_lifespan_days
    FROM customers c
    LEFT JOIN orders o
        ON c.customer_id = o.customer_id
        AND o.payment_status = 'success'
    GROUP BY 1, 2, 3, 4, 5
)

SELECT * FROM customer_orders
```

**Naming Convention:** `<entity>.sql` or `fct_<entity>.sql`, `dim_<entity>.sql`

---

### Model Configuration

Models can be configured in multiple places with increasing specificity:

```yaml
# dbt_project.yml - Project-level defaults
models:
  my_project:
    staging:
      +materialized: view
      +schema: staging
    intermediate:
      +materialized: ephemeral
    marts:
      +materialized: table
      +schema: marts
```

```sql
-- In-file config (overrides project config)
{{ config(
    materialized='incremental',
    unique_key='order_id',
    on_schema_change='fail',
    tags=['daily', 'critical']
) }}

SELECT ...
```

**Configuration Precedence:**
1. In-file config (highest)
2. Model-specific YAML
3. Directory config in dbt_project.yml
4. Project defaults (lowest)

---

## 3.2 Materializations - Persistence Strategies

### Overview of Materializations

| Materialization | Database Object | Rebuild Strategy | Use Case | Speed |
|----------------|-----------------|------------------|----------|-------|
| **view** | VIEW | Full refresh | Small datasets, rarely queried | âš¡âš¡âš¡ Instant |
| **table** | TABLE | Full refresh | Medium datasets, frequently queried | âš¡ Slow |
| **incremental** | TABLE | Append/merge new records | Large datasets, append-only | âš¡âš¡ Fast |
| **ephemeral** | CTE | No object created | Reusable SQL logic | âš¡âš¡âš¡ N/A |

---

### View Materialization

**Definition:** Creates a database VIEW - a saved SQL query that executes every time it's queried.

```sql
{{ config(materialized='view') }}

SELECT
    customer_id,
    first_name,
    last_name,
    email
FROM {{ source('raw', 'customers') }}
WHERE active = true
```

**Generated SQL:**
```sql
CREATE OR REPLACE VIEW analytics.stg_customers AS
SELECT
    customer_id,
    first_name,
    last_name,
    email
FROM raw.customers
WHERE active = true
```

**Advantages:**
- âœ… Instant builds (no data copying)
- âœ… Always reflects latest source data
- âœ… No storage costs
- âœ… Great for development

**Disadvantages:**
- âŒ Slower query performance
- âŒ Repeated computation for each query
- âŒ Can't add indexes

**Best For:**
- Staging models (light transformations)
- Development/testing
- Infrequently queried data
- When freshness is critical

---

### Table Materialization

**Definition:** Creates a physical TABLE by running the SELECT query and storing results.

```sql
{{ config(
    materialized='table',
    schema='marts'
) }}

WITH customer_orders AS (
    SELECT
        customer_id,
        COUNT(*) AS order_count,
        SUM(order_total) AS lifetime_value
    FROM {{ ref('stg_orders') }}
    GROUP BY customer_id
)

SELECT * FROM customer_orders
```

**Generated SQL:**
```sql
CREATE TABLE analytics.marts.customer_metrics AS
SELECT
    customer_id,
    COUNT(*) AS order_count,
    SUM(order_total) AS lifetime_value
FROM analytics.stg_orders
GROUP BY customer_id
```

**Rebuild Process:**
1. Begin transaction
2. Drop existing table (if exists)
3. Create new table with query results
4. Commit transaction

**Advantages:**
- âœ… Fast query performance
- âœ… Can add indexes/constraints
- âœ… Stable for reporting
- âœ… Predictable compute costs

**Disadvantages:**
- âŒ Slower builds (full refresh)
- âŒ Storage costs
- âŒ Data staleness between runs

**Best For:**
- Marts layer (reporting tables)
- Frequently queried models
- Models with complex aggregations
- BI tool dashboards

---

### Incremental Materialization

**Definition:** On first run creates a table; on subsequent runs only processes new/changed records.

```sql
{{ config(
    materialized='incremental',
    unique_key='event_id',
    on_schema_change='fail'
) }}

SELECT
    event_id,
    user_id,
    event_type,
    event_timestamp,
    properties
FROM {{ source('raw', 'events') }}

{% if is_incremental() %}
    -- Only new records since last run
    WHERE event_timestamp > (SELECT MAX(event_timestamp) FROM {{ this }})
{% endif %}
```

**Generated SQL (First Run):**
```sql
CREATE TABLE analytics.events AS
SELECT
    event_id,
    user_id,
    event_type,
    event_timestamp,
    properties
FROM raw.events
```

**Generated SQL (Subsequent Runs):**
```sql
-- Create temporary table with new data
CREATE TEMP TABLE events__dbt_tmp AS
SELECT
    event_id,
    user_id,
    event_type,
    event_timestamp,
    properties
FROM raw.events
WHERE event_timestamp > (SELECT MAX(event_timestamp) FROM analytics.events)

-- Merge into main table
MERGE INTO analytics.events USING events__dbt_tmp
    ON analytics.events.event_id = events__dbt_tmp.event_id
WHEN MATCHED THEN UPDATE SET ...
WHEN NOT MATCHED THEN INSERT ...
```

**Key Components:**

1. **unique_key:** Column(s) to identify unique records
2. **is_incremental():** Jinja macro that returns true after first run
3. **{{ this }}:** Reference to the current model's table

---

### Incremental Strategies

Different warehouses support different merge strategies:

#### 1. Append Strategy (Default)

**Use Case:** Immutable event data (logs, clickstream, transactions)

```sql
{{ config(
    materialized='incremental',
    incremental_strategy='append'
) }}

SELECT
    event_id,
    event_timestamp,
    user_id,
    event_type
FROM {{ source('raw', 'events') }}

{% if is_incremental() %}
    WHERE event_timestamp > (SELECT MAX(event_timestamp) FROM {{ this }})
{% endif %}
```

**Behavior:** Simply appends new rows without checking for duplicates.

**Advantages:**
- âœ… Fastest strategy
- âœ… Simple logic

**Disadvantages:**
- âŒ Can create duplicates if source has them
- âŒ Can't update existing records

---

#### 2. Merge Strategy

**Use Case:** Mutable data (customer profiles, order status updates)

```sql
{{ config(
    materialized='incremental',
    unique_key='customer_id',
    incremental_strategy='merge'
) }}

SELECT
    customer_id,
    first_name,
    last_name,
    email,
    updated_at
FROM {{ source('raw', 'customers') }}

{% if is_incremental() %}
    WHERE updated_at > (SELECT MAX(updated_at) FROM {{ this }})
{% endif %}
```

**Behavior:**
- If unique_key exists â†’ UPDATE
- If unique_key doesn't exist â†’ INSERT

**Advantages:**
- âœ… Handles updates to existing records
- âœ… Prevents duplicates
- âœ… Upsert semantics

**Disadvantages:**
- âŒ Slower than append
- âŒ Requires unique_key

**Supported Warehouses:** Snowflake, BigQuery, Databricks, Redshift

---

#### 3. Delete+Insert Strategy

**Use Case:** When merge is unavailable or partitioned data

```sql
{{ config(
    materialized='incremental',
    unique_key='date_day',
    incremental_strategy='delete+insert'
) }}

SELECT
    DATE_TRUNC('day', event_timestamp) AS date_day,
    user_id,
    COUNT(*) AS event_count
FROM {{ source('raw', 'events') }}

{% if is_incremental() %}
    WHERE DATE_TRUNC('day', event_timestamp) > (
        SELECT MAX(date_day) FROM {{ this }}
    )
{% endif %}

GROUP BY 1, 2
```

**Behavior:**
1. Delete existing rows matching unique_key from new data
2. Insert all new rows

**Advantages:**
- âœ… Works on warehouses without merge
- âœ… Good for partitioned data

**Disadvantages:**
- âŒ Less atomic than merge
- âŒ Can have brief inconsistency during delete/insert

---

### Incremental Best Practices

```sql
{{ config(
    materialized='incremental',
    unique_key='order_id',
    on_schema_change='fail',  -- Fail on schema changes
    tags=['hourly', 'critical']
) }}

WITH source AS (
    SELECT * FROM {{ source('raw', 'orders') }}
),

enriched AS (
    SELECT
        order_id,
        customer_id,
        order_total,
        order_status,
        created_at,
        updated_at,
        -- Add surrogate key for data quality
        {{ dbt_utils.generate_surrogate_key(['order_id', 'updated_at']) }} AS sk
    FROM source

    {% if is_incremental() %}
        -- Two conditions for safety:
        -- 1. New records by timestamp
        WHERE updated_at > (SELECT MAX(updated_at) FROM {{ this }})

        -- 2. Or recently modified records (catch late-arriving updates)
        OR updated_at > CURRENT_TIMESTAMP - INTERVAL '3 days'
    {% endif %}
)

SELECT * FROM enriched
```

**Best Practices:**
1. **Always use unique_key** for non-append strategies
2. **Test uniqueness** with dbt tests
3. **Use updated_at** timestamps when available
4. **Include lookback window** for late-arriving data
5. **Full refresh periodically** to prevent data drift
6. **Monitor table size** to ensure incremental logic works

**Full Refresh Override:**
```bash
# Force full rebuild of incremental model
dbt run --select my_incremental_model --full-refresh
```

---

### Ephemeral Materialization

**Definition:** Doesn't create a database object - instead, SQL is interpolated as a CTE in dependent models.

```sql
-- models/staging/stg_orders_with_discounts.sql
{{ config(materialized='ephemeral') }}

SELECT
    order_id,
    customer_id,
    order_total,
    -- Complex discount calculation
    CASE
        WHEN customer_tier = 'gold' THEN order_total * 0.20
        WHEN customer_tier = 'silver' THEN order_total * 0.10
        ELSE 0
    END AS discount_amount
FROM {{ ref('stg_orders') }}
```

```sql
-- models/marts/customer_orders.sql
SELECT
    order_id,
    customer_id,
    order_total,
    discount_amount,
    order_total - discount_amount AS final_total
FROM {{ ref('stg_orders_with_discounts') }}
```

**Compiled SQL:**
```sql
-- The ephemeral model is inlined as a CTE
WITH stg_orders_with_discounts AS (
    SELECT
        order_id,
        customer_id,
        order_total,
        CASE
            WHEN customer_tier = 'gold' THEN order_total * 0.20
            WHEN customer_tier = 'silver' THEN order_total * 0.10
            ELSE 0
        END AS discount_amount
    FROM analytics.stg_orders
)

SELECT
    order_id,
    customer_id,
    order_total,
    discount_amount,
    order_total - discount_amount AS final_total
FROM stg_orders_with_discounts
```

**Advantages:**
- âœ… No database objects created
- âœ… DRY (Don't Repeat Yourself) logic
- âœ… No storage costs
- âœ… Modular code organization

**Disadvantages:**
- âŒ Can't query directly
- âŒ Repeated computation if used by multiple models
- âŒ Can make compiled SQL complex

**Best For:**
- Reusable logic (macros alternative)
- Intermediate calculations
- Models used by only one downstream model
- Keeping data warehouse clean

---

## 3.3 Snapshots - Slowly Changing Dimensions (Type 2 SCD)

### What are Snapshots?

**Definition:** Snapshots capture the state of mutable tables over time, implementing Type 2 Slowly Changing Dimensions to track historical changes.

**Use Case:** Track how data changes over time (e.g., customer addresses, product prices, order statuses).

```sql
-- snapshots/customers_snapshot.sql

{% snapshot customers_snapshot %}

{{
    config(
      target_schema='snapshots',
      unique_key='customer_id',
      strategy='timestamp',
      updated_at='updated_at'
    )
}}

SELECT * FROM {{ source('raw', 'customers') }}

{% endsnapshot %}
```

**Generated Table Structure:**
```sql
CREATE TABLE snapshots.customers_snapshot (
    customer_id INT,
    first_name VARCHAR,
    email VARCHAR,
    updated_at TIMESTAMP,
    -- dbt adds these:
    dbt_valid_from TIMESTAMP,
    dbt_valid_to TIMESTAMP,
    dbt_scd_id VARCHAR,
    dbt_updated_at TIMESTAMP
)
```

**Example Data:**

| customer_id | email | updated_at | dbt_valid_from | dbt_valid_to |
|-------------|-------|------------|----------------|--------------|
| 101 | old@email.com | 2024-01-01 | 2024-01-01 | 2024-03-15 |
| 101 | new@email.com | 2024-03-15 | 2024-03-15 | NULL |

---

### Snapshot Strategies

#### 1. Timestamp Strategy

**Use Case:** Source table has an `updated_at` column.

```sql
{% snapshot orders_snapshot %}

{{
    config(
      target_schema='snapshots',
      unique_key='order_id',
      strategy='timestamp',
      updated_at='updated_at',
      invalidate_hard_deletes=True
    )
}}

SELECT * FROM {{ source('raw', 'orders') }}

{% endsnapshot %}
```

**How It Works:**
1. Compare `updated_at` in source to last snapshot
2. If changed â†’ close old record, insert new record
3. If deleted (and `invalidate_hard_deletes=True`) â†’ close record

---

#### 2. Check Strategy

**Use Case:** No reliable timestamp; check specific columns for changes.

```sql
{% snapshot products_snapshot %}

{{
    config(
      target_schema='snapshots',
      unique_key='product_id',
      strategy='check',
      check_cols=['price', 'category', 'status']
    )
}}

SELECT * FROM {{ source('raw', 'products') }}

{% endsnapshot %}
```

**How It Works:**
1. Hash specified columns
2. Compare hash to previous snapshot
3. If different â†’ close old record, insert new record

**Variations:**
- `check_cols=['price', 'status']` - Specific columns
- `check_cols='all'` - All columns (slower)

---

### Running Snapshots

```bash
# Run all snapshots
dbt snapshot

# Run specific snapshot
dbt snapshot --select customers_snapshot

# Snapshots run independently from models
dbt run  # Does NOT run snapshots
```

**Scheduling:**
- Run snapshots on a regular schedule (hourly, daily)
- Run BEFORE dbt models that depend on them
- Capture state before transformations run

---

### Querying Snapshots

```sql
-- Get current state of all customers
SELECT *
FROM snapshots.customers_snapshot
WHERE dbt_valid_to IS NULL

-- Get customer state on specific date
SELECT *
FROM snapshots.customers_snapshot
WHERE '2024-06-01' BETWEEN dbt_valid_from AND COALESCE(dbt_valid_to, '9999-12-31')

-- See all historical changes for a customer
SELECT
    customer_id,
    email,
    dbt_valid_from,
    dbt_valid_to,
    DATEDIFF('day', dbt_valid_from, COALESCE(dbt_valid_to, CURRENT_DATE)) AS days_active
FROM snapshots.customers_snapshot
WHERE customer_id = 101
ORDER BY dbt_valid_from
```

---

### Snapshot Best Practices

**1. Choose the Right Strategy**
```sql
-- Has updated_at? Use timestamp
strategy='timestamp', updated_at='updated_at'

-- No timestamp? Use check with specific columns
strategy='check', check_cols=['critical', 'columns']
```

**2. Handle Hard Deletes**
```sql
{{
    config(
      invalidate_hard_deletes=True  -- Close records when deleted from source
    )
}}
```

**3. Test Snapshot Quality**
```yaml
# tests/snapshot_quality.yml
snapshots:
  - name: customers_snapshot
    tests:
      - dbt_utils.recency:
          datepart: day
          field: dbt_updated_at
          interval: 1
```

**4. Monitor Snapshot Size**
- Snapshots grow over time (every change = new row)
- Consider archiving old snapshots
- Monitor storage costs

**5. Document Snapshot Strategy**
```yaml
version: 2

snapshots:
  - name: customers_snapshot
    description: "Historical customer data using timestamp strategy"
    config:
      strategy: timestamp
      updated_at: updated_at
    columns:
      - name: customer_id
        description: "Unique customer identifier"
      - name: dbt_valid_from
        description: "Timestamp when this version became active"
      - name: dbt_valid_to
        description: "Timestamp when this version was replaced (NULL = current)"
```

---

## 3.4 Jinja Templating in Models

### Jinja Basics

**Definition:** Jinja is a templating language that adds programming logic (loops, conditionals, variables) to SQL.

```sql
-- Basic variable
{% set payment_methods = ['credit_card', 'paypal', 'bank_transfer'] %}

SELECT
    order_id,
    -- Use variable in SQL
    CASE payment_method
        {% for method in payment_methods %}
        WHEN '{{ method }}' THEN 1
        {% endfor %}
        ELSE 0
    END AS is_valid_payment
FROM {{ ref('orders') }}
```

---

### ref() Function - Model Dependencies

**Purpose:** Reference other dbt models and build dependency graph.

```sql
-- models/marts/customer_orders.sql

WITH customers AS (
    SELECT * FROM {{ ref('stg_customers') }}
),

orders AS (
    SELECT * FROM {{ ref('stg_orders') }}
),

customer_order_summary AS (
    SELECT
        c.customer_id,
        c.first_name,
        c.last_name,
        COUNT(o.order_id) AS total_orders,
        SUM(o.order_total) AS lifetime_value
    FROM customers c
    LEFT JOIN orders o ON c.customer_id = o.customer_id
    GROUP BY 1, 2, 3
)

SELECT * FROM customer_order_summary
```

**Why use ref()?**

1. **Environment Awareness:**
```sql
-- Development: {{ ref('stg_orders') }} â†’ dev_analytics.stg_orders
-- Production: {{ ref('stg_orders') }} â†’ analytics.stg_orders
```

2. **Automatic Dependencies:**
```
stg_customers â”€â”
               â”œâ”€â†’ customer_orders
stg_orders â”€â”€â”€â”€â”˜
```

3. **Refactoring Safety:**
```sql
-- Rename model from stg_orders to staging_orders
-- Just rename the file - all refs update automatically!
{{ ref('staging_orders') }}
```

---

### source() Function - Source References

**Purpose:** Reference raw source tables loaded by EL tools.

```yaml
# models/staging/sources.yml
version: 2

sources:
  - name: raw
    database: production
    schema: raw_data
    tables:
      - name: customers
        description: "Raw customer data from Fivetran"
      - name: orders
        description: "Raw order data from Fivetran"
```

```sql
-- models/staging/stg_customers.sql

SELECT
    id AS customer_id,
    first_name,
    last_name,
    email,
    created_at
FROM {{ source('raw', 'customers') }}
```

**Compiles to:**
```sql
SELECT
    id AS customer_id,
    first_name,
    last_name,
    email,
    created_at
FROM production.raw_data.customers
```

**Benefits:**
- Document data lineage from source
- Test source freshness
- Abstract physical table names
- Enable source-level testing

---

### Conditional Logic with Jinja

```sql
{{ config(materialized='incremental') }}

SELECT
    event_id,
    user_id,
    event_type,
    event_timestamp,
    properties
FROM {{ source('events', 'clickstream') }}

{% if is_incremental() %}
    -- Incremental run: only new data
    WHERE event_timestamp > (SELECT MAX(event_timestamp) FROM {{ this }})

{% else %}
    -- Full refresh: apply filters
    WHERE event_timestamp >= '2024-01-01'

{% endif %}
```

**Common Jinja Conditionals:**
- `is_incremental()` - True if incremental model on subsequent runs
- `{{ this }}` - Reference to current model
- `target.name` - Current environment (dev/prod)
- `var('variable_name')` - Access project variables

---

### Environment-Specific Logic

```sql
-- models/marts/customer_metrics.sql

WITH customers AS (
    SELECT * FROM {{ ref('stg_customers') }}
),

orders AS (
    SELECT * FROM {{ ref('stg_orders') }}

    {% if target.name == 'dev' %}
        -- In development, only process last 30 days
        WHERE order_date >= CURRENT_DATE - 30
    {% endif %}
)

SELECT
    c.customer_id,
    COUNT(o.order_id) AS order_count,
    SUM(o.order_total) AS lifetime_value
FROM customers c
LEFT JOIN orders o ON c.customer_id = o.customer_id
GROUP BY 1
```

---

### Loops for Dynamic SQL

```sql
-- Generate UNION ALL for multiple tables

{% set tables = ['orders_2022', 'orders_2023', 'orders_2024'] %}

{% for table in tables %}
    SELECT
        '{{ table }}' AS source_table,
        *
    FROM {{ source('raw', table) }}
    {% if not loop.last %}
    UNION ALL
    {% endif %}
{% endfor %}
```

**Compiles to:**
```sql
SELECT 'orders_2022' AS source_table, * FROM raw.orders_2022
UNION ALL
SELECT 'orders_2023' AS source_table, * FROM raw.orders_2023
UNION ALL
SELECT 'orders_2024' AS source_table, * FROM raw.orders_2024
```

---

### Macros - Reusable Jinja Functions

```sql
-- macros/cents_to_dollars.sql

{% macro cents_to_dollars(column_name) %}
    ({{ column_name }} / 100.0)::numeric(10,2)
{% endmacro %}
```

```sql
-- models/staging/stg_payments.sql

SELECT
    payment_id,
    {{ cents_to_dollars('amount_cents') }} AS amount_usd,
    {{ cents_to_dollars('fee_cents') }} AS fee_usd
FROM {{ source('stripe', 'payments') }}
```

**Compiles to:**
```sql
SELECT
    payment_id,
    (amount_cents / 100.0)::numeric(10,2) AS amount_usd,
    (fee_cents / 100.0)::numeric(10,2) AS fee_usd
FROM stripe.payments
```

---

### Using dbt_utils Package

```yaml
# packages.yml
packages:
  - package: dbt-labs/dbt_utils
    version: 1.1.1
```

```bash
dbt deps  # Install packages
```

```sql
-- models/staging/stg_orders.sql

SELECT
    {{ dbt_utils.generate_surrogate_key(['order_id', 'created_at']) }} AS order_sk,
    order_id,
    customer_id,
    order_total,
    created_at
FROM {{ source('raw', 'orders') }}
```

**Common dbt_utils Macros:**
- `generate_surrogate_key()` - Create composite keys
- `pivot()` - Pivot tables dynamically
- `union_relations()` - Union multiple tables
- `get_column_values()` - Get distinct column values
- `star()` - Select all columns except specified

---

## 3.5 Model Selection and Execution

### Selection Syntax

```bash
# Run specific model
dbt run --select customer_lifetime_value

# Run all models in directory
dbt run --select staging.*

# Run model and all upstream dependencies
dbt run --select +customer_lifetime_value

# Run model and all downstream dependencies
dbt run --select customer_lifetime_value+

# Run model, upstream, and downstream
dbt run --select +customer_lifetime_value+

# Run by tag
dbt run --select tag:daily

# Exclude models
dbt run --exclude staging.*

# Run multiple selections
dbt run --select staging.* marts.finance.*
```

---

### Graph Operators

```bash
# Ancestors (upstream)
dbt run --select +customer_orders  # Runs dependencies first

# Descendants (downstream)
dbt run --select stg_customers+  # Runs dependents after

# Intersection
dbt run --select staging.*,tag:daily  # Models in staging AND tagged daily

# Union
dbt run --select staging.* tag:daily  # Models in staging OR tagged daily
```

---

### State-Based Selection

```bash
# Run only modified models
dbt run --select state:modified --state target/

# Run modified models and downstream
dbt run --select state:modified+ --state target/

# Run only new models
dbt run --select state:new --state target/
```

**Use Case:** CI/CD pipelines - only run changed models

---

## ðŸŽ¯ Best Practices

### Model Design

1. **One Model, One Purpose**
   - Each model should do one transformation well
   - Don't combine staging + business logic

2. **Layer Your Models**
   - Staging â†’ Intermediate â†’ Marts
   - Clear separation of concerns

3. **Use Descriptive Names**
   - `stg_stripe_payments` not `payments_clean`
   - `fct_orders` not `orders_final`

4. **Keep SQL Readable**
   - Use CTEs for clarity
   - Comment complex logic
   - Format consistently

### Materialization Strategy

1. **Start with Views**
   - Materialize as table only when needed
   - Profile query performance first

2. **Use Incremental for Large Datasets**
   - Event logs, clickstream, transactions
   - Test with full-refresh periodically

3. **Ephemeral for Reusable Logic**
   - Intermediate calculations
   - Models used by only one downstream model

4. **Tables for Marts**
   - Optimize for BI tool performance
   - Trade build time for query speed

### Performance

1. **Monitor Build Times**
   ```bash
   dbt run --select customer_lifetime_value --profile
   ```

2. **Use Incremental Wisely**
   - Include lookback window for late data
   - Full refresh on schedule

3. **Limit CTEs in Complex Models**
   - Break into intermediate models
   - Use ephemeral for simple logic

4. **Leverage Warehouse Features**
   - Clustering keys (Snowflake)
   - Partitioning (BigQuery)
   - Distribution styles (Redshift)

### Testing

1. **Test Primary Keys**
   ```yaml
   columns:
     - name: customer_id
       tests:
         - unique
         - not_null
   ```

2. **Test Incremental Logic**
   - Verify no duplicates
   - Confirm row counts

3. **Test Snapshot Quality**
   - Check for unexpected changes
   - Monitor table growth

---

## ðŸ”— Related Notes

- [[01. dbt Fundamentals - Introduction & Core Concepts|Previous: dbt Fundamentals]]
- [[04. dbt Testing & Documentation|Next: Testing & Documentation]]
- [[02. dbt Project Structure & Configuration|dbt Project Structure]]
- [[README|Project Overview]]

---

## ðŸ“š Key Takeaways

1. **Models are SELECT statements** - dbt handles DDL and dependencies
2. **Three layers: staging, intermediate, marts** - Clear separation of concerns
3. **Four materializations: view, table, incremental, ephemeral** - Choose based on data size and query patterns
4. **Incremental models save time** - Process only new/changed records for large datasets
5. **Snapshots track history** - Implement Type 2 SCDs for slowly changing dimensions
6. **ref() and source() build lineage** - Automatic dependency management
7. **Jinja adds power to SQL** - Conditionals, loops, and reusable macros
8. **Start simple, optimize later** - Profile before optimizing

---

**Last Updated:** 2025-10-17
**Chapter Status:** âœ… Complete
