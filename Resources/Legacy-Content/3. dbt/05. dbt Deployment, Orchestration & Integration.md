---
title: dbt Deployment, Orchestration & Integration
date: 2025-10-17
tags: [dbt, deployment, orchestration, ci-cd, airflow, prefect, dagster, production, data-engineering]
status: active
learning_phase: "Advanced Queries (Analytical)"
source: https://docs.getdbt.com/docs/deploy/deployments
---
 
**Created:** 2025-10-17
**Last Updated:** 2025-10-17
**Status:** ✅ Complete

---

## Overview

Moving dbt from development to production requires understanding environments, orchestration, CI/CD, and integration with the broader data stack. This chapter covers how to deploy dbt reliably, schedule it effectively, and integrate it with orchestration tools, BI platforms, and data catalogs.

**Key Value Proposition:** Production-ready dbt deployment ensures reliable, scheduled transformations with proper testing, monitoring, and integration across your data ecosystem.

---

## 5.1 Development vs Production Environments

### Environment Strategy

dbt projects typically have three environments:

```
Development (dev) → Staging (staging) → Production (prod)
(Local/Branch)      (PR/Testing)        (Main/Scheduled)
```

---

### Development Environment

**Purpose:** Individual developer workspace for building and testing models.

**Characteristics:**
- Runs on local machine or IDE
- Uses personal dev schema (e.g., `dbt_jonas`)
- Works on feature branches
- Fast iteration cycles
- Subset of data for speed

**Configuration:**
```yaml
# profiles.yml (local machine)
my_project:
  target: dev
  outputs:
    dev:
      type: snowflake
      account: abc123
      user: jonas.blasques
      password: "{{ env_var('DBT_PASSWORD') }}"
      role: transformer_dev
      database: analytics
      warehouse: transforming
      schema: dbt_jonas  # Personal schema
      threads: 4
```

**Typical Workflow:**
```bash
# Work on feature branch
git checkout -b feature/customer-segmentation

# Build only what you're working on
dbt run --select customer_segments

# Test changes
dbt test --select customer_segments+

# Commit when ready
git add .
git commit -m "Add customer segmentation model"
git push origin feature/customer-segmentation
```

---

### Staging Environment

**Purpose:** Pre-production testing environment that mirrors production setup.

**Characteristics:**
- Runs in CI/CD pipeline
- Uses shared staging schema
- Tests pull requests
- Validates before merge
- May use sample data or full data

**Configuration:**
```yaml
# profiles.yml (CI environment)
my_project:
  target: staging
  outputs:
    staging:
      type: snowflake
      account: abc123
      user: dbt_ci_user
      password: "{{ env_var('DBT_CI_PASSWORD') }}"
      role: transformer_staging
      database: analytics
      warehouse: transforming
      schema: staging  # Shared staging schema
      threads: 8
```

**CI/CD Workflow:**
```yaml
# .github/workflows/dbt_ci.yml
name: dbt CI

on:
  pull_request:
    branches: [main]

jobs:
  dbt-ci:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Install dbt
        run: pip install dbt-snowflake

      - name: Run dbt models (slim CI)
        run: |
          dbt run --select state:modified+ --defer --state prod-manifest/

      - name: Run dbt tests
        run: |
          dbt test --select state:modified+ --defer --state prod-manifest/

      - name: Check source freshness
        run: dbt source freshness

      - name: Generate docs
        run: dbt docs generate
```

---

### Production Environment

**Purpose:** Live environment serving business users and BI tools.

**Characteristics:**
- Runs on schedule (Airflow, dbt Cloud)
- Uses production schema
- Triggered from main branch
- Full data processing
- Monitored and alerted

**Configuration:**
```yaml
# profiles.yml (production)
my_project:
  target: prod
  outputs:
    prod:
      type: snowflake
      account: abc123
      user: dbt_prod_user
      password: "{{ env_var('DBT_PROD_PASSWORD') }}"
      role: transformer_prod
      database: analytics
      warehouse: transforming_xl
      schema: prod  # Production schema
      threads: 16  # More threads for production
```

**Production Deployment:**
```yaml
# .github/workflows/dbt_deploy.yml
name: dbt Deploy to Production

on:
  push:
    branches: [main]

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Install dbt
        run: pip install dbt-snowflake

      - name: Run dbt seed
        run: dbt seed --target prod

      - name: Run dbt snapshots
        run: dbt snapshot --target prod

      - name: Run dbt models
        run: dbt run --target prod

      - name: Run dbt tests
        run: dbt test --target prod

      - name: Save manifest for next run
        run: |
          cp target/manifest.json prod-manifest/manifest.json
          git add prod-manifest/manifest.json
          git commit -m "Update production manifest"
          git push

      - name: Send success notification
        if: success()
        run: |
          curl -X POST $SLACK_WEBHOOK \
            -d '{"text": "✅ dbt production run completed successfully"}'

      - name: Send failure notification
        if: failure()
        run: |
          curl -X POST $SLACK_WEBHOOK \
            -d '{"text": "❌ dbt production run failed. Check logs."}'
```

---

### Environment Comparison

| Aspect | Development | Staging | Production |
|--------|-------------|---------|------------|
| **Schema** | Personal (dbt_user) | Shared (staging) | Production (prod) |
| **Data** | Subset/Sample | Full or sample | Full dataset |
| **Warehouse** | Small (XS/S) | Medium (M/L) | Large (L/XL) |
| **Runs** | Manual | On PR | Scheduled |
| **Threads** | 2-4 | 4-8 | 8-16+ |
| **Who Uses** | Individual dev | CI/CD | Business users |
| **Testing** | Quick iteration | Full test suite | Full + monitoring |

---

### Environment-Specific Logic

```sql
-- models/staging/stg_orders.sql

SELECT
    order_id,
    customer_id,
    order_total,
    order_date,
    order_status

FROM {{ source('raw', 'orders') }}

{% if target.name == 'dev' %}
    -- In dev, only process recent data for speed
    WHERE order_date >= CURRENT_DATE - INTERVAL '30 days'
{% elif target.name == 'staging' %}
    -- In staging, process last 90 days
    WHERE order_date >= CURRENT_DATE - INTERVAL '90 days'
{% else %}
    -- In production, process all data
    WHERE order_date >= '2020-01-01'
{% endif %}
```

**Target Variables:**
- `target.name` - Environment name (dev, staging, prod)
- `target.schema` - Schema name
- `target.database` - Database name
- `target.type` - Warehouse type (snowflake, bigquery, etc.)
- `target.threads` - Number of threads

---

## 5.2 CI/CD Workflows

### What is CI/CD for dbt?

**Continuous Integration (CI):** Automatically test code changes before merging.

**Continuous Deployment (CD):** Automatically deploy changes to production after merge.

---

### Slim CI - Efficient Testing

**Problem:** Running all models and tests on every PR is slow and expensive.

**Solution:** Only run modified models and their downstream dependencies.

```bash
# Traditional CI (slow - runs everything)
dbt run
dbt test

# Slim CI (fast - runs only changed models)
dbt run --select state:modified+ --defer --state prod-manifest/
dbt test --select state:modified+ --defer --state prod-manifest/
```

**How it Works:**
1. Compare current PR to production manifest
2. Identify modified models
3. Run only those models + downstream dependencies
4. Defer to production for unchanged upstream models

---

### GitHub Actions Example (Comprehensive)

```yaml
# .github/workflows/dbt_ci.yml
name: dbt CI

on:
  pull_request:
    branches: [main]
    paths:
      - 'models/**'
      - 'macros/**'
      - 'tests/**'
      - 'dbt_project.yml'

env:
  DBT_PROFILES_DIR: ./
  DBT_PROJECT_DIR: ./

jobs:
  dbt-ci:
    runs-on: ubuntu-latest

    steps:
      # 1. Checkout code
      - name: Checkout code
        uses: actions/checkout@v3

      # 2. Set up Python
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      # 3. Install dbt
      - name: Install dbt
        run: |
          pip install --upgrade pip
          pip install dbt-snowflake==1.7.0

      # 4. Download production manifest
      - name: Download prod manifest
        run: |
          mkdir -p prod-manifest
          aws s3 cp s3://my-bucket/dbt/manifest.json prod-manifest/manifest.json

      # 5. Check dbt version
      - name: Check dbt version
        run: dbt --version

      # 6. Debug connection
      - name: Debug connection
        run: dbt debug --target staging
        env:
          DBT_SNOWFLAKE_PASSWORD: ${{ secrets.DBT_STAGING_PASSWORD }}

      # 7. Install dependencies
      - name: Install dbt packages
        run: dbt deps

      # 8. Run changed models (Slim CI)
      - name: Run dbt models (Slim CI)
        run: |
          dbt run \
            --select state:modified+ \
            --defer \
            --state prod-manifest/ \
            --target staging \
            --vars '{"is_ci": true}'
        env:
          DBT_SNOWFLAKE_PASSWORD: ${{ secrets.DBT_STAGING_PASSWORD }}

      # 9. Test changed models
      - name: Run dbt tests
        run: |
          dbt test \
            --select state:modified+ \
            --defer \
            --state prod-manifest/ \
            --target staging
        env:
          DBT_SNOWFLAKE_PASSWORD: ${{ secrets.DBT_STAGING_PASSWORD }}

      # 10. Check source freshness
      - name: Check source freshness
        run: dbt source freshness --target staging
        env:
          DBT_SNOWFLAKE_PASSWORD: ${{ secrets.DBT_STAGING_PASSWORD }}

      # 11. Generate documentation
      - name: Generate docs
        run: dbt docs generate --target staging
        env:
          DBT_SNOWFLAKE_PASSWORD: ${{ secrets.DBT_STAGING_PASSWORD }}

      # 12. Upload docs as artifact
      - name: Upload docs
        uses: actions/upload-artifact@v3
        with:
          name: dbt-docs
          path: target/

      # 13. Post results to PR
      - name: Comment PR with results
        uses: actions/github-script@v6
        if: always()
        with:
          script: |
            const output = `
            ## dbt CI Results
            - ✅ Models ran successfully
            - ✅ Tests passed
            - 📊 [View documentation](https://github.com/${{github.repository}}/actions/runs/${{github.run_id}})
            `;
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: output
            })
```

---

### Production Deployment Workflow

```yaml
# .github/workflows/dbt_deploy.yml
name: dbt Production Deployment

on:
  push:
    branches: [main]
  schedule:
    # Run daily at 2 AM UTC
    - cron: '0 2 * * *'

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install dbt
        run: pip install dbt-snowflake==1.7.0

      - name: Install dependencies
        run: dbt deps

      # Run in correct order
      - name: Load seed data
        run: dbt seed --target prod
        env:
          DBT_SNOWFLAKE_PASSWORD: ${{ secrets.DBT_PROD_PASSWORD }}

      - name: Run snapshots
        run: dbt snapshot --target prod
        env:
          DBT_SNOWFLAKE_PASSWORD: ${{ secrets.DBT_PROD_PASSWORD }}

      - name: Run models
        run: dbt run --target prod
        env:
          DBT_SNOWFLAKE_PASSWORD: ${{ secrets.DBT_PROD_PASSWORD }}

      - name: Run tests
        run: dbt test --target prod
        env:
          DBT_SNOWFLAKE_PASSWORD: ${{ secrets.DBT_PROD_PASSWORD }}

      # Save artifacts
      - name: Upload manifest
        run: |
          aws s3 cp target/manifest.json s3://my-bucket/dbt/manifest.json
          aws s3 cp target/run_results.json s3://my-bucket/dbt/run_results.json

      # Alert on failure
      - name: Notify on failure
        if: failure()
        run: |
          curl -X POST ${{ secrets.SLACK_WEBHOOK }} \
            -H 'Content-Type: application/json' \
            -d '{
              "text": "❌ dbt production run failed",
              "blocks": [
                {
                  "type": "section",
                  "text": {
                    "type": "mrkdwn",
                    "text": "*dbt Production Run Failed*\n<https://github.com/${{github.repository}}/actions/runs/${{github.run_id}}|View Logs>"
                  }
                }
              ]
            }'
```

---

## 5.3 Orchestration with Airflow

### Why Orchestrate dbt?

**Challenges without orchestration:**
- Manual scheduling
- No dependency management with other jobs
- Limited monitoring
- No retry logic

**Benefits with orchestration:**
- Scheduled runs (hourly, daily, weekly)
- Dependencies on upstream jobs (e.g., wait for Fivetran)
- Monitoring and alerting
- Retry on failure
- Historical run tracking

---

### dbt + Airflow Integration

```python
# dags/dbt_daily_run.py

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.providers.slack.operators.slack_webhook import SlackWebhookOperator

default_args = {
    'owner': 'analytics',
    'depends_on_past': False,
    'email': ['analytics@company.com'],
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
    'execution_timeout': timedelta(hours=2)
}

with DAG(
    'dbt_daily_run',
    default_args=default_args,
    description='Daily dbt transformation pipeline',
    schedule_interval='0 2 * * *',  # 2 AM daily
    start_date=datetime(2024, 1, 1),
    catchup=False,
    tags=['dbt', 'analytics', 'daily']
) as dag:

    # Task 1: Check source freshness
    check_sources = BashOperator(
        task_id='check_source_freshness',
        bash_command='cd /opt/dbt && dbt source freshness --target prod',
        env={
            'DBT_SNOWFLAKE_PASSWORD': '{{ var.value.dbt_prod_password }}'
        }
    )

    # Task 2: Run snapshots
    run_snapshots = BashOperator(
        task_id='run_snapshots',
        bash_command='cd /opt/dbt && dbt snapshot --target prod',
        env={
            'DBT_SNOWFLAKE_PASSWORD': '{{ var.value.dbt_prod_password }}'
        }
    )

    # Task 3: Run staging models
    run_staging = BashOperator(
        task_id='run_staging_models',
        bash_command='cd /opt/dbt && dbt run --select staging.* --target prod',
        env={
            'DBT_SNOWFLAKE_PASSWORD': '{{ var.value.dbt_prod_password }}'
        }
    )

    # Task 4: Run intermediate models
    run_intermediate = BashOperator(
        task_id='run_intermediate_models',
        bash_command='cd /opt/dbt && dbt run --select intermediate.* --target prod',
        env={
            'DBT_SNOWFLAKE_PASSWORD': '{{ var.value.dbt_prod_password }}'
        }
    )

    # Task 5: Run marts models
    run_marts = BashOperator(
        task_id='run_marts_models',
        bash_command='cd /opt/dbt && dbt run --select marts.* --target prod',
        env={
            'DBT_SNOWFLAKE_PASSWORD': '{{ var.value.dbt_prod_password }}'
        }
    )

    # Task 6: Run tests
    run_tests = BashOperator(
        task_id='run_tests',
        bash_command='cd /opt/dbt && dbt test --target prod',
        env={
            'DBT_SNOWFLAKE_PASSWORD': '{{ var.value.dbt_prod_password }}'
        }
    )

    # Task 7: Generate documentation
    generate_docs = BashOperator(
        task_id='generate_docs',
        bash_command='cd /opt/dbt && dbt docs generate --target prod',
        env={
            'DBT_SNOWFLAKE_PASSWORD': '{{ var.value.dbt_prod_password }}'
        }
    )

    # Task 8: Notify success
    notify_success = SlackWebhookOperator(
        task_id='notify_success',
        http_conn_id='slack_webhook',
        message='✅ dbt daily run completed successfully',
        channel='#data-alerts'
    )

    # Define task dependencies
    check_sources >> run_snapshots >> run_staging >> run_intermediate >> run_marts >> run_tests >> generate_docs >> notify_success
```

---

### Using Cosmos (Astronomer's dbt Integration)

**Cosmos** generates Airflow tasks automatically from your dbt project.

```python
# dags/dbt_cosmos_dag.py

from datetime import datetime
from airflow import DAG
from cosmos import DbtDag, ProjectConfig, ProfileConfig, ExecutionConfig
from cosmos.profiles import SnowflakeUserPasswordProfileMapping

profile_config = ProfileConfig(
    profile_name="my_project",
    target_name="prod",
    profile_mapping=SnowflakeUserPasswordProfileMapping(
        conn_id="snowflake_prod",
        profile_args={
            "database": "analytics",
            "schema": "prod"
        }
    )
)

# Automatically creates tasks for every model
dbt_dag = DbtDag(
    project_config=ProjectConfig("/opt/dbt"),
    profile_config=profile_config,
    execution_config=ExecutionConfig(
        dbt_executable_path="/usr/local/bin/dbt"
    ),
    schedule_interval="0 2 * * *",
    start_date=datetime(2024, 1, 1),
    catchup=False,
    dag_id="dbt_cosmos_dag"
)
```

**Benefits of Cosmos:**
- Auto-generates tasks from dbt DAG
- Model-level granularity in Airflow
- Automatic retry at model level
- Better observability

---

## 5.4 Orchestration with Prefect

### dbt + Prefect Integration

```python
# flows/dbt_flow.py

from prefect import flow, task
from prefect.blocks.system import Secret
from prefect_dbt.cli import DbtCliProfile, DbtCoreOperation
from prefect_dbt.cli.commands import trigger_dbt_cli_command
import subprocess

@task(name="Check dbt source freshness", retries=2)
def check_source_freshness():
    """Check that source data is fresh"""
    result = trigger_dbt_cli_command(
        command="dbt source freshness --target prod",
        project_dir="/opt/dbt"
    )
    return result

@task(name="Run dbt snapshots", retries=2)
def run_snapshots():
    """Capture snapshots before running models"""
    result = trigger_dbt_cli_command(
        command="dbt snapshot --target prod",
        project_dir="/opt/dbt"
    )
    return result

@task(name="Run dbt models", retries=3)
def run_dbt_models(select: str = None):
    """Run dbt models with optional selection"""
    command = "dbt run --target prod"
    if select:
        command += f" --select {select}"

    result = trigger_dbt_cli_command(
        command=command,
        project_dir="/opt/dbt"
    )
    return result

@task(name="Run dbt tests", retries=2)
def run_dbt_tests(select: str = None):
    """Run dbt tests with optional selection"""
    command = "dbt test --target prod"
    if select:
        command += f" --select {select}"

    result = trigger_dbt_cli_command(
        command=command,
        project_dir="/opt/dbt"
    )
    return result

@task(name="Generate dbt docs")
def generate_docs():
    """Generate dbt documentation"""
    result = trigger_dbt_cli_command(
        command="dbt docs generate --target prod",
        project_dir="/opt/dbt"
    )
    return result

@flow(name="dbt Daily Run")
def dbt_daily_flow():
    """Daily dbt transformation pipeline"""

    # 1. Check source freshness
    check_source_freshness()

    # 2. Run snapshots
    run_snapshots()

    # 3. Run models by layer
    run_dbt_models(select="staging.*")
    run_dbt_models(select="intermediate.*")
    run_dbt_models(select="marts.*")

    # 4. Run all tests
    run_dbt_tests()

    # 5. Generate documentation
    generate_docs()

# Deploy the flow
if __name__ == "__main__":
    dbt_daily_flow.serve(
        name="dbt-daily-deployment",
        cron="0 2 * * *",  # 2 AM daily
        tags=["dbt", "analytics", "production"]
    )
```

---

### Prefect with dbt Cloud

```python
# flows/dbt_cloud_flow.py

from prefect import flow, task
from prefect_dbt.cloud import DbtCloudCredentials, DbtCloudJob

@task(name="Trigger dbt Cloud Job")
def trigger_dbt_cloud_job(job_id: int):
    """Trigger a dbt Cloud job and wait for completion"""

    credentials = DbtCloudCredentials.load("dbt-cloud-credentials")
    dbt_cloud_job = DbtCloudJob(
        dbt_cloud_credentials=credentials,
        job_id=job_id,
        wait_for_job_run_completion=True
    )

    run = dbt_cloud_job.trigger()
    return run

@flow(name="dbt Cloud Orchestration")
def dbt_cloud_flow():
    """Trigger dbt Cloud jobs from Prefect"""

    # Trigger staging job
    staging_run = trigger_dbt_cloud_job(job_id=12345)

    # Trigger production job (depends on staging)
    prod_run = trigger_dbt_cloud_job(job_id=67890)

    return prod_run

if __name__ == "__main__":
    dbt_cloud_flow()
```

---

## 5.5 Orchestration with Dagster

### dbt + Dagster Integration

Dagster has native dbt support through `dagster-dbt`.

```python
# assets.py

from dagster import asset, AssetExecutionContext, Output, MaterializeResult
from dagster_dbt import DbtCliResource, dbt_assets

# Point to your dbt project
dbt_project_dir = "/opt/dbt"
dbt_manifest_path = f"{dbt_project_dir}/target/manifest.json"

# Create dbt assets from manifest
@dbt_assets(manifest=dbt_manifest_path)
def my_dbt_assets(context: AssetExecutionContext, dbt: DbtCliResource):
    """
    Dagster automatically creates an asset for each dbt model.
    Each model becomes a node in the Dagster asset graph.
    """
    yield from dbt.cli(["build"], context=context).stream()

# Custom asset that depends on dbt models
@asset(deps=[my_dbt_assets])
def send_analytics_report(context):
    """Send email report after dbt models complete"""
    # Query the data warehouse
    # Generate report
    # Send email
    context.log.info("Analytics report sent")
    return Output(value=True)
```

```python
# definitions.py

from dagster import Definitions, ScheduleDefinition, define_asset_job
from dagster_dbt import DbtCliResource
from .assets import my_dbt_assets, send_analytics_report

# Define dbt resource
dbt_resource = DbtCliResource(project_dir="/opt/dbt")

# Create job that runs dbt assets
dbt_job = define_asset_job(
    name="dbt_daily_job",
    selection=[my_dbt_assets, send_analytics_report]
)

# Schedule the job
dbt_schedule = ScheduleDefinition(
    job=dbt_job,
    cron_schedule="0 2 * * *",  # 2 AM daily
)

# Define all assets, jobs, and schedules
defs = Definitions(
    assets=[my_dbt_assets, send_analytics_report],
    jobs=[dbt_job],
    schedules=[dbt_schedule],
    resources={"dbt": dbt_resource}
)
```

**Benefits of Dagster + dbt:**
- Each dbt model is a Dagster asset
- Automatic lineage from dbt → other assets
- Asset materialization tracking
- Rich metadata and logging
- Type-safe dependencies

---

## 5.6 dbt Cloud Features

### dbt Cloud Scheduler

**Built-in job scheduling** without external orchestration.

**Creating a Job:**
1. Navigate to Deploy → Jobs
2. Click "Create Job"
3. Configure:
   - Name: "Daily Production Run"
   - Environment: Production
   - Commands:
     ```
     dbt source freshness
     dbt snapshot
     dbt run
     dbt test
     ```
   - Schedule: Cron `0 2 * * *`
   - Run timeout: 2 hours

**Job Configuration:**
```yaml
# dbt_cloud.yml (configuration as code)
jobs:
  - name: "Daily Production Run"
    environment: Production
    schedule:
      cron: "0 2 * * *"
      timezone: "America/New_York"
    commands:
      - "dbt source freshness"
      - "dbt snapshot"
      - "dbt run"
      - "dbt test"
    settings:
      threads: 16
      target_name: prod
    triggers:
      on_merge: true
      github_webhook: true
```

---

### CI/CD in dbt Cloud

**Slim CI** automatically enabled for pull requests.

**Configuration:**
1. Connect GitHub/GitLab repository
2. Enable "Run on Pull Request"
3. Configure CI job:
   ```
   dbt run --select state:modified+ --defer
   dbt test --select state:modified+ --defer
   ```

**Benefits:**
- Automatic PR checks
- Run only changed models
- Defer to production for unchanged models
- Comment results on PR

---

### Monitoring and Alerting

**Job Notifications:**
```yaml
notifications:
  - type: slack
    on_success: false
    on_failure: true
    webhook_url: "https://hooks.slack.com/services/..."

  - type: email
    on_success: false
    on_failure: true
    email_addresses:
      - "analytics@company.com"

  - type: pagerduty
    on_failure: true
    integration_key: "abc123..."
```

**Run History:**
- View all historical runs
- Filter by job, status, date
- Drill into specific model runs
- View logs and compiled SQL

---

### dbt Cloud API

**Trigger jobs programmatically:**

```python
import requests
import time

# Trigger dbt Cloud job
def trigger_dbt_cloud_job(account_id, job_id, api_token):
    url = f"https://cloud.getdbt.com/api/v2/accounts/{account_id}/jobs/{job_id}/run/"
    headers = {
        "Authorization": f"Token {api_token}",
        "Content-Type": "application/json"
    }
    payload = {
        "cause": "Triggered from Python script"
    }

    response = requests.post(url, headers=headers, json=payload)
    response.raise_for_status()
    run_id = response.json()['data']['id']

    return run_id

# Wait for job completion
def wait_for_job(account_id, run_id, api_token, timeout=3600):
    url = f"https://cloud.getdbt.com/api/v2/accounts/{account_id}/runs/{run_id}/"
    headers = {"Authorization": f"Token {api_token}"}

    start_time = time.time()
    while time.time() - start_time < timeout:
        response = requests.get(url, headers=headers)
        response.raise_for_status()

        status = response.json()['data']['status']
        if status == 10:  # Success
            return True
        elif status in [20, 30]:  # Error or cancelled
            return False

        time.sleep(30)  # Check every 30 seconds

    raise TimeoutError("Job did not complete within timeout")

# Use in orchestration
run_id = trigger_dbt_cloud_job(
    account_id=12345,
    job_id=67890,
    api_token="your_api_token"
)

success = wait_for_job(
    account_id=12345,
    run_id=run_id,
    api_token="your_api_token"
)

if success:
    print("dbt Cloud job completed successfully")
else:
    print("dbt Cloud job failed")
```

---

## 5.7 Integration with BI Tools

### Looker Integration

**LookML from dbt:**

```yaml
# looker.yml (dbt project config)
models:
  my_project:
    marts:
      +meta:
        looker:
          label: "{{ this.name | replace('_', ' ') | title }}"
          group_label: "Analytics"
```

**Generate LookML:**
```bash
# Using lightdash CLI
npm install -g @lightdash/cli
lightdash generate-lookml

# Or manually sync via dbt-looker package
dbt run-operation generate_model_yaml
```

---

### Tableau Integration

**Direct connection to dbt models:**

1. Configure Tableau data source to point to prod schema
2. Use dbt docs to understand model lineage
3. Create custom SQL data sources referencing dbt models

```sql
-- Custom SQL in Tableau
SELECT * FROM {{ ref('customer_lifetime_value') }}
-- Replace with actual table name: analytics.prod.customer_lifetime_value
```

---

### Power BI Integration

**Connecting Power BI:**

1. Use "Get Data" → Database connector (Snowflake/BigQuery/etc.)
2. Point to production schema where dbt materializes tables
3. Import dbt model tables directly

**Best Practice:** Create Power BI datasets from dbt marts layer (not staging).

---

### Metabase Integration

**Metabase automatically discovers dbt models** in connected database.

**Enhanced with dbt metadata:**

```yaml
# models/marts/customer_lifetime_value.yml
version: 2

models:
  - name: customer_lifetime_value
    meta:
      metabase:
        display_name: "Customer LTV"
        description: "Customer lifetime value by cohort"
        points_of_interest: "Use for customer segmentation analysis"
```

---

## 5.8 Data Catalog Integration

### Atlan Integration

```yaml
# models/marts/customers.yml
version: 2

models:
  - name: customers
    meta:
      atlan:
        asset_type: "Dataset"
        certificate_status: "Verified"
        owners: ["analytics-team"]
        tags: ["pii", "gdpr", "customer-data"]
```

**Automatic lineage:** Atlan ingests dbt metadata and creates visual lineage.

---

### Select Star Integration

**Auto-discovery of dbt models:**
1. Connect Select Star to data warehouse
2. Select Star detects dbt models
3. Imports descriptions, lineage, and column metadata

**Popular queries surface in catalog** showing how models are used.

---

### Metaphor Data Integration

```bash
# Push dbt metadata to Metaphor
metaphor dbt extract \
  --manifest target/manifest.json \
  --catalog target/catalog.json \
  --run-results target/run_results.json
```

---

## 5.9 Monitoring and Observability

### dbt Cloud Metrics

**Built-in metrics:**
- Model run duration
- Test pass/fail rates
- Row count changes
- Source freshness

**Accessing metrics:**
- dbt Cloud dashboard
- Metadata API
- Export to monitoring tools

---

### Custom Monitoring

```sql
-- models/monitoring/model_run_stats.sql

WITH run_results AS (
    SELECT
        model_name,
        execution_time_seconds,
        rows_affected,
        status,
        run_started_at
    FROM {{ ref('dbt_run_results') }}  -- From dbt artifacts
)

SELECT
    model_name,
    AVG(execution_time_seconds) AS avg_runtime,
    MAX(execution_time_seconds) AS max_runtime,
    SUM(rows_affected) AS total_rows_processed
FROM run_results
WHERE run_started_at >= CURRENT_DATE - 30
GROUP BY model_name
HAVING avg_runtime > 300  -- Alert on models taking >5 minutes
```

---

### Monte Carlo Integration

**Data observability platform** that monitors dbt models.

```yaml
# monte_carlo.yml
monitors:
  - name: "Customer LTV Freshness"
    type: freshness
    table: analytics.prod.customer_lifetime_value
    threshold: 6 hours

  - name: "Orders Volume"
    type: volume
    table: analytics.prod.fct_orders
    threshold: 10%  # Alert if 10% change

  - name: "Revenue Schema"
    type: schema
    table: analytics.prod.fct_revenue
    alert_on_change: true
```

---

## 🎯 Best Practices

### Environment Management

1.**Separate Schemas per Environment**
   - dev: dbt_<username>
   - staging: staging
   - prod: prod

2. **Use Environment Variables**
   ```yaml
   password: "{{ env_var('DBT_PASSWORD') }}"
   ```

3. **Different Warehouse Sizes**
   - dev: XS (cost-effective)
   - staging: S/M (realistic testing)
   - prod: L/XL (performance)

### CI/CD

1. **Implement Slim CI**
   - Only run changed models
   - Defer to production
   - Faster PR feedback

2. **Run Tests in CI**
   - Block merges on test failures
   - Use severity appropriately

3. **Automate Deployments**
   - Deploy on merge to main
   - Use blue/green or canary if possible

### Orchestration

1. **Choose the Right Tool**
   - Small team: dbt Cloud Scheduler
   - Existing Airflow: Use Airflow
   - Modern stack: Dagster or Prefect

2. **Run in Correct Order**
   ```
   1. Source freshness check
   2. Snapshots
   3. Models (staging → intermediate → marts)
   4. Tests
   5. Documentation
   ```

3. **Implement Retries**
   - Retry transient failures
   - Alert on persistent failures

### Monitoring

1. **Track Key Metrics**
   - Model run duration
   - Test pass rates
   - Row count changes
   - Source freshness

2. **Set Up Alerts**
   - Slack for failures
   - PagerDuty for critical jobs
   - Email for daily summaries

3. **Monitor Costs**
   - Track warehouse usage
   - Optimize expensive models
   - Use incremental for large tables

---

## 🔗 Related Notes

- [[04. dbt Testing & Documentation|Previous: Testing & Documentation]]
- [[03. dbt Models & Materializations|dbt Models & Materializations]]
- [[10. Workflow Orchestration - Airflow, Prefect, Dagster|Python: Orchestration Tools]]
- [[README|Project Overview]]

---

## 📚 Key Takeaways

1. **Three environments: dev, staging, prod** - Each with appropriate configuration
2. **Slim CI for efficiency** - Only test changed models in pull requests
3. **Orchestration enables scheduling** - Airflow, Prefect, Dagster, or dbt Cloud
4. **dbt Cloud provides managed infrastructure** - Built-in scheduler, CI/CD, monitoring
5. **Integration with BI tools** - Looker, Tableau, Power BI, Metabase
6. **Data catalogs enhance discoverability** - Atlan, Select Star, Metaphor
7. **Monitoring prevents issues** - Track metrics, set alerts, observe trends
8. **Automate everything** - From testing to deployment to documentation
9. **Run jobs in correct order** - Freshness → snapshots → models → tests → docs
10. **Choose orchestration based on stack** - Use what your team already knows

---

**Last Updated:** 2025-10-17
**Chapter Status:** ✅ Complete
