---
title: dbt Fundamentals - Introduction & Core Concepts
date: 2025-10-17
tags: [dbt, data-transformation, ELT, analytics-engineering, data-warehouse, sql, foundation-basics]
status: active
learning_phase: "Foundation (Basics)"
source: https://docs.getdbt.com/docs/introduction
---

**Created:** 2025-10-17
**Last Updated:** 2025-10-17
**Status:** ‚úÖ Complete

---

## Overview

dbt (data build tool) is a transformation workflow that helps data teams transform data in their warehouses using SQL and software engineering best practices. It enables analytics engineers to own the "T" in ELT (Extract, Load, Transform) by writing transformations as SELECT statements that dbt turns into tables and views.

**Key Value Proposition:** dbt combines the best practices of software engineering (version control, testing, documentation, CI/CD) with the analytical power of SQL to make data transformation accessible, reliable, and collaborative.

---

## 1.1 What is dbt?

### Core Definition

**dbt (data build tool)** is an open-source command-line tool that enables data analysts and engineers to transform data in their warehouse more effectively by:
- Writing business logic with just a SQL SELECT statement
- Managing dependencies between transformations
- Testing data quality
- Documenting data models
- Deploying with confidence

---

### The ELT Paradigm Shift

Modern data pipelines follow the **ELT pattern** (Extract, Load, Transform):

```
Traditional ETL:
Extract ‚Üí Transform ‚Üí Load
(Transform happens outside warehouse)

Modern ELT:
Extract ‚Üí Load ‚Üí Transform
(Transform happens inside warehouse)
```

**Why ELT?**
- **Cloud warehouses are powerful**: Snowflake, BigQuery, Redshift can handle massive transformations
- **Raw data preservation**: Keep original data for reprocessing
- **Faster time-to-insight**: Load first, transform as needed
- **Cost-effective**: Pay for compute when you need it

**dbt's Role:** Owns the "T" (Transform) in ELT

---

### How dbt Works

```sql
-- You write this (a SELECT statement):
SELECT
    customer_id,
    SUM(order_total) AS lifetime_value
FROM {{ ref('stg_orders') }}
GROUP BY customer_id

-- dbt turns it into this:
CREATE TABLE analytics.customer_lifetime_value AS
SELECT
    customer_id,
    SUM(order_total) AS lifetime_value
FROM analytics.stg_orders
GROUP BY customer_id
```

**Key Mechanisms:**
1. You write SELECT queries (called **models**)
2. dbt handles the DDL (CREATE TABLE, CREATE VIEW)
3. dbt manages dependencies automatically using `ref()`
4. dbt runs transformations in the correct order
5. dbt tests, documents, and deploys your code

---

## 1.2 dbt Core vs dbt Cloud

### dbt Core

**Definition:** Open-source command-line tool that runs transformations locally or in CI/CD pipelines.

**Key Features:**
- ‚úÖ Free and open source
- ‚úÖ Run locally via CLI
- ‚úÖ Full control over infrastructure
- ‚úÖ Integrate with any orchestration tool (Airflow, Prefect, Dagster)
- ‚úÖ Git-based workflow
- ‚úÖ All core transformation capabilities

**Installation:**
```bash
# Install via pip
pip install dbt-core

# Install warehouse-specific adapter
pip install dbt-snowflake  # or dbt-bigquery, dbt-redshift, dbt-postgres
```

**Typical Use Cases:**
- Local development
- Custom CI/CD pipelines
- Integration with existing orchestration
- Full infrastructure control
- Cost-conscious teams

---

### dbt Cloud

**Definition:** Managed, web-based service built on top of dbt Core with additional collaboration and automation features.

**Key Features:**
- ‚úÖ Web-based IDE
- ‚úÖ Built-in scheduler
- ‚úÖ CI/CD capabilities
- ‚úÖ Role-based access control
- ‚úÖ Visual lineage graph
- ‚úÖ Metadata API
- ‚úÖ Monitoring and alerting
- ‚úÖ dbt Catalog for data discovery

**Pricing:**
- Developer Plan: Free (1 seat, limited features)
- Team Plan: $100/developer/month
- Enterprise: Custom pricing

**Typical Use Cases:**
- Teams wanting managed infrastructure
- Less DevOps overhead
- Built-in collaboration features
- Non-technical users need access
- Enterprise security requirements

---

### Feature Comparison Matrix

| Feature | dbt Core | dbt Cloud |
|---------|----------|-----------|
| **Price** | Free | Free tier + paid plans |
| **SQL Transformations** | ‚úÖ | ‚úÖ |
| **Testing & Documentation** | ‚úÖ | ‚úÖ |
| **CLI Access** | ‚úÖ | ‚úÖ |
| **Web IDE** | ‚ùå | ‚úÖ |
| **Scheduler** | Manual/External | ‚úÖ Built-in |
| **CI/CD** | Manual Setup | ‚úÖ Built-in |
| **Lineage Graph** | CLI only | ‚úÖ Interactive UI |
| **Collaboration** | Git-based | ‚úÖ Enhanced |
| **RBAC** | ‚ùå | ‚úÖ |
| **Metadata API** | ‚ùå | ‚úÖ |
| **Infrastructure** | You manage | Fully managed |

---

### Decision Framework: Core vs Cloud

**Choose dbt Core when:**
- Budget is tight (it's free!)
- You have strong DevOps/engineering team
- You already have orchestration (Airflow, Prefect, Dagster)
- You need full infrastructure control
- You want to avoid vendor lock-in

**Choose dbt Cloud when:**
- You want managed infrastructure
- You need built-in scheduling
- You want less DevOps overhead
- You need role-based access controls
- Your team includes less technical users
- You want native CI/CD without setup

**Hybrid Approach:**
Many teams use **dbt Core for development** and **dbt Cloud for production** - getting the best of both worlds.

---

## 1.3 Key Concepts & Terminology

### Models

**Definition:** A model is a single SQL file containing a SELECT statement that defines a transformation. Each model typically creates one table or view in your warehouse.

```sql
-- models/staging/stg_customers.sql
SELECT
    id AS customer_id,
    UPPER(first_name) AS first_name,
    UPPER(last_name) AS last_name,
    email,
    created_at
FROM {{ source('raw', 'customers') }}
```

**Key Points:**
- One model = one .sql file = one table/view
- Models reference each other using `{{ ref('model_name') }}`
- Models are organized in directories
- Models are executed in dependency order

---

### Materializations

**Definition:** How dbt creates your model in the warehouse (table, view, incremental, ephemeral).

```yaml
# dbt_project.yml
models:
  my_project:
    staging:
      +materialized: view
    marts:
      +materialized: table
```

**Available Materializations:**

| Type | Description | Use Case | Rebuild Time |
|------|-------------|----------|--------------|
| **view** | Creates a view | Fast builds, small queries | Instant |
| **table** | Creates a table | Query performance | Full rebuild |
| **incremental** | Appends/updates only new records | Large datasets | Fast |
| **ephemeral** | CTE (doesn't create object) | Intermediate logic | N/A |

---

### Sources

**Definition:** Raw tables loaded into your warehouse by EL tools (Fivetran, Stitch, Airbyte).

```yaml
# models/staging/sources.yml
version: 2

sources:
  - name: raw
    database: production
    schema: raw_data
    tables:
      - name: customers
      - name: orders
```

```sql
-- Reference in models
SELECT * FROM {{ source('raw', 'customers') }}
```

**Benefits:**
- Document where data comes from
- Test freshness of source data
- Generate lineage from source ‚Üí model
- Decouple from physical table names

---

### ref() Function

**Definition:** Reference other models in your project to build dependencies.

```sql
-- models/marts/customer_lifetime_value.sql
SELECT
    c.customer_id,
    c.first_name,
    c.last_name,
    SUM(o.order_total) AS lifetime_value
FROM {{ ref('stg_customers') }} c
LEFT JOIN {{ ref('stg_orders') }} o
    ON c.customer_id = o.customer_id
GROUP BY c.customer_id, c.first_name, c.last_name
```

**Why use ref()?**
- ‚úÖ Automatically builds dependency graph
- ‚úÖ Runs models in correct order
- ‚úÖ Environment-aware (dev/staging/prod schemas)
- ‚úÖ Refactoring-friendly (rename models easily)

**Comparison:**
```sql
-- ‚ùå Bad: Hard-coded schema
SELECT * FROM analytics.stg_customers

-- ‚úÖ Good: Using ref()
SELECT * FROM {{ ref('stg_customers') }}
```

---

### Tests

**Definition:** Assertions about your data that return failing records.

```yaml
# models/staging/stg_customers.yml
version: 2

models:
  - name: stg_customers
    columns:
      - name: customer_id
        tests:
          - unique
          - not_null
      - name: email
        tests:
          - not_null
```

**Built-in Tests:**
- `unique`: Column has no duplicates
- `not_null`: Column has no NULL values
- `accepted_values`: Column only contains specified values
- `relationships`: Foreign key integrity

**Custom Tests:**
```sql
-- tests/assert_positive_order_totals.sql
SELECT *
FROM {{ ref('stg_orders') }}
WHERE order_total <= 0
```

If this query returns rows, the test fails.

---

### Documentation

**Definition:** Descriptions of your models, columns, and data lineage automatically generated by dbt.

```yaml
# models/staging/stg_customers.yml
version: 2

models:
  - name: stg_customers
    description: "Cleaned and standardized customer data from raw source"
    columns:
      - name: customer_id
        description: "Unique identifier for customer"
      - name: first_name
        description: "Customer first name (uppercased for consistency)"
      - name: email
        description: "Customer email address"
```

**Generate docs:**
```bash
dbt docs generate
dbt docs serve  # Opens browser with interactive documentation
```

**What You Get:**
- Interactive lineage graph
- Column-level documentation
- Model descriptions
- Source freshness
- Test results
- Compiled SQL

---

## 1.4 The dbt Workflow

### Development Workflow

```
1. Write SQL models
   ‚Üì
2. Define sources and tests
   ‚Üì
3. Run dbt locally
   ‚Üì
4. Review compiled SQL
   ‚Üì
5. Test data quality
   ‚Üì
6. Commit to Git
   ‚Üì
7. Open Pull Request
   ‚Üì
8. CI runs tests
   ‚Üì
9. Merge to main
   ‚Üì
10. Deploy to production
```

---

### Core dbt Commands

```bash
# Development Commands
dbt run              # Execute all models
dbt run --select model_name  # Run specific model
dbt run --select tag:daily   # Run models with tag
dbt run --models staging.*   # Run all staging models

# Testing Commands
dbt test             # Run all tests
dbt test --select model_name  # Test specific model

# Documentation Commands
dbt docs generate    # Generate documentation
dbt docs serve       # Serve docs on localhost

# Utility Commands
dbt compile          # Compile but don't run
dbt clean            # Delete compiled files
dbt debug            # Test connection to warehouse
dbt deps             # Install package dependencies
dbt seed             # Load CSV files to warehouse

# Selection Syntax
dbt run --select +model_name   # Run model and all upstream dependencies
dbt run --select model_name+   # Run model and all downstream dependencies
dbt run --select +model_name+  # Run model, upstream, and downstream
```

---

### Typical Day in the Life

**Morning:**
```bash
git pull origin main
dbt deps  # Update packages
dbt debug  # Verify connection
```

**Development:**
```bash
# Create new model
vim models/marts/customer_segments.sql

# Run just this model
dbt run --select customer_segments

# Test it
dbt test --select customer_segments

# See compiled SQL
cat target/compiled/my_project/models/marts/customer_segments.sql
```

**Before Committing:**
```bash
# Run all affected models
dbt run --select customer_segments+

# Run all tests
dbt test

# Generate docs
dbt docs generate
```

**Git Workflow:**
```bash
git checkout -b feature/customer-segments
git add models/marts/customer_segments.sql
git commit -m "Add customer segmentation model"
git push origin feature/customer-segments
# Open PR in GitHub/GitLab
```

---

## 1.5 Benefits of dbt

### For Analytics Engineers

**1. Version Control for Analytics**
- Track changes to transformations
- Collaborate using Git
- Review code with pull requests
- Roll back bad changes

**2. Testing Built-In**
- Catch data quality issues early
- Document data assumptions
- Automated testing in CI/CD
- Custom business logic tests

**3. Documentation Auto-Generated**
- No more stale documentation
- Visual lineage graphs
- Column-level descriptions
- Always up-to-date

**4. DRY (Don't Repeat Yourself)**
- Reuse code with macros
- Share logic across models
- Use packages from community
- Consistent transformations

---

### For Data Teams

**1. Collaboration**
- Shared codebase in Git
- Code review process
- Pair programming possible
- Knowledge sharing

**2. Reliability**
- Automated testing
- CI/CD pipelines
- Dependency management
- Idempotent runs

**3. Scalability**
- Incremental models for big data
- Parallel execution
- Selective runs
- Efficient refreshes

**4. Productivity**
- Focus on SQL, not orchestration
- Faster iteration cycles
- Reusable components
- Less manual work

---

### For Organizations

**1. Cost Savings**
- Efficient warehouse usage
- Incremental processing
- Avoid redundant compute
- Optimized queries

**2. Data Quality**
- Systematic testing
- Early issue detection
- Documented assumptions
- Consistent standards

**3. Compliance**
- Audit trail via Git
- Change tracking
- Access controls (dbt Cloud)
- Documentation for stakeholders

**4. Time-to-Insight**
- Faster development cycles
- Reusable patterns
- Self-service analytics
- Democratized data transformation

---

## 1.6 When to Use dbt

### Ideal Use Cases

**‚úÖ Use dbt for:**

1. **Analytical Transformations**
   - Aggregations, joins, window functions
   - Business logic (metrics, KPIs)
   - Data modeling (star schema, dimensional models)
   - Reporting layer preparation

2. **Data Quality**
   - Testing data assumptions
   - Validating business rules
   - Ensuring referential integrity
   - Monitoring data freshness

3. **Collaboration**
   - Teams working on shared data models
   - Code review processes
   - Documentation requirements
   - Reproducible workflows

4. **SQL-Based Transformations**
   - Cloud data warehouses (Snowflake, BigQuery, Redshift)
   - SQL-centric data teams
   - Analytics engineering culture

---

### When NOT to Use dbt

**‚ùå Don't use dbt for:**

1. **Data Extraction**
   - Use EL tools: Fivetran, Stitch, Airbyte
   - dbt doesn't extract from APIs or databases

2. **Data Loading**
   - Use EL tools to load into warehouse
   - dbt assumes data is already in the warehouse

3. **Orchestration (with dbt Core)**
   - Use Airflow, Prefect, Dagster
   - dbt Core doesn't schedule jobs

4. **Real-Time Streaming**
   - Use Kafka, Flink, Spark Streaming
   - dbt is for batch transformations

5. **Complex Python Logic**
   - Use Python for ML pipelines
   - dbt is primarily SQL (some Python support)

---

## 1.7 The dbt Ecosystem

### Warehouse Adapters (Official)

- **Snowflake** (dbt-snowflake)
- **BigQuery** (dbt-bigquery)
- **Redshift** (dbt-redshift)
- **Postgres** (dbt-postgres)
- **Databricks** (dbt-databricks)
- **Spark** (dbt-spark)

### Community Adapters

- **DuckDB** (local analytics)
- **ClickHouse** (OLAP)
- **Trino/Presto** (distributed SQL)
- **SQLite** (local development)

### Packages (dbt Package Hub)

```yaml
# packages.yml
packages:
  - package: dbt-labs/dbt_utils
    version: 1.1.1
  - package: dbt-labs/codegen
    version: 0.11.0
  - package: calogica/dbt_expectations
    version: 0.10.0
```

**Popular Packages:**
- **dbt_utils**: Macros for common tasks
- **dbt_expectations**: Great Expectations-style tests
- **codegen**: Auto-generate boilerplate code
- **audit_helper**: Compare model runs
- **dbt_project_evaluator**: Analyze project structure

---

### Integration Tools

**Data Catalogs:**
- Atlan
- Select Star
- Dataedo
- Metaphor Data

**Orchestration:**
- Apache Airflow (astronomer/airflow-dbt)
- Dagster (dagster-dbt)
- Prefect (prefect-dbt)

**BI Tools:**
- Looker (LookML integration)
- Tableau
- Power BI
- Metabase

---

## üéØ Best Practices

### 1. Project Organization
- Use staging ‚Üí intermediate ‚Üí marts layers
- One model per file
- Consistent naming conventions
- Modular, reusable code

### 2. Model Design
- Models should do one thing well
- Use clear, descriptive names
- Document complex logic
- Keep SQL readable

### 3. Testing
- Test primary keys (unique, not_null)
- Test referential integrity
- Document assumptions as tests
- Run tests in CI/CD

### 4. Documentation
- Describe all models and columns
- Explain business logic
- Keep docs up-to-date
- Use dbt docs generate

### 5. Performance
- Start with views, move to tables as needed
- Use incremental for large datasets
- Optimize SQL queries
- Monitor warehouse costs

---

## üîó Related Notes

- [[02. dbt Project Structure & Configuration|Next: Chapter 2 - Project Structure]]
- [[10. Workflow Orchestration - Airflow, Prefect, Dagster|Python: Orchestration Tools]]
- [[README|Project Overview]]

---

## üìö Key Takeaways

1. **dbt owns the "T" in ELT** - Transform data in your warehouse using SQL
2. **dbt Core is free and open source** - dbt Cloud adds managed features
3. **Models are SELECT statements** - dbt handles DDL and dependencies
4. **Testing and documentation are first-class** - Built into the workflow
5. **Software engineering meets analytics** - Git, CI/CD, code review for data
6. **Use ref() for dependencies** - Automatic lineage and execution order
7. **Materializations control persistence** - Views, tables, incremental, ephemeral
8. **dbt doesn't do EL** - Use dedicated tools for extraction and loading

---

**Last Updated:** 2025-10-17
**Chapter Status:** ‚úÖ Complete
