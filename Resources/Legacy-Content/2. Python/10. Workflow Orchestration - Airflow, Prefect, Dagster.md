---
title: Workflow Orchestration - Airflow, Prefect, Dagster
date: 2025-10-16
tags: [python, orchestration, airflow, prefect, dagster, workflows, data-engineering, scheduling]
status: active
learning_phase: "Advanced (Orchestration)"
---

**Created:** 2025-10-16
**Last Updated:** 2025-10-16
**Status:** ✅ Complete

---

## Overview

**Workflow orchestration coordinates and schedules data pipelines** - It manages dependencies, handles failures, provides monitoring, and ensures tasks run in the correct order. This note compares the three leading Python orchestration frameworks.

**Key Question:** Which orchestration tool fits your team's needs?

---

## What is Workflow Orchestration?

### The Challenge

```
Manual Pipeline Execution:
- Run script1.py → Wait → Check logs → Run script2.py → Repeat

Problems:
❌ Manual intervention required
❌ No dependency management
❌ No retry logic
❌ No monitoring/alerting
❌ No schedule management
❌ Doesn't scale
```

### The Solution: Orchestration

```
Orchestrated Pipeline:
✅ Automated scheduling (every day at 2 AM)
✅ Dependency graphs (task B waits for task A)
✅ Automatic retries on failure
✅ Monitoring dashboard
✅ Alert notifications
✅ Parallel execution
✅ Resource management
```

---

## The Three Leading Tools

### Quick Comparison

| Feature | Airflow | Prefect | Dagster |
|---------|---------|---------|---------|
| **Release Year** | 2015 | 2018 | 2019 |
| **Maturity** | Most mature | Growing | Modern |
| **Learning Curve** | Steep | Gentle | Moderate |
| **Architecture** | Centralized | Hybrid/Decentralized | Modular |
| **Best For** | Complex workflows | Rapid development | Data-first teams |
| **Community** | Largest | Growing | Focused |
| **Cloud Offering** | MWAA, Astronomer | Prefect Cloud | Dagster Cloud |
| **dbt Integration** | Good | Good | Excellent |

---

## 1. Apache Airflow - The Industry Standard

### Overview

**Airflow** is the de-facto standard orchestrator since 2015. Uses DAG-based workflows with a pluggable operator ecosystem.

**Architecture:**
- Centralized scheduler
- Metadata database (Postgres/MySQL)
- Distributed workers (Celery, Kubernetes, LocalExecutor)
- Web UI for monitoring

---

### Basic Airflow DAG

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from datetime import datetime, timedelta

# Define default arguments
default_args = {
    'owner': 'data-team',
    'depends_on_past': False,
    'email': ['alerts@example.com'],
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 3,
    'retry_delay': timedelta(minutes=5),
}

# Define DAG
dag = DAG(
    'customer_etl_pipeline',
    default_args=default_args,
    description='Daily customer data ETL',
    schedule_interval='0 2 * * *',  # Run at 2 AM daily
    start_date=datetime(2024, 1, 1),
    catchup=False,
    tags=['etl', 'customers'],
)

# Define tasks
def extract_customers(**context):
    """Extract customer data from database"""
    import pandas as pd
    # Extraction logic
    df = pd.read_sql("SELECT * FROM customers", conn)
    # Save to temp location
    df.to_csv('/tmp/customers_raw.csv', index=False)
    return '/tmp/customers_raw.csv'

def transform_customers(**context):
    """Clean and transform customer data"""
    import pandas as pd
    # Get file from previous task
    ti = context['ti']
    input_file = ti.xcom_pull(task_ids='extract')

    df = pd.read_csv(input_file)
    # Transformation logic
    df = df.dropna()
    df = df.drop_duplicates()
    df.to_csv('/tmp/customers_clean.csv', index=False)
    return '/tmp/customers_clean.csv'

def load_customers(**context):
    """Load customer data to warehouse"""
    import pandas as pd
    ti = context['ti']
    input_file = ti.xcom_pull(task_ids='transform')

    df = pd.read_csv(input_file)
    # Load logic
    df.to_sql('customers_clean', warehouse_conn, if_exists='replace')

# Create task instances
extract_task = PythonOperator(
    task_id='extract',
    python_callable=extract_customers,
    dag=dag,
)

transform_task = PythonOperator(
    task_id='transform',
    python_callable=transform_customers,
    dag=dag,
)

load_task = PythonOperator(
    task_id='load',
    python_callable=load_customers,
    dag=dag,
)

# Define dependencies
extract_task >> transform_task >> load_task
```

---

### Airflow Advanced Patterns

```python
from airflow import DAG
from airflow.operators.python import PythonOperator, BranchPythonOperator
from airflow.operators.dummy import DummyOperator
from airflow.sensors.time_delta import TimeDeltaSensor
from datetime import datetime, timedelta

dag = DAG('advanced_pipeline', start_date=datetime(2024, 1, 1))

# 1. BRANCHING - Conditional execution
def choose_branch(**context):
    """Decide which path to take"""
    if context['execution_date'].weekday() == 0:  # Monday
        return 'full_refresh'
    return 'incremental_update'

branch_task = BranchPythonOperator(
    task_id='choose_path',
    python_callable=choose_branch,
    dag=dag,
)

full_refresh = PythonOperator(
    task_id='full_refresh',
    python_callable=lambda: print("Full refresh"),
    dag=dag,
)

incremental = PythonOperator(
    task_id='incremental_update',
    python_callable=lambda: print("Incremental"),
    dag=dag,
)

# 2. SENSOR - Wait for external condition
wait_for_file = TimeDeltaSensor(
    task_id='wait_for_upstream',
    delta=timedelta(hours=1),  # Wait up to 1 hour
    dag=dag,
)

# 3. DYNAMIC TASK GENERATION
def process_partition(partition_id):
    print(f"Processing partition {partition_id}")

# Generate tasks dynamically
for i in range(5):
    task = PythonOperator(
        task_id=f'process_partition_{i}',
        python_callable=process_partition,
        op_args=[i],
        dag=dag,
    )

# 4. TRIGGER RULES - Advanced dependencies
join_task = DummyOperator(
    task_id='join',
    trigger_rule='all_success',  # all_failed, one_success, etc.
    dag=dag,
)

# Dependencies
branch_task >> [full_refresh, incremental]
[full_refresh, incremental] >> join_task
```

---

### When to Choose Airflow

**✅ Choose Airflow when:**
- You have complex, heterogeneous workloads (Spark, dbt, APIs)
- You need the largest operator ecosystem
- Your organization already uses Airflow
- You need strict governance and change management
- You have DevOps resources to manage infrastructure

**❌ Avoid Airflow if:**
- You want rapid prototyping
- Your team is small (< 5 people)
- You don't want to manage infrastructure
- You need more modern developer experience

---

## 2. Prefect - Developer-First Orchestration

### Overview

**Prefect** emphasizes simplicity and developer experience. Decouples orchestration from execution, making it very flexible.

**Architecture:**
- Prefect Cloud (SaaS) or Prefect Server (self-hosted)
- Agents poll for work
- Execute flows anywhere (local, Docker, Kubernetes, ECS)
- No scheduler network ingress needed

---

### Basic Prefect Flow

```python
from prefect import flow, task
from prefect.task_runners import ConcurrentTaskRunner
from datetime import timedelta
import pandas as pd

@task(retries=3, retry_delay_seconds=60)
def extract_customers():
    """Extract customer data"""
    df = pd.read_sql("SELECT * FROM customers", conn)
    return df

@task
def transform_customers(df: pd.DataFrame) -> pd.DataFrame:
    """Clean and transform data"""
    df = df.dropna()
    df = df.drop_duplicates()
    return df

@task
def load_customers(df: pd.DataFrame):
    """Load to warehouse"""
    df.to_sql('customers_clean', warehouse_conn, if_exists='replace')
    return len(df)

@flow(name="customer-etl", task_runner=ConcurrentTaskRunner())
def customer_etl_pipeline():
    """Main ETL flow"""
    # Extract
    raw_data = extract_customers()

    # Transform
    clean_data = transform_customers(raw_data)

    # Load
    row_count = load_customers(clean_data)

    return row_count

# Run the flow
if __name__ == "__main__":
    result = customer_etl_pipeline()
    print(f"Loaded {result} rows")
```

---

### Prefect Advanced Features

```python
from prefect import flow, task
from prefect.tasks import exponential_backoff
from prefect.deployments import Deployment
from prefect.server.schemas.schedules import CronSchedule
import httpx

# 1. RETRY WITH BACKOFF
@task(retries=5, retry_delay_seconds=exponential_backoff(backoff_factor=10))
async def fetch_api_data(url: str):
    """Fetch data with exponential backoff retry"""
    async with httpx.AsyncClient() as client:
        response = await client.get(url)
        response.raise_for_status()
        return response.json()

# 2. PARALLEL EXECUTION WITH MAP
@task
def process_customer(customer_id: int):
    """Process single customer"""
    # Processing logic
    return f"Processed customer {customer_id}"

@flow
def parallel_customer_processing():
    """Process customers in parallel"""
    customer_ids = [1, 2, 3, 4, 5]

    # Map automatically parallelizes
    results = process_customer.map(customer_ids)

    return results

# 3. CONDITIONAL LOGIC
@task
def should_run_full_refresh(date) -> bool:
    """Check if full refresh is needed"""
    return date.weekday() == 0  # Monday

@flow
def conditional_etl():
    """ETL with conditional logic"""
    from datetime import datetime

    today = datetime.now()
    full_refresh = should_run_full_refresh(today)

    if full_refresh:
        # Run full refresh
        print("Running full refresh")
    else:
        # Run incremental
        print("Running incremental")

# 4. SUBFLOWS - Modular workflows
@flow
def extract_flow():
    """Sub-flow for extraction"""
    return extract_customers()

@flow
def transform_flow(data):
    """Sub-flow for transformation"""
    return transform_customers(data)

@flow
def main_etl_flow():
    """Compose sub-flows"""
    data = extract_flow()
    clean_data = transform_flow(data)
    load_customers(clean_data)

# 5. SCHEDULING
deployment = Deployment.build_from_flow(
    flow=customer_etl_pipeline,
    name="customer-etl-daily",
    schedule=CronSchedule(cron="0 2 * * *"),  # 2 AM daily
    tags=["production", "etl"],
)
deployment.apply()
```

---

### When to Choose Prefect

**✅ Choose Prefect when:**
- You want rapid iteration and development
- You need flexible deployment (hybrid/on-prem/cloud)
- You prefer Python-native workflows
- You want to avoid managing stateful services
- Your team is small to medium-sized

**❌ Avoid Prefect if:**
- You need the largest operator ecosystem (Airflow wins)
- Your organization requires strict governance
- You need heavy dbt integration (Dagster better)

---

## 3. Dagster - Asset-Centric Orchestration

### Overview

**Dagster** focuses on data assets and lineage. Designed for data-first teams, especially dbt users. Emphasizes testing and modularity.

**Architecture:**
- Dagster Daemon (schedulers/sensors)
- User code deployments (gRPC servers)
- Dagit UI for visualization
- Asset-based paradigm (not just tasks)

---

### Basic Dagster Assets

```python
from dagster import asset, AssetExecutionContext, Definitions
import pandas as pd

@asset
def customers_raw(context: AssetExecutionContext) -> pd.DataFrame:
    """Extract raw customer data from source"""
    context.log.info("Extracting customers from database")
    df = pd.read_sql("SELECT * FROM customers", conn)
    return df

@asset
def customers_clean(context: AssetExecutionContext, customers_raw: pd.DataFrame) -> pd.DataFrame:
    """Clean and transform customer data"""
    context.log.info(f"Transforming {len(customers_raw)} customers")

    # Clean data
    df = customers_raw.dropna()
    df = df.drop_duplicates()

    context.log.info(f"After cleaning: {len(df)} customers")
    return df

@asset
def customers_enriched(context: AssetExecutionContext, customers_clean: pd.DataFrame) -> pd.DataFrame:
    """Enrich customer data with additional fields"""
    df = customers_clean.copy()

    # Add calculated fields
    df['age_group'] = pd.cut(df['age'], bins=[0, 18, 30, 50, 100],
                              labels=['0-18', '19-30', '31-50', '51+'])
    df['balance_tier'] = pd.cut(df['balance'], bins=[0, 100, 500, float('inf')],
                                 labels=['low', 'medium', 'high'])

    return df

@asset
def customer_warehouse_table(customers_enriched: pd.DataFrame) -> None:
    """Load enriched data to warehouse"""
    customers_enriched.to_sql('customers', warehouse_conn, if_exists='replace')

# Define all assets
defs = Definitions(
    assets=[customers_raw, customers_clean, customers_enriched, customer_warehouse_table]
)
```

---

### Dagster Advanced Features

```python
from dagster import (
    asset, AssetExecutionContext, Definitions,
    ScheduleDefinition, DailyPartitionsDefinition,
    AssetIn, Output, MetadataValue
)
import pandas as pd

# 1. PARTITIONED ASSETS
daily_partitions = DailyPartitionsDefinition(start_date="2024-01-01")

@asset(partitions_def=daily_partitions)
def daily_orders(context: AssetExecutionContext) -> pd.DataFrame:
    """Orders partitioned by day"""
    partition_key = context.partition_key
    query = f"SELECT * FROM orders WHERE date = '{partition_key}'"
    return pd.read_sql(query, conn)

# 2. ASSET DEPENDENCIES WITH METADATA
@asset(
    ins={"customers": AssetIn(key="customers_clean")},
    metadata={
        "owner": "data-team",
        "sla_minutes": 60,
        "data_quality_checks": ["no_nulls", "unique_ids"]
    }
)
def customer_analytics(customers: pd.DataFrame) -> Output[pd.DataFrame]:
    """Generate customer analytics with metadata"""
    analytics = customers.groupby('age_group').agg({
        'customer_id': 'count',
        'balance': ['mean', 'sum']
    })

    # Return with metadata
    return Output(
        analytics,
        metadata={
            "num_rows": len(analytics),
            "total_customers": len(customers),
            "preview": MetadataValue.md(analytics.head().to_markdown())
        }
    )

# 3. SENSORS - Event-driven execution
from dagster import sensor, RunRequest, SkipReason
import os

@sensor(job=customer_pipeline_job)
def new_file_sensor():
    """Trigger pipeline when new file appears"""
    file_path = "/data/incoming/customers.csv"

    if os.path.exists(file_path):
        return RunRequest(
            run_key=str(os.path.getmtime(file_path)),
            run_config={"file_path": file_path}
        )
    return SkipReason("No new file detected")

# 4. SCHEDULES
daily_schedule = ScheduleDefinition(
    job=customer_pipeline_job,
    cron_schedule="0 2 * * *",  # 2 AM daily
)

# 5. RESOURCES - Dependency injection
from dagster import resource, ResourceDefinition

@resource
def database_connection(context):
    """Database connection resource"""
    return create_db_connection(
        host=context.resource_config["host"],
        database=context.resource_config["database"]
    )

@asset(required_resource_keys={"database"})
def customers_from_db(context: AssetExecutionContext):
    """Use database resource"""
    conn = context.resources.database
    return pd.read_sql("SELECT * FROM customers", conn)

# Define everything
defs = Definitions(
    assets=[daily_orders, customer_analytics, customers_from_db],
    schedules=[daily_schedule],
    sensors=[new_file_sensor],
    resources={"database": database_connection}
)
```

---

### When to Choose Dagster

**✅ Choose Dagster when:**
- You prioritize data lineage and asset tracking
- You heavily use dbt (best integration)
- Your team values testing and modularity
- You want strong type checking
- You care about data asset visualization

**❌ Avoid Dagster if:**
- You need the widest ecosystem (Airflow)
- You want the simplest API (Prefect)
- Your workflows aren't data-centric

---

## Side-by-Side Comparison

### Hello World Example

**Airflow:**
```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime

def hello():
    print("Hello World")

dag = DAG('hello', start_date=datetime(2024, 1, 1))
task = PythonOperator(task_id='hello', python_callable=hello, dag=dag)
```

**Prefect:**
```python
from prefect import flow, task

@task
def hello():
    print("Hello World")

@flow
def hello_flow():
    hello()

hello_flow()
```

**Dagster:**
```python
from dagster import asset, Definitions

@asset
def hello():
    print("Hello World")

defs = Definitions(assets=[hello])
```

---

### Decision Matrix

| Scenario | Best Choice | Why |
|----------|-------------|-----|
| Startup, small team | **Prefect** | Simplest, least overhead |
| Enterprise, complex pipelines | **Airflow** | Most mature, largest ecosystem |
| Heavy dbt users | **Dagster** | Best dbt integration |
| Need asset lineage | **Dagster** | Asset-centric design |
| Hybrid/on-prem deployment | **Prefect** | Best deployment flexibility |
| Existing Airflow team | **Airflow** | Leverage existing knowledge |
| Rapid prototyping | **Prefect** | Fastest development |
| Type-safe workflows | **Dagster** | Strong typing support |

---

## Pricing Comparison (2025)

**Airflow:**
- Open-source: Free
- MWAA (AWS): ~$0.49/hr (~$350/month minimum)
- Astronomer: Contact sales
- Self-hosted: Infrastructure costs + DevOps time

**Prefect:**
- Open-source server: Free
- Prefect Cloud: Usage-based (task-run credits)
- Starter: ~$250/month
- Pro: Custom pricing

**Dagster:**
- Open-source: Free
- Dagster Cloud Hybrid: ~$0.50/compute hour
- Dagster Cloud Serverless: Usage-based
- Starter: Contact sales

---

## Migration Considerations

### From Airflow to Prefect

```python
# Airflow
from airflow import DAG
from airflow.operators.python import PythonOperator

dag = DAG('pipeline', ...)
task = PythonOperator(task_id='task', python_callable=fn, dag=dag)

# Prefect equivalent
from prefect import flow, task

@task
def fn():
    pass

@flow
def pipeline():
    fn()
```

### From Airflow to Dagster

```python
# Airflow task becomes Dagster asset
# Focus on data assets, not just tasks
@asset
def data_asset():
    return result
```

---

## 🎯 Best Practices (All Tools)

### Universal Principles

1. **Idempotency** - Tasks should produce same result if run multiple times
2. **Small tasks** - Break work into manageable units
3. **Clear dependencies** - Explicit is better than implicit
4. **Error handling** - Expect failures, handle gracefully
5. **Logging** - Comprehensive logging for debugging
6. **Testing** - Test pipelines before production
7. **Monitoring** - Track success/failure rates
8. **Documentation** - Document pipeline purpose and dependencies

---

## 🔗 Related Notes

- [[07. Testing Data Pipelines with Python|Chapter 7 - Testing]]
- [[08. Logging & Monitoring for Data Pipelines|Chapter 8 - Logging]]
- [[06. Python for Data Transformation|Chapter 6 - Transformation]]
- [[README|Project Overview]]

---

## 📚 Key Takeaways

1. **Three main orchestrators** - Airflow (mature), Prefect (flexible), Dagster (data-first)
2. **Airflow for enterprises** - Most mature, largest ecosystem, complex setups
3. **Prefect for simplicity** - Easy development, flexible deployment
4. **Dagster for data teams** - Asset-centric, strong dbt integration, lineage
5. **All handle scheduling** - Cron schedules, dependencies, retries
6. **Choose based on team** - Size, skills, requirements matter more than features
7. **Migration possible** - Not locked in, can migrate between tools
8. **Start simple** - Don't over-engineer, grow into orchestration needs

---

**Last Updated:** 2025-10-16
**Status:** ✅ Complete
