---
title: Data Validation & Quality Assurance
date: 2025-10-16
tags: [python, data-validation, data-quality, pydantic, pandera, great-expectations, schema-validation, data-engineering]
status: active
learning_phase: "Best Practices (Quality)"
---

**Created:** 2025-10-16
**Last Updated:** 2025-10-16
**Status:** ✅ Complete

---

## Overview

**Data validation ensures data meets expected standards before processing** - It prevents bad data from propagating through pipelines and causing downstream issues. This note covers comprehensive validation strategies using modern Python tools.

**Key Principle:** Validate early, validate often. Catch data quality issues at ingestion, not at analysis.

---

## Why Data Validation Matters

### The Cost of Bad Data

```
Bad Input → Pipeline Processing → Bad Output → Wrong Decisions
```

**Common data quality issues:**
- Null values in required fields
- Duplicate records
- Invalid data types (string instead of number)
- Values outside acceptable ranges
- Schema mismatches
- Data drift over time

### Benefits of Validation

1. **Early error detection** - Catch issues at ingestion
2. **Data quality guarantees** - Ensure data meets standards
3. **Pipeline reliability** - Prevent failures downstream
4. **Documentation** - Schemas document expected structure
5. **Debugging** - Pinpoint exactly what's wrong

---

## Data Validation Landscape

### Tool Comparison

| Tool | Best For | Complexity | Integration |
|------|----------|------------|-------------|
| **Pydantic** | API validation, object schemas | Low | FastAPI, general Python |
| **Pandera** | DataFrame validation, statistical tests | Medium | pandas, polars, Spark |
| **Great Expectations** | Production pipelines, automated systems | High | Airflow, dbt, orchestration |

**Rule of Thumb:**
- **API/Input validation** → Pydantic
- **DataFrame validation** → Pandera
- **Production-grade pipelines** → Great Expectations

---

## 1. Pydantic - Schema Validation for Objects

### Core Concept

**Pydantic** validates Python objects using type hints. Perfect for API inputs, configuration files, and structured data records.

---

### Basic Pydantic Models

```python
from pydantic import BaseModel, Field, validator
from typing import Optional
from datetime import date

class Customer(BaseModel):
    """Customer data model with validation"""
    customer_id: int = Field(..., gt=0, description="Positive integer ID")
    name: str = Field(..., min_length=1, max_length=100)
    email: str
    age: Optional[int] = Field(None, ge=0, le=120)
    balance: float = Field(default=0.0, ge=0.0)
    signup_date: date
    status: str = Field(..., pattern="^(active|inactive|suspended)$")

    @validator('email')
    def validate_email(cls, v):
        """Custom email validation"""
        if '@' not in v:
            raise ValueError('Invalid email format')
        return v.lower()  # Normalize to lowercase

    @validator('name')
    def validate_name(cls, v):
        """Ensure name is title case"""
        return v.strip().title()

# ✅ Valid data
try:
    customer = Customer(
        customer_id=123,
        name="john doe",
        email="John.Doe@Example.com",
        age=30,
        balance=100.50,
        signup_date="2024-01-15",
        status="active"
    )
    print(customer)
    # customer_id=123 name='John Doe' email='john.doe@example.com' ...
except Exception as e:
    print(f"Validation error: {e}")

# ❌ Invalid data
try:
    invalid_customer = Customer(
        customer_id=-1,  # Negative ID
        name="",  # Empty name
        email="invalid-email",  # No @
        age=150,  # Too old
        balance=-50,  # Negative balance
        signup_date="2024-01-15",
        status="unknown"  # Invalid status
    )
except Exception as e:
    print(f"Validation error: {e}")
    # ValidationError: multiple validation errors...
```

---

### Validating Lists of Records

```python
from pydantic import BaseModel, ValidationError
from typing import List
import json

class Order(BaseModel):
    order_id: int
    customer_id: int
    amount: float
    status: str

class OrderBatch(BaseModel):
    """Validate entire batch of orders"""
    orders: List[Order]
    batch_id: str
    timestamp: str

# Load and validate JSON data
json_data = '''
{
    "batch_id": "batch-001",
    "timestamp": "2024-01-15T10:30:00",
    "orders": [
        {"order_id": 1, "customer_id": 101, "amount": 99.99, "status": "completed"},
        {"order_id": 2, "customer_id": 102, "amount": 149.99, "status": "pending"}
    ]
}
'''

try:
    batch = OrderBatch.parse_raw(json_data)
    print(f"Valid batch with {len(batch.orders)} orders")
except ValidationError as e:
    print(f"Validation failed: {e.json()}")
```

---

### Validating CSV Files with Pydantic

```python
from pydantic import BaseModel, validator, ValidationError
from typing import List
import csv

class CustomerRecord(BaseModel):
    customer_id: int
    name: str
    email: str
    age: int
    balance: float

    @validator('age')
    def age_must_be_positive(cls, v):
        if v < 0:
            raise ValueError('Age must be positive')
        return v

def validate_csv_file(filepath: str) -> tuple[List[CustomerRecord], List[dict]]:
    """Validate CSV file and return valid records + errors"""
    valid_records = []
    errors = []

    with open(filepath, 'r') as f:
        reader = csv.DictReader(f)
        for row_num, row in enumerate(reader, start=1):
            try:
                # Convert types as needed
                row['customer_id'] = int(row['customer_id'])
                row['age'] = int(row['age'])
                row['balance'] = float(row['balance'])

                # Validate
                record = CustomerRecord(**row)
                valid_records.append(record)

            except (ValidationError, ValueError) as e:
                errors.append({
                    'row': row_num,
                    'data': row,
                    'error': str(e)
                })

    return valid_records, errors

# Usage
valid, errors = validate_csv_file('customers.csv')
print(f"Valid records: {len(valid)}")
print(f"Errors: {len(errors)}")

if errors:
    for error in errors[:5]:  # Show first 5 errors
        print(f"Row {error['row']}: {error['error']}")
```

---

## 2. Pandera - DataFrame Validation

### Core Concept

**Pandera** validates pandas DataFrames using declarative schemas. Supports statistical tests, column relationships, and multiple DataFrame libraries (pandas, polars, Spark).

---

### Basic Pandera Schema

```python
import pandas as pd
import pandera as pa
from pandera import Column, Check, DataFrameSchema

# Define schema
customer_schema = DataFrameSchema({
    "customer_id": Column(int, Check.greater_than(0), unique=True, nullable=False),
    "name": Column(str, Check.str_length(min_value=1, max_value=100), nullable=False),
    "email": Column(str, Check.str_contains("@"), nullable=False),
    "age": Column(int, Check.in_range(min_value=0, max_value=120), nullable=True),
    "balance": Column(float, Check.greater_than_or_equal_to(0), nullable=False),
    "signup_date": Column(pd.Timestamp, nullable=False),
    "status": Column(str, Check.isin(['active', 'inactive', 'suspended']))
}, strict=True)  # strict=True means no extra columns allowed

# Sample data
df = pd.DataFrame({
    "customer_id": [1, 2, 3],
    "name": ["Alice", "Bob", "Carol"],
    "email": ["alice@example.com", "bob@example.com", "carol@example.com"],
    "age": [25, 30, 35],
    "balance": [100.0, 200.0, 300.0],
    "signup_date": pd.to_datetime(["2024-01-01", "2024-01-02", "2024-01-03"]),
    "status": ["active", "active", "inactive"]
})

# Validate
try:
    validated_df = customer_schema.validate(df)
    print("✅ Data is valid!")
except pa.errors.SchemaError as e:
    print(f"❌ Validation failed: {e}")
```

---

### Advanced Pandera Features

```python
import pandas as pd
import pandera as pa
from pandera import Column, Check, DataFrameSchema

# Statistical checks
order_schema = DataFrameSchema({
    "order_id": Column(int, unique=True),
    "customer_id": Column(int),
    "amount": Column(
        float,
        checks=[
            Check.greater_than(0),
            Check.less_than(10000),
            Check(lambda s: s.mean() > 50, name="mean_above_50"),  # Custom check
            Check(lambda s: s.std() < 100, name="reasonable_variance")
        ]
    ),
    "order_date": Column(pd.Timestamp)
})

# Multi-column checks
def check_date_consistency(df):
    """Ensure created_at is before updated_at"""
    return (df["created_at"] <= df["updated_at"]).all()

enhanced_schema = DataFrameSchema(
    {
        "id": Column(int),
        "created_at": Column(pd.Timestamp),
        "updated_at": Column(pd.Timestamp)
    },
    checks=Check(check_date_consistency, name="date_consistency")
)

# Coerce types (automatic conversion)
flexible_schema = DataFrameSchema({
    "customer_id": Column(int, coerce=True),  # Convert "123" to 123
    "age": Column(int, coerce=True),
    "balance": Column(float, coerce=True)
})

# With coercion
df_str = pd.DataFrame({
    "customer_id": ["1", "2", "3"],  # Strings
    "age": ["25", "30", "35"],
    "balance": ["100.0", "200.0", "300.0"]
})

validated = flexible_schema.validate(df_str)
print(validated.dtypes)
# customer_id    int64
# age           int64
# balance     float64
```

---

### Pandera Class-Based API

```python
import pandera as pa
from pandera.typing import Series, DataFrame
import pandas as pd

class CustomerSchema(pa.DataFrameModel):
    """Type-safe DataFrame schema using class syntax"""

    customer_id: Series[int] = pa.Field(gt=0, unique=True)
    name: Series[str] = pa.Field(str_length={"min_value": 1, "max_value": 100})
    email: Series[str] = pa.Field(str_contains="@")
    age: Series[int] = pa.Field(in_range={"min_value": 0, "max_value": 120}, nullable=True)
    balance: Series[float] = pa.Field(ge=0.0)
    status: Series[str] = pa.Field(isin=['active', 'inactive', 'suspended'])

    class Config:
        strict = True  # No extra columns
        coerce = True  # Auto type conversion

    @pa.check("balance")
    def balance_reasonable(cls, balance: Series[float]) -> Series[bool]:
        """Custom validator: balance shouldn't be too high"""
        return balance < 1000000

# Use the schema
@pa.check_types
def process_customers(df: DataFrame[CustomerSchema]) -> DataFrame[CustomerSchema]:
    """Function with type-checked DataFrame"""
    # Process data knowing it's valid
    df = df.copy()
    df['balance_tier'] = pd.cut(
        df['balance'],
        bins=[0, 100, 500, float('inf')],
        labels=['low', 'medium', 'high']
    )
    return df

# Usage
df = pd.DataFrame({
    "customer_id": [1, 2, 3],
    "name": ["Alice", "Bob", "Carol"],
    "email": ["alice@ex.com", "bob@ex.com", "carol@ex.com"],
    "age": [25, 30, 35],
    "balance": [100.0, 200.0, 300.0],
    "status": ["active", "active", "inactive"]
})

result = process_customers(df)  # Automatically validated!
```

---

### Handling Validation Errors

```python
import pandera as pa
import pandas as pd

schema = pa.DataFrameSchema({
    "customer_id": pa.Column(int, unique=True),
    "age": pa.Column(int, pa.Check.in_range(0, 120))
})

# Data with errors
df = pd.DataFrame({
    "customer_id": [1, 2, 2],  # Duplicate!
    "age": [25, 150, 30]  # Out of range!
})

try:
    schema.validate(df, lazy=True)  # lazy=True collects all errors
except pa.errors.SchemaErrors as err:
    print("Validation Errors:")
    print(err.failure_cases)
    #    schema_context column  check          check_number  failure_case
    # 0  Column        age     in_range(0,120)  1            150
    # 1  DataFrameSchema customer_id unique    2            2

    # Get invalid rows
    print("\nInvalid rows:")
    print(err.data)
```

---

## 3. Great Expectations - Production-Grade Validation

### Core Concept

**Great Expectations** is a heavyweight validation framework for production pipelines. Integrates with Airflow, dbt, and data catalogs. Best for automated, monitored validation systems.

---

### Basic Great Expectations Usage

```python
import great_expectations as gx
import pandas as pd

# Create Data Context
context = gx.get_context()

# Load data as expectation
df = pd.DataFrame({
    "customer_id": [1, 2, 3],
    "name": ["Alice", "Bob", "Carol"],
    "age": [25, 30, 35],
    "balance": [100.0, 200.0, 300.0]
})

# Validate with expectations
batch = context.sources.add_or_update_pandas("my_datasource").read_dataframe(df)

# Define expectations
batch.expect_column_values_to_not_be_null("customer_id")
batch.expect_column_values_to_be_unique("customer_id")
batch.expect_column_values_to_be_between("age", min_value=0, max_value=120)
batch.expect_column_values_to_be_in_set("status", ["active", "inactive"])
batch.expect_column_mean_to_be_between("balance", min_value=50, max_value=500)

# Run validation
results = batch.validate()

if results["success"]:
    print("✅ All expectations met!")
else:
    print("❌ Validation failed:")
    for result in results["results"]:
        if not result["success"]:
            print(f"  - {result['expectation_config']['expectation_type']}")
```

---

### Great Expectations in ETL Pipeline

```python
import great_expectations as gx
import pandas as pd

def validate_customer_data(df: pd.DataFrame) -> tuple[bool, dict]:
    """Validate customer data with Great Expectations"""

    context = gx.get_context()
    batch = context.sources.add_or_update_pandas("customers").read_dataframe(df)

    # Schema expectations
    batch.expect_table_columns_to_match_ordered_list([
        "customer_id", "name", "email", "age", "balance"
    ])

    # Column expectations
    batch.expect_column_values_to_not_be_null("customer_id")
    batch.expect_column_values_to_be_unique("customer_id")
    batch.expect_column_values_to_not_be_null("name")
    batch.expect_column_values_to_match_regex("email", r"^[^@]+@[^@]+\.[^@]+$")
    batch.expect_column_values_to_be_between("age", 0, 120)
    batch.expect_column_values_to_be_of_type("balance", "float")

    # Statistical expectations
    batch.expect_column_mean_to_be_between("balance", 0, 10000)
    batch.expect_table_row_count_to_be_between(min_value=1, max_value=1000000)

    # Run validation
    results = batch.validate()

    return results["success"], results

# Usage in ETL
df = extract_customers()
is_valid, results = validate_customer_data(df)

if is_valid:
    transform_and_load(df)
else:
    log_validation_errors(results)
    raise ValueError("Data validation failed")
```

---

## Comparison: When to Use Each Tool

### Pydantic

```python
# ✅ Best for:
# - API request/response validation
# - Configuration files
# - Single records/objects
# - FastAPI integration

from pydantic import BaseModel

class APIRequest(BaseModel):
    user_id: int
    query: str
    limit: int = 10

request = APIRequest(user_id=123, query="search term")
```

### Pandera

```python
# ✅ Best for:
# - DataFrame validation (pandas, polars, Spark)
# - Data science workflows
# - Statistical tests on data
# - Type-safe DataFrame operations

import pandera as pa

schema = pa.DataFrameSchema({
    "user_id": pa.Column(int),
    "score": pa.Column(float, pa.Check.in_range(0, 100))
})

validated_df = schema.validate(df)
```

### Great Expectations

```python
# ✅ Best for:
# - Production data pipelines
# - Automated validation systems
# - Integration with Airflow/dbt
# - Data profiling and documentation
# - Team collaboration on data quality

import great_expectations as gx

context = gx.get_context()
batch = context.sources.add_pandas("source").read_dataframe(df)
batch.expect_column_mean_to_be_between("revenue", 100, 1000)
results = batch.validate()
```

---

## Practical Validation Patterns

### Pattern 1: Multi-Stage Validation

```python
import pandas as pd
import pandera as pa
from typing import Tuple, List

def validate_pipeline_data(df: pd.DataFrame) -> Tuple[pd.DataFrame, List[str]]:
    """Multi-stage validation with detailed error reporting"""
    errors = []

    # Stage 1: Schema validation
    try:
        schema = pa.DataFrameSchema({
            "customer_id": pa.Column(int),
            "email": pa.Column(str),
            "balance": pa.Column(float)
        })
        df = schema.validate(df, lazy=True)
    except pa.errors.SchemaErrors as e:
        errors.append(f"Schema errors: {len(e.failure_cases)}")
        return df, errors

    # Stage 2: Data quality checks
    null_counts = df.isnull().sum()
    if null_counts.any():
        errors.append(f"Null values found: {null_counts[null_counts > 0].to_dict()}")

    duplicate_count = df.duplicated(subset=['customer_id']).sum()
    if duplicate_count > 0:
        errors.append(f"Duplicate customer_ids: {duplicate_count}")

    # Stage 3: Business rule validation
    negative_balance = (df['balance'] < 0).sum()
    if negative_balance > 0:
        errors.append(f"Negative balances: {negative_balance}")

    invalid_emails = ~df['email'].str.contains('@', na=False)
    if invalid_emails.any():
        errors.append(f"Invalid emails: {invalid_emails.sum()}")

    # Stage 4: Statistical validation
    if df['balance'].mean() < 0:
        errors.append("Average balance is negative")

    return df, errors

# Usage
df = pd.read_csv('customers.csv')
validated_df, validation_errors = validate_pipeline_data(df)

if validation_errors:
    print("Validation warnings/errors:")
    for error in validation_errors:
        print(f"  - {error}")
    # Decide: continue with warnings or fail?
else:
    print("All validations passed!")
```

---

### Pattern 2: Validation with Remediation

```python
import pandas as pd
import pandera as pa

def validate_and_clean(df: pd.DataFrame) -> pd.DataFrame:
    """Validate data and attempt to fix common issues"""

    # Remove duplicates
    initial_count = len(df)
    df = df.drop_duplicates(subset=['customer_id'])
    if len(df) < initial_count:
        print(f"Removed {initial_count - len(df)} duplicate rows")

    # Fix data types
    df['customer_id'] = pd.to_numeric(df['customer_id'], errors='coerce')
    df['age'] = pd.to_numeric(df['age'], errors='coerce')
    df['balance'] = pd.to_numeric(df['balance'], errors='coerce')

    # Handle nulls
    df = df.dropna(subset=['customer_id', 'email'])  # Required fields
    df['age'].fillna(0, inplace=True)  # Optional field
    df['balance'].fillna(0.0, inplace=True)

    # Normalize strings
    df['email'] = df['email'].str.lower().str.strip()
    df['name'] = df['name'].str.strip().str.title()

    # Validate after cleaning
    schema = pa.DataFrameSchema({
        "customer_id": pa.Column(int, unique=True, nullable=False),
        "name": pa.Column(str, nullable=False),
        "email": pa.Column(str, pa.Check.str_contains("@")),
        "age": pa.Column(int, pa.Check.greater_than_or_equal_to(0)),
        "balance": pa.Column(float, pa.Check.greater_than_or_equal_to(0))
    })

    return schema.validate(df)

# Usage
raw_df = pd.read_csv('messy_customers.csv')
clean_df = validate_and_clean(raw_df)
```

---

## 🎯 Best Practices

### Validation Strategy

1. **Validate at boundaries** - When data enters/exits your system
2. **Fail fast** - Validate before expensive operations
3. **Schema versioning** - Track schema changes over time
4. **Document expectations** - Schemas are documentation
5. **Monitor validation failures** - Track patterns over time

### Error Handling

```python
# ✅ Good: Collect all errors, report clearly
try:
    schema.validate(df, lazy=True)  # Collect all errors
except pa.errors.SchemaErrors as e:
    print(f"Found {len(e.failure_cases)} validation errors")
    print(e.failure_cases)
    # Log to monitoring system
    raise

# ❌ Bad: Fail on first error, unclear message
schema.validate(df)  # Stops at first error
```

### Schema Evolution

```python
# Version schemas to handle changes
class CustomerSchemaV1(pa.DataFrameModel):
    customer_id: Series[int]
    name: Series[str]
    email: Series[str]

class CustomerSchemaV2(pa.DataFrameModel):
    customer_id: Series[int]
    name: Series[str]
    email: Series[str]
    phone: Series[str] = pa.Field(nullable=True)  # New field

def validate_customer_data(df: pd.DataFrame, version: int = 2):
    schema = CustomerSchemaV2 if version == 2 else CustomerSchemaV1
    return schema.validate(df)
```

---

## 🔗 Related Notes

- [[07. Testing Data Pipelines with Python|Chapter 7 - Testing]]
- [[08. Logging & Monitoring for Data Pipelines|Chapter 8 - Logging & Monitoring]]
- [[06. Python for Data Transformation|Chapter 6 - Data Transformation]]
- [[README|Project Overview]]

---

## 📚 Key Takeaways

1. **Three main tools** - Pydantic (objects), Pandera (DataFrames), Great Expectations (production)
2. **Validate at boundaries** - When data enters/exits your control
3. **Pydantic for APIs** - Type-safe validation for structured objects
4. **Pandera for DataFrames** - Statistical tests and schema enforcement
5. **Great Expectations for production** - Automated, monitored, integrated
6. **Fail fast** - Validate before expensive operations
7. **Document with schemas** - Schemas are living documentation
8. **Monitor failures** - Track validation patterns over time

---

**Last Updated:** 2025-10-16
**Status:** ✅ Complete
