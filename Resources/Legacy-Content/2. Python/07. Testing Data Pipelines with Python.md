---
title: Testing Data Pipelines with Python
date: 2025-10-16
tags: [python, testing, pytest, unit-tests, integration-tests, data-quality, data-engineering, best-practices]
status: active
learning_phase: "Best Practices (Testing)"
---

**Created:** 2025-10-16
**Last Updated:** 2025-10-16
**Status:** âœ… Complete

---

## Overview

**Testing is crucial for production-ready data pipelines** - It ensures data quality, catches bugs early, and enables confident deployments. This note covers comprehensive testing strategies for data engineering projects.

**Key Philosophy:** Start with end-to-end tests (test outputs first), then move inward to unit tests. Having system testing, data quality checks, and monitoring in place gives you confidence to move fast and deliver quality data.

---

## Why Test Data Pipelines?

### The Cost of Bad Data

```
Bad Data â†’ Wrong Insights â†’ Bad Decisions â†’ Business Impact
```

**Common pipeline failures:**
- Schema changes breaking downstream processes
- Data quality issues (nulls, duplicates, outliers)
- Logic errors in transformations
- Integration failures between systems
- Performance degradation over time

### Benefits of Testing

1. **Early bug detection** - Catch issues before production
2. **Confident refactoring** - Change code without fear
3. **Documentation** - Tests show how code should work
4. **Faster debugging** - Identify exactly what broke
5. **CI/CD enablement** - Automated deployment gates

---

## Types of Tests for Data Pipelines

### Test Pyramid for Data Engineering

```
           /\
          /  \      End-to-End Tests (Fewer, Slower)
         /    \     Integration Tests (Moderate)
        /______\    Unit Tests (Many, Fast)
       /        \   Data Quality Tests (Continuous)
      /__________\
```

---

## 1. Unit Tests - Test Individual Components

### Core Concept

**Unit tests** validate individual functions/components in isolation. They're fast, deterministic, and focused on single functionality.

**Characteristics:**
- Test one logical unit at a time
- No external dependencies (databases, APIs)
- Fast execution (< 1 second per test)
- Use mock data
- Run frequently during development

---

### pytest Basics

```python
# test_transformations.py
import pytest

def clean_customer_name(name):
    """Remove extra whitespace and title case"""
    if not name:
        return ""
    return ' '.join(name.strip().split()).title()

# Test function (pytest automatically discovers tests_ files)
def test_clean_customer_name():
    # Arrange
    raw_name = "  john   DOE  "

    # Act
    result = clean_customer_name(raw_name)

    # Assert
    assert result == "John Doe"

def test_clean_customer_name_empty():
    assert clean_customer_name("") == ""
    assert clean_customer_name(None) == ""

def test_clean_customer_name_single_word():
    assert clean_customer_name("alice") == "Alice"
```

**Run tests:**
```bash
pytest test_transformations.py
pytest -v  # Verbose output
pytest -k "clean"  # Run tests matching pattern
pytest --cov  # Show code coverage
```

---

### pytest Fixtures - Reusable Test Data

```python
import pytest
import pandas as pd

# Fixture: Setup that can be shared across tests
@pytest.fixture
def sample_customers():
    """Provide sample customer data for testing"""
    return pd.DataFrame({
        'customer_id': [1, 2, 3, 4],
        'name': ['Alice', 'Bob', 'Carol', 'Dave'],
        'age': [25, 30, 35, None],
        'balance': [100.0, 200.0, None, 400.0]
    })

@pytest.fixture
def sample_orders():
    """Provide sample order data"""
    return pd.DataFrame({
        'order_id': [101, 102, 103],
        'customer_id': [1, 1, 2],
        'amount': [50.0, 75.0, 120.0]
    })

# Use fixtures in tests
def test_remove_nulls(sample_customers):
    result = sample_customers.dropna()
    assert len(result) == 2  # Only 2 rows have no nulls
    assert result['customer_id'].tolist() == [1, 2]

def test_customer_order_join(sample_customers, sample_orders):
    result = sample_customers.merge(
        sample_orders,
        on='customer_id',
        how='left'
    )
    assert len(result) == 7  # Customer 1 has 2 orders
    assert result['order_id'].notna().sum() == 3
```

---

### Parametrized Tests - Multiple Test Cases

```python
import pytest

def calculate_discount(price, discount_pct):
    """Calculate discounted price"""
    if discount_pct < 0 or discount_pct > 100:
        raise ValueError("Discount must be between 0 and 100")
    return price * (1 - discount_pct / 100)

# Test multiple scenarios with one function
@pytest.mark.parametrize("price,discount,expected", [
    (100, 10, 90),      # 10% off
    (100, 0, 100),      # No discount
    (100, 100, 0),      # 100% off
    (50, 20, 40),       # 20% off
    (75.50, 15, 64.175) # Decimal amounts
])
def test_calculate_discount(price, discount, expected):
    result = calculate_discount(price, discount)
    assert result == pytest.approx(expected)

# Test error conditions
@pytest.mark.parametrize("price,discount", [
    (100, -10),   # Negative discount
    (100, 150),   # Over 100%
])
def test_calculate_discount_invalid(price, discount):
    with pytest.raises(ValueError):
        calculate_discount(price, discount)
```

---

### Testing Data Transformations

```python
import pandas as pd
import pytest

def enrich_customer_data(df):
    """Add calculated columns to customer data"""
    df = df.copy()

    # Age group
    df['age_group'] = pd.cut(
        df['age'],
        bins=[0, 18, 30, 50, 100],
        labels=['0-18', '19-30', '31-50', '51+']
    )

    # Balance tier
    df['balance_tier'] = df['balance'].apply(
        lambda x: 'high' if x >= 500 else 'medium' if x >= 100 else 'low'
    )

    # Name length
    df['name_length'] = df['name'].str.len()

    return df

@pytest.fixture
def input_data():
    return pd.DataFrame({
        'customer_id': [1, 2, 3, 4],
        'name': ['Alice', 'Bob', 'Charlotte', 'Dan'],
        'age': [25, 35, 45, 65],
        'balance': [50, 150, 600, 300]
    })

def test_enrich_customer_data_columns_created(input_data):
    """Test that all expected columns are created"""
    result = enrich_customer_data(input_data)

    expected_columns = ['customer_id', 'name', 'age', 'balance',
                       'age_group', 'balance_tier', 'name_length']
    assert list(result.columns) == expected_columns

def test_enrich_customer_data_age_groups(input_data):
    """Test age group categorization"""
    result = enrich_customer_data(input_data)

    expected_age_groups = ['19-30', '31-50', '31-50', '51+']
    assert result['age_group'].astype(str).tolist() == expected_age_groups

def test_enrich_customer_data_balance_tiers(input_data):
    """Test balance tier logic"""
    result = enrich_customer_data(input_data)

    assert result.loc[0, 'balance_tier'] == 'low'      # 50
    assert result.loc[1, 'balance_tier'] == 'medium'   # 150
    assert result.loc[2, 'balance_tier'] == 'high'     # 600
    assert result.loc[3, 'balance_tier'] == 'medium'   # 300

def test_enrich_customer_data_name_length(input_data):
    """Test name length calculation"""
    result = enrich_customer_data(input_data)

    assert result.loc[0, 'name_length'] == 5   # Alice
    assert result.loc[2, 'name_length'] == 9   # Charlotte

def test_enrich_customer_data_preserves_original_data(input_data):
    """Test that function doesn't mutate input"""
    original = input_data.copy()
    _ = enrich_customer_data(input_data)

    pd.testing.assert_frame_equal(input_data, original)
```

---

### Mocking External Dependencies

```python
import pytest
from unittest.mock import Mock, patch
import requests

def fetch_customer_from_api(customer_id):
    """Fetch customer data from external API"""
    response = requests.get(f"https://api.example.com/customers/{customer_id}")
    response.raise_for_status()
    return response.json()

# Mock the API call
@patch('requests.get')
def test_fetch_customer_from_api_success(mock_get):
    # Arrange: Configure mock response
    mock_response = Mock()
    mock_response.json.return_value = {
        'customer_id': 123,
        'name': 'Alice Johnson',
        'email': 'alice@example.com'
    }
    mock_response.raise_for_status.return_value = None
    mock_get.return_value = mock_response

    # Act
    result = fetch_customer_from_api(123)

    # Assert
    assert result['customer_id'] == 123
    assert result['name'] == 'Alice Johnson'
    mock_get.assert_called_once_with('https://api.example.com/customers/123')

@patch('requests.get')
def test_fetch_customer_from_api_failure(mock_get):
    # Arrange: Mock API failure
    mock_get.side_effect = requests.exceptions.HTTPError("404 Not Found")

    # Act & Assert
    with pytest.raises(requests.exceptions.HTTPError):
        fetch_customer_from_api(999)
```

---

## 2. Integration Tests - Test Component Interactions

### Core Concept

**Integration tests** verify that different components work together correctly. They test interfaces between systems.

**Characteristics:**
- Test multiple components together
- May use test databases/APIs
- Slower than unit tests
- Test real interactions
- Validate data flow

---

### Testing Database Interactions

```python
import pytest
import sqlite3
import pandas as pd

@pytest.fixture
def test_db():
    """Create in-memory test database"""
    conn = sqlite3.connect(':memory:')

    # Create test table
    conn.execute("""
        CREATE TABLE customers (
            customer_id INTEGER PRIMARY KEY,
            name TEXT NOT NULL,
            email TEXT,
            balance REAL
        )
    """)

    # Insert test data
    conn.execute("""
        INSERT INTO customers VALUES
        (1, 'Alice', 'alice@example.com', 100.0),
        (2, 'Bob', 'bob@example.com', 200.0),
        (3, 'Carol', 'carol@example.com', 300.0)
    """)
    conn.commit()

    yield conn

    conn.close()

def load_customers_from_db(conn, min_balance=0):
    """Load customers with balance above threshold"""
    query = """
        SELECT customer_id, name, email, balance
        FROM customers
        WHERE balance >= ?
        ORDER BY balance DESC
    """
    return pd.read_sql_query(query, conn, params=(min_balance,))

def test_load_customers_from_db_all(test_db):
    """Test loading all customers"""
    result = load_customers_from_db(test_db)

    assert len(result) == 3
    assert result['customer_id'].tolist() == [3, 2, 1]  # Ordered by balance DESC

def test_load_customers_from_db_filtered(test_db):
    """Test loading with balance filter"""
    result = load_customers_from_db(test_db, min_balance=150)

    assert len(result) == 2
    assert result['name'].tolist() == ['Carol', 'Bob']

def test_load_customers_from_db_no_results(test_db):
    """Test when no customers meet criteria"""
    result = load_customers_from_db(test_db, min_balance=1000)

    assert len(result) == 0
```

---

### Testing ETL Pipelines End-to-End

```python
import pytest
import pandas as pd
from pathlib import Path
import tempfile

def run_etl_pipeline(input_path, output_path):
    """Simple ETL: Extract, Transform, Load"""
    # Extract
    df = pd.read_csv(input_path)

    # Transform
    df = df.dropna()
    df = df.drop_duplicates(subset=['customer_id'])
    df['name'] = df['name'].str.title()
    df['age_group'] = pd.cut(df['age'], bins=[0, 30, 50, 100], labels=['young', 'middle', 'senior'])

    # Load
    df.to_csv(output_path, index=False)

    return len(df)

@pytest.fixture
def input_csv(tmp_path):
    """Create temporary input CSV"""
    csv_path = tmp_path / "input.csv"

    df = pd.DataFrame({
        'customer_id': [1, 2, 2, 3, 4],  # Has duplicate
        'name': ['alice', 'BOB', 'BOB', 'carol', 'dave'],
        'age': [25, 35, 35, 45, 65],
        'balance': [100, 200, 200, None, 400]  # Has null
    })

    df.to_csv(csv_path, index=False)
    return csv_path

def test_etl_pipeline_integration(input_csv, tmp_path):
    """Test complete ETL pipeline"""
    output_path = tmp_path / "output.csv"

    # Run pipeline
    row_count = run_etl_pipeline(input_csv, output_path)

    # Verify output exists
    assert output_path.exists()

    # Load and verify results
    result = pd.read_csv(output_path)

    # Should have 3 rows (removed null and duplicate)
    assert len(result) == 3
    assert row_count == 3

    # Names should be title case
    assert result['name'].tolist() == ['Alice', 'Bob', 'Dave']

    # No duplicates
    assert result['customer_id'].nunique() == 3

    # No nulls
    assert result.isna().sum().sum() == 0

    # Age groups created
    assert 'age_group' in result.columns
    assert result['age_group'].tolist() == ['young', 'middle', 'senior']
```

---

## 3. Data Quality Tests - Validate Data Properties

### Core Concept

**Data quality tests** validate data characteristics: completeness, accuracy, consistency, uniqueness, and timeliness.

---

### Basic Data Quality Checks

```python
import pandas as pd
import pytest

def validate_customer_data_quality(df):
    """Run data quality checks on customer data"""
    errors = []

    # 1. Completeness: Required fields not null
    required_fields = ['customer_id', 'name', 'email']
    for field in required_fields:
        null_count = df[field].isna().sum()
        if null_count > 0:
            errors.append(f"{field} has {null_count} null values")

    # 2. Uniqueness: No duplicate customer IDs
    duplicate_count = df['customer_id'].duplicated().sum()
    if duplicate_count > 0:
        errors.append(f"Found {duplicate_count} duplicate customer_ids")

    # 3. Validity: Age in reasonable range
    invalid_ages = df[(df['age'] < 0) | (df['age'] > 120)]['age'].count()
    if invalid_ages > 0:
        errors.append(f"Found {invalid_ages} customers with invalid ages")

    # 4. Validity: Email format
    email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
    invalid_emails = ~df['email'].str.match(email_pattern, na=False)
    if invalid_emails.sum() > 0:
        errors.append(f"Found {invalid_emails.sum()} invalid email formats")

    # 5. Consistency: Balance non-negative
    negative_balance = df[df['balance'] < 0]['balance'].count()
    if negative_balance > 0:
        errors.append(f"Found {negative_balance} customers with negative balance")

    return errors

def test_data_quality_good_data():
    """Test with clean data"""
    df = pd.DataFrame({
        'customer_id': [1, 2, 3],
        'name': ['Alice', 'Bob', 'Carol'],
        'email': ['alice@example.com', 'bob@example.com', 'carol@example.com'],
        'age': [25, 35, 45],
        'balance': [100, 200, 300]
    })

    errors = validate_customer_data_quality(df)
    assert len(errors) == 0, f"Expected no errors, got: {errors}"

def test_data_quality_missing_values():
    """Test detection of missing values"""
    df = pd.DataFrame({
        'customer_id': [1, 2, None],
        'name': ['Alice', None, 'Carol'],
        'email': ['alice@example.com', 'bob@example.com', None],
        'age': [25, 35, 45],
        'balance': [100, 200, 300]
    })

    errors = validate_customer_data_quality(df)
    assert len(errors) == 3  # customer_id, name, email nulls
    assert any('customer_id' in e for e in errors)
    assert any('name' in e for e in errors)
    assert any('email' in e for e in errors)

def test_data_quality_invalid_values():
    """Test detection of invalid values"""
    df = pd.DataFrame({
        'customer_id': [1, 2, 2],  # Duplicate
        'name': ['Alice', 'Bob', 'Carol'],
        'email': ['alice@example.com', 'invalid-email', 'carol@example.com'],
        'age': [25, -5, 150],  # Invalid ages
        'balance': [100, -50, 300]  # Negative balance
    })

    errors = validate_customer_data_quality(df)
    assert len(errors) == 4  # duplicates, invalid email, invalid age, negative balance
```

---

### Statistical Data Quality Tests

```python
import pandas as pd
import pytest

def test_revenue_distribution():
    """Test that revenue follows expected distribution"""
    df = pd.DataFrame({
        'revenue': [100, 150, 200, 180, 190, 210, 175, 165]
    })

    # Mean should be around 171
    assert 160 <= df['revenue'].mean() <= 180

    # Standard deviation should be reasonable
    assert df['revenue'].std() < 50

    # No extreme outliers (> 3 std devs from mean)
    mean = df['revenue'].mean()
    std = df['revenue'].std()
    outliers = df[(df['revenue'] < mean - 3*std) | (df['revenue'] > mean + 3*std)]
    assert len(outliers) == 0

def test_expected_row_count():
    """Test that data volume is within expected range"""
    df = pd.DataFrame({'customer_id': range(1000)})

    # Expect between 900 and 1100 rows
    assert 900 <= len(df) <= 1100

def test_category_distribution():
    """Test that categories have reasonable distribution"""
    df = pd.DataFrame({
        'category': ['A'] * 100 + ['B'] * 150 + ['C'] * 120
    })

    distribution = df['category'].value_counts(normalize=True)

    # No category should dominate > 50%
    assert distribution.max() < 0.5

    # No category should be < 20%
    assert distribution.min() > 0.2
```

---

## 4. End-to-End Tests - Complete System Validation

### Core Concept

**E2E tests** validate entire workflows from start to finish, testing the system as users/processes interact with it.

```python
import pytest
import pandas as pd
from pathlib import Path

def complete_pipeline(raw_data_path, output_path):
    """Complete data pipeline: Extract, Clean, Transform, Aggregate, Load"""
    # Extract
    df = pd.read_csv(raw_data_path)

    # Clean
    df = df.dropna(subset=['customer_id', 'order_date'])
    df = df.drop_duplicates()

    # Transform
    df['order_date'] = pd.to_datetime(df['order_date'])
    df['year'] = df['order_date'].dt.year
    df['month'] = df['order_date'].dt.month

    # Aggregate
    summary = df.groupby(['customer_id', 'year', 'month']).agg({
        'order_id': 'count',
        'amount': ['sum', 'mean', 'max']
    }).reset_index()

    summary.columns = ['customer_id', 'year', 'month', 'order_count',
                       'total_amount', 'avg_amount', 'max_amount']

    # Load
    summary.to_csv(output_path, index=False)

    return summary

@pytest.fixture
def raw_orders(tmp_path):
    """Create raw order data"""
    csv_path = tmp_path / "raw_orders.csv"

    df = pd.DataFrame({
        'order_id': [1, 2, 3, 4, 5, 6],
        'customer_id': [101, 101, 102, 102, 103, None],  # One null
        'amount': [50, 75, 100, 120, 200, 150],
        'order_date': ['2024-01-15', '2024-01-20', '2024-01-18',
                      '2024-02-01', '2024-02-05', '2024-02-10']
    })

    df.to_csv(csv_path, index=False)
    return csv_path

def test_complete_pipeline_e2e(raw_orders, tmp_path):
    """Test entire pipeline end-to-end"""
    output_path = tmp_path / "summary.csv"

    # Run pipeline
    result = complete_pipeline(raw_orders, output_path)

    # Verify file created
    assert output_path.exists()

    # Verify transformations
    assert len(result) == 3  # 3 customer-month combinations (excluding null)

    # Check customer 101 in January
    customer_101_jan = result[
        (result['customer_id'] == 101) &
        (result['year'] == 2024) &
        (result['month'] == 1)
    ]
    assert len(customer_101_jan) == 1
    assert customer_101_jan['order_count'].iloc[0] == 2
    assert customer_101_jan['total_amount'].iloc[0] == 125
    assert customer_101_jan['avg_amount'].iloc[0] == 62.5

    # Verify no nulls in output
    assert result.isna().sum().sum() == 0

    # Verify output schema
    expected_columns = ['customer_id', 'year', 'month', 'order_count',
                       'total_amount', 'avg_amount', 'max_amount']
    assert list(result.columns) == expected_columns
```

---

## Test Organization Best Practices

### Directory Structure

```
project/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ extractors/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ api_extractor.py
â”‚   â”œâ”€â”€ transformers/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ data_cleaner.py
â”‚   â””â”€â”€ loaders/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ db_loader.py
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ conftest.py  # Shared fixtures
â”‚   â”œâ”€â”€ unit/
â”‚   â”‚   â”œâ”€â”€ test_api_extractor.py
â”‚   â”‚   â”œâ”€â”€ test_data_cleaner.py
â”‚   â”‚   â””â”€â”€ test_db_loader.py
â”‚   â”œâ”€â”€ integration/
â”‚   â”‚   â”œâ”€â”€ test_extract_transform.py
â”‚   â”‚   â””â”€â”€ test_transform_load.py
â”‚   â”œâ”€â”€ data_quality/
â”‚   â”‚   â””â”€â”€ test_data_validation.py
â”‚   â””â”€â”€ e2e/
â”‚       â””â”€â”€ test_complete_pipeline.py
â”œâ”€â”€ pytest.ini
â””â”€â”€ requirements.txt
```

---

### pytest Configuration

```ini
# pytest.ini
[pytest]
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*

# Markers for organizing tests
markers =
    unit: Unit tests (fast, isolated)
    integration: Integration tests (moderate speed)
    e2e: End-to-end tests (slow)
    quality: Data quality tests
    smoke: Quick smoke tests

# Coverage settings
addopts =
    --verbose
    --strict-markers
    --cov=src
    --cov-report=html
    --cov-report=term-missing
```

**Run specific test types:**
```bash
pytest -m unit           # Only unit tests
pytest -m integration    # Only integration tests
pytest -m "not e2e"      # Skip slow e2e tests
pytest -m smoke          # Quick smoke tests
```

---

### Shared Fixtures (conftest.py)

```python
# tests/conftest.py
import pytest
import pandas as pd
import tempfile
from pathlib import Path

@pytest.fixture(scope="session")
def test_data_dir():
    """Create temporary directory for test data"""
    with tempfile.TemporaryDirectory() as tmpdir:
        yield Path(tmpdir)

@pytest.fixture
def sample_customer_df():
    """Standard test customer data"""
    return pd.DataFrame({
        'customer_id': [1, 2, 3, 4, 5],
        'name': ['Alice', 'Bob', 'Carol', 'Dave', 'Eve'],
        'age': [25, 30, 35, 40, 45],
        'balance': [100, 200, 300, 400, 500]
    })

@pytest.fixture
def sample_orders_df():
    """Standard test order data"""
    return pd.DataFrame({
        'order_id': [101, 102, 103, 104],
        'customer_id': [1, 1, 2, 3],
        'amount': [50, 75, 120, 200],
        'order_date': pd.to_datetime(['2024-01-01', '2024-01-15',
                                      '2024-02-01', '2024-02-15'])
    })
```

---

## CI/CD Integration

### GitHub Actions Example

```yaml
# .github/workflows/tests.yml
name: Run Tests

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        pip install pytest pytest-cov

    - name: Run unit tests
      run: pytest tests/unit -v

    - name: Run integration tests
      run: pytest tests/integration -v

    - name: Run data quality tests
      run: pytest tests/data_quality -v

    - name: Generate coverage report
      run: pytest --cov=src --cov-report=xml

    - name: Upload coverage
      uses: codecov/codecov-action@v3
```

---

## ðŸŽ¯ Best Practices

### Testing Strategy

1. **Start with end-to-end** - Test outputs first, ensure system works
2. **Add unit tests** - Test individual components thoroughly
3. **Use real data samples** - Test with production-like data
4. **Test edge cases** - Nulls, duplicates, empty sets, extremes
5. **Test failures** - Ensure errors are handled gracefully

### Writing Good Tests

```python
# âœ… Good: Clear, focused, well-named
def test_customer_age_group_categorizes_young_adults_correctly():
    customer = {'age': 25}
    result = categorize_age_group(customer)
    assert result == 'young_adult'

# âŒ Bad: Vague name, tests multiple things
def test_stuff():
    # Tests 5 different things
    assert True
```

### Test Data Management

```python
# âœ… Good: Use fixtures for reusable data
@pytest.fixture
def valid_customer():
    return {'id': 1, 'name': 'Alice', 'age': 30}

def test_with_fixture(valid_customer):
    result = process_customer(valid_customer)
    assert result is not None

# âŒ Bad: Duplicate test data in every test
def test_customer_1():
    customer = {'id': 1, 'name': 'Alice', 'age': 30}
    # ...

def test_customer_2():
    customer = {'id': 1, 'name': 'Alice', 'age': 30}  # Duplicated!
    # ...
```

### Assertions

```python
# âœ… Good: Specific assertions with context
assert result['customer_count'] == 3, f"Expected 3 customers, got {result['customer_count']}"
assert 'email' in result, "Result missing email field"

# âŒ Bad: Generic assertions without context
assert result
assert len(result) > 0
```

---

## Common Testing Patterns

### Testing PySpark Code

```python
import pytest
from pyspark.sql import SparkSession
from pyspark.sql.functions import col

@pytest.fixture(scope="session")
def spark():
    """Create Spark session for testing"""
    return SparkSession.builder \
        .master("local[2]") \
        .appName("pytest-spark") \
        .getOrCreate()

def clean_customers(spark_df):
    """PySpark transformation"""
    return spark_df \
        .filter(col('age') >= 18) \
        .filter(col('balance') > 0) \
        .dropDuplicates(['customer_id'])

def test_clean_customers_pyspark(spark):
    # Create test data
    data = [
        (1, 'Alice', 25, 100),
        (2, 'Bob', 17, 50),    # Underage
        (3, 'Carol', 30, -10),  # Negative balance
        (1, 'Alice', 25, 100),  # Duplicate
    ]
    schema = ['customer_id', 'name', 'age', 'balance']
    df = spark.createDataFrame(data, schema)

    # Transform
    result = clean_customers(df)

    # Assert
    assert result.count() == 1  # Only Alice remains
    assert result.first()['customer_id'] == 1
```

---

## ðŸ”— Related Notes

- [[04. Python Basics - Data Structures & Control Flow|Chapter 4 - Python Basics]]
- [[05. Python for Data Integration - Extract & Load|Chapter 5 - Data Integration]]
- [[06. Python for Data Transformation|Chapter 6 - Data Transformation]]
- [[README|Project Overview]]

---

## ðŸ“š Key Takeaways

1. **Test pyramid** - Many unit tests, fewer integration tests, few e2e tests
2. **Start with outputs** - Test end-to-end first, then drill down
3. **pytest is powerful** - Fixtures, parametrization, markers make testing easy
4. **Data quality tests** - Validate completeness, accuracy, consistency, uniqueness
5. **Mock external systems** - Unit tests should be fast and isolated
6. **CI/CD integration** - Automate tests to catch issues early
7. **Test edge cases** - Nulls, duplicates, empty sets, extreme values
8. **Good tests are documentation** - They show how code should work

---

**Last Updated:** 2025-10-16
**Status:** âœ… Complete
