---
title: Performance Optimization & Dependency Management
date: 2025-10-16
tags: [python, performance, optimization, multiprocessing, async, poetry, dependencies, data-engineering, best-practices]
status: active
learning_phase: "Advanced (Performance)"
---

**Created:** 2025-10-16
**Last Updated:** 2025-10-16
**Status:** ✅ Complete

---

## Overview

**Performance optimization and dependency management are critical for production data engineering** - This note covers techniques to speed up Python code and manage project dependencies effectively.

**Key Principle:** Optimize first for single-CPU performance, then scale horizontally. Don't jump to multiprocessing without profiling.

---

# Part 1: Performance Optimization

## Understanding Python Performance

### The GIL (Global Interpreter Lock)

**What is it?** Python's GIL prevents multiple threads from executing Python code simultaneously.

**Impact:**
- ✅ Threading works for I/O-bound tasks (waiting for network/disk)
- ❌ Threading doesn't help CPU-bound tasks (computations)

```python
import time
import threading

# I/O-bound: Threading helps
def download_file(url):
    time.sleep(1)  # Simulates I/O wait
    return url

# CPU-bound: Threading doesn't help
def compute_fibonacci(n):
    if n <= 1:
        return n
    return compute_fibonacci(n-1) + compute_fibonacci(n-2)
```

---

## Choosing the Right Concurrency Model

### Decision Tree

```
Is your task CPU-bound (heavy computations)?
├─ YES → Use multiprocessing
└─ NO → Is it I/O-bound (network, disk)?
    ├─ YES → Use async/await or threading
    └─ Are you sure? → Profile first!
```

---

## 1. Async/Await - For I/O-Bound Tasks

### Core Concept

**Async I/O** allows a single thread to handle multiple I/O operations concurrently by switching between tasks during wait times.

**Best for:** API calls, database queries, file I/O

---

### Basic Async Example

```python
import asyncio
import httpx
import time

# ❌ Synchronous - Slow (10 seconds)
def fetch_sync(urls):
    results = []
    for url in urls:
        response = httpx.get(url)
        results.append(response.text)
    return results

# ✅ Asynchronous - Fast (1 second)
async def fetch_async(urls):
    async with httpx.AsyncClient() as client:
        tasks = [client.get(url) for url in urls]
        responses = await asyncio.gather(*tasks)
        return [r.text for r in responses]

# Usage
urls = [f"https://api.example.com/data/{i}" for i in range(10)]

# Sync: 10 seconds (sequential)
start = time.time()
results = fetch_sync(urls)
print(f"Sync took: {time.time() - start:.2f}s")

# Async: ~1 second (concurrent)
start = time.time()
results = asyncio.run(fetch_async(urls))
print(f"Async took: {time.time() - start:.2f}s")
```

---

### Async Database Queries

```python
import asyncio
import asyncpg  # PostgreSQL async driver

async def fetch_customers_async(pool, customer_ids):
    """Fetch multiple customers concurrently"""
    async def fetch_one(customer_id):
        async with pool.acquire() as conn:
            return await conn.fetchrow(
                "SELECT * FROM customers WHERE customer_id = $1",
                customer_id
            )

    # Fetch all customers concurrently
    tasks = [fetch_one(cid) for cid in customer_ids]
    results = await asyncio.gather(*tasks)
    return results

async def main():
    # Create connection pool
    pool = await asyncpg.create_pool(
        host='localhost',
        database='analytics',
        user='user',
        password='password'
    )

    customer_ids = list(range(1, 101))  # 100 customers
    results = await fetch_customers_async(pool, customer_ids)

    await pool.close()

# Run
asyncio.run(main())
```

---

## 2. Multiprocessing - For CPU-Bound Tasks

### Core Concept

**Multiprocessing** creates separate Python processes, each with its own GIL. True parallelism for CPU-intensive work.

**Best for:** Data transformations, heavy computations, parallelizable tasks

---

### Basic Multiprocessing

```python
from multiprocessing import Pool, cpu_count
import time

def process_chunk(data_chunk):
    """CPU-intensive processing"""
    # Heavy computation
    result = []
    for item in data_chunk:
        # Expensive operation
        processed = sum([i**2 for i in range(1000)])
        result.append(processed)
    return result

# ❌ Sequential - Slow
def process_sequential(data):
    return [process_chunk([item]) for item in data]

# ✅ Parallel - Fast
def process_parallel(data, num_workers=None):
    if num_workers is None:
        num_workers = cpu_count()

    # Split data into chunks
    chunk_size = len(data) // num_workers
    chunks = [data[i:i+chunk_size] for i in range(0, len(data), chunk_size)]

    # Process in parallel
    with Pool(num_workers) as pool:
        results = pool.map(process_chunk, chunks)

    # Flatten results
    return [item for chunk in results for item in chunk]

# Usage
data = list(range(1000))

# Sequential: ~10 seconds
start = time.time()
results = process_sequential(data)
print(f"Sequential: {time.time() - start:.2f}s")

# Parallel: ~2 seconds (on 8-core CPU)
start = time.time()
results = process_parallel(data)
print(f"Parallel: {time.time() - start:.2f}s")
```

---

### Multiprocessing for DataFrames

```python
from multiprocessing import Pool, cpu_count
import pandas as pd
import numpy as np

def process_partition(df_chunk):
    """Process a chunk of DataFrame"""
    # Heavy transformations
    df_chunk['calculated'] = df_chunk['value'].apply(lambda x: x**2)
    df_chunk['category'] = pd.cut(df_chunk['calculated'],
                                   bins=[0, 100, 500, 1000, float('inf')],
                                   labels=['low', 'medium', 'high', 'very_high'])
    return df_chunk

def parallel_dataframe_processing(df, num_workers=None):
    """Process DataFrame in parallel"""
    if num_workers is None:
        num_workers = cpu_count()

    # Split DataFrame into chunks
    chunks = np.array_split(df, num_workers)

    # Process chunks in parallel
    with Pool(num_workers) as pool:
        processed_chunks = pool.map(process_partition, chunks)

    # Combine results
    return pd.concat(processed_chunks, ignore_index=True)

# Usage
df = pd.DataFrame({
    'id': range(10000),
    'value': np.random.randn(10000)
})

result = parallel_dataframe_processing(df)
print(f"Processed {len(result)} rows")
```

---

### When NOT to Use Multiprocessing

```python
# ❌ Don't use for small datasets
data = [1, 2, 3, 4, 5]  # Too small, overhead > benefit

# ❌ Don't use for I/O-bound tasks
def read_files(files):
    with Pool() as pool:
        return pool.map(read_file, files)  # Use async instead!

# ❌ Don't use with large data transfers
# Serializing/deserializing data between processes is expensive
huge_df = pd.DataFrame(...)  # 10GB
# Better to use Spark/Dask for this

# ✅ DO use for CPU-bound, parallelizable work
def heavy_computation(data):
    with Pool() as pool:
        return pool.map(expensive_function, data)
```

---

## 3. Threading - For I/O-Bound Tasks (Alternative to Async)

### When to Use Threading

Threading is useful for I/O-bound tasks when:
- You can't use async (libraries don't support it)
- Simple concurrent I/O operations
- Compatibility with legacy code

```python
from concurrent.futures import ThreadPoolExecutor, as_completed
import requests

def fetch_url(url):
    """Fetch URL using requests (not async)"""
    response = requests.get(url)
    return url, len(response.text)

def fetch_all_threaded(urls, max_workers=10):
    """Fetch multiple URLs using threading"""
    results = {}

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # Submit all tasks
        future_to_url = {executor.submit(fetch_url, url): url for url in urls}

        # Process as they complete
        for future in as_completed(future_to_url):
            url = future_to_url[future]
            try:
                url, size = future.result()
                results[url] = size
            except Exception as e:
                results[url] = f"Error: {e}"

    return results

# Usage
urls = [f"https://api.example.com/data/{i}" for i in range(20)]
results = fetch_all_threaded(urls)
```

---

## 4. Performance Profiling - Measure First!

### Using cProfile

```python
import cProfile
import pstats
from pstats import SortKey

def slow_function():
    """Function to profile"""
    result = []
    for i in range(1000):
        result.append(sum([j**2 for j in range(1000)]))
    return result

# Profile the function
profiler = cProfile.Profile()
profiler.enable()

slow_function()

profiler.disable()

# Print stats
stats = pstats.Stats(profiler)
stats.sort_stats(SortKey.CUMULATIVE)
stats.print_stats(10)  # Top 10 slowest functions
```

---

### Line Profiler - Find Slow Lines

```python
# Install: pip install line_profiler

# Add @profile decorator (don't import it!)
@profile
def process_data():
    data = load_data()         # Line 1
    cleaned = clean_data(data) # Line 2
    transformed = transform(cleaned)  # Line 3
    return transformed

# Run: kernprof -l -v script.py
# Output shows time per line:
# Line #  Hits  Time     Per Hit  % Time  Line Contents
# 1       1     2.3      2.3      60.0    data = load_data()
# 2       1     0.8      0.8      20.0    cleaned = clean_data()
# 3       1     0.8      0.8      20.0    transformed = transform()
```

---

## 5. Optimization Techniques

### 1. Use Built-in Functions (They're C-optimized)

```python
import time

data = list(range(1000000))

# ❌ Slow: Python loop
start = time.time()
total = 0
for x in data:
    total += x
print(f"Loop: {time.time() - start:.4f}s")

# ✅ Fast: Built-in sum (C-optimized)
start = time.time()
total = sum(data)
print(f"Built-in: {time.time() - start:.4f}s")  # ~10x faster!
```

---

### 2. Use List Comprehensions

```python
# ❌ Slower: append in loop
result = []
for x in range(1000):
    result.append(x**2)

# ✅ Faster: List comprehension
result = [x**2 for x in range(1000)]  # ~20% faster

# ✅ Even faster for simple operations: map
result = list(map(lambda x: x**2, range(1000)))
```

---

### 3. Use NumPy for Numerical Operations

```python
import numpy as np
import time

# Python list operations
data = list(range(1000000))
start = time.time()
result = [x**2 for x in data]
print(f"Python list: {time.time() - start:.4f}s")

# NumPy array operations
data_np = np.array(data)
start = time.time()
result_np = data_np ** 2
print(f"NumPy: {time.time() - start:.4f}s")  # ~50x faster!
```

---

### 4. Cache Expensive Computations

```python
from functools import lru_cache
import time

# ❌ Without cache - Slow for repeated calls
def fibonacci(n):
    if n < 2:
        return n
    return fibonacci(n-1) + fibonacci(n-2)

# ✅ With cache - Much faster
@lru_cache(maxsize=None)
def fibonacci_cached(n):
    if n < 2:
        return n
    return fibonacci_cached(n-1) + fibonacci_cached(n-2)

# Without cache: ~5 seconds
start = time.time()
result = fibonacci(35)
print(f"No cache: {time.time() - start:.4f}s")

# With cache: ~0.0001 seconds
start = time.time()
result = fibonacci_cached(35)
print(f"With cache: {time.time() - start:.4f}s")  # ~50,000x faster!
```

---

### 5. Use Generators for Memory Efficiency

```python
import sys

# ❌ List: Stores everything in memory
def get_numbers_list(n):
    return [i for i in range(n)]

# ✅ Generator: Lazy evaluation
def get_numbers_generator(n):
    for i in range(n):
        yield i

# Memory comparison
list_version = get_numbers_list(1000000)
generator_version = get_numbers_generator(1000000)

print(f"List size: {sys.getsizeof(list_version)} bytes")      # ~8 MB
print(f"Generator size: {sys.getsizeof(generator_version)} bytes")  # ~200 bytes

# Use generator in pipeline
def process_large_file(filename):
    """Process file line by line without loading all into memory"""
    with open(filename) as f:
        for line in f:  # File objects are generators!
            yield process_line(line)

# Use it
for result in process_large_file('big_data.csv'):
    save_result(result)  # Processes one line at a time
```

---

# Part 2: Dependency Management

## Why Dependency Management Matters

### The Problem

```
Without proper dependency management:
❌ "Works on my machine" syndrome
❌ Version conflicts between projects
❌ Dependency hell
❌ Irreproducible builds
❌ Security vulnerabilities
❌ Difficult onboarding
```

### The Solution

```
With Poetry/pip-tools:
✅ Reproducible environments
✅ Lock files for deterministic installs
✅ Isolated project dependencies
✅ Easy collaboration
✅ Security updates tracking
✅ Simple onboarding
```

---

## Python Dependency Tools Comparison

| Tool | Best For | Lock File | Resolver |
|------|----------|-----------|----------|
| **pip** | Simple projects | ❌ No | Basic |
| **pip-tools** | Requirements management | ✅ Yes | Good |
| **Poetry** | Modern projects | ✅ Yes | Excellent |
| **Pipenv** | Legacy alternative | ✅ Yes | Slow |
| **conda** | Scientific computing | ✅ Yes | Cross-language |

**Recommendation for data engineering: Poetry**

---

## Poetry - Modern Dependency Management

### Why Poetry?

1. **pyproject.toml** - Single file for project metadata and deps
2. **poetry.lock** - Deterministic builds
3. **Virtual environment management** - Automatic creation
4. **Dependency groups** - Separate dev/test/prod dependencies
5. **Publishing** - Easy package publishing to PyPI

---

### Getting Started with Poetry

```bash
# Install Poetry
curl -sSL https://install.python-poetry.org | python3 -

# Create new project
poetry new my-data-pipeline
cd my-data-pipeline

# Or initialize existing project
cd existing-project
poetry init

# Add dependencies
poetry add pandas numpy requests

# Add dev dependencies
poetry add --group dev pytest black ruff

# Install all dependencies
poetry install

# Activate virtual environment
poetry shell

# Run command in poetry environment
poetry run python script.py
```

---

### pyproject.toml Example

```toml
[tool.poetry]
name = "data-pipeline"
version = "0.1.0"
description = "ETL pipeline for customer data"
authors = ["Jonas Blasques <jonas@example.com>"]
readme = "README.md"
python = "^3.11"

[tool.poetry.dependencies]
# Production dependencies
pandas = "^2.0.0"
numpy = "^1.26.0"
requests = "^2.31.0"
pydantic = "^2.5.0"
python-dotenv = "^1.0.0"

[tool.poetry.group.dev.dependencies]
# Development dependencies
pytest = "^7.4.0"
pytest-cov = "^4.1.0"
black = "^23.0.0"
ruff = "^0.1.0"
mypy = "^1.7.0"

[tool.poetry.group.data.dependencies]
# Optional data science dependencies
jupyterlab = "^4.0.0"
matplotlib = "^3.8.0"
seaborn = "^0.13.0"

[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"
```

---

### Poetry Commands

```bash
# Dependency management
poetry add package              # Add dependency
poetry add --group dev package  # Add dev dependency
poetry remove package           # Remove dependency
poetry update                   # Update all dependencies
poetry update package           # Update specific package
poetry show                     # List installed packages
poetry show --tree              # Show dependency tree

# Environment management
poetry env list                 # List virtual environments
poetry env use python3.11       # Use specific Python version
poetry env info                 # Show environment info
poetry shell                    # Activate virtual environment

# Project management
poetry install                  # Install all dependencies
poetry install --no-dev         # Install only production deps
poetry install --with data      # Install with optional group
poetry build                    # Build package (wheel/sdist)
poetry publish                  # Publish to PyPI

# Lock file
poetry lock                     # Update lock file
poetry lock --no-update         # Regenerate lock without updating
```

---

### Virtual Environments Best Practices

```bash
# ✅ Best practice: One environment per project
my-project/
├── .venv/              # Virtual environment
├── pyproject.toml      # Dependencies
├── poetry.lock         # Locked versions
└── src/

# Poetry creates .venv automatically in project directory
# Configure Poetry:
poetry config virtualenvs.in-project true

# ❌ Don't install into global Python
pip install pandas  # Affects all projects!

# ✅ Always use project environment
poetry add pandas   # Only affects this project
```

---

### Dependency Groups for Different Environments

```toml
[tool.poetry.group.dev.dependencies]
# Development tools
pytest = "^7.4.0"
black = "^23.0.0"
ruff = "^0.1.0"

[tool.poetry.group.test.dependencies]
# Testing dependencies
pytest-cov = "^4.1.0"
pytest-mock = "^3.12.0"

[tool.poetry.group.docs.dependencies]
# Documentation
mkdocs = "^1.5.0"
mkdocs-material = "^9.4.0"

# Install specific groups
poetry install --with test      # Install test deps
poetry install --only main      # Only production deps
poetry install --with test,docs # Multiple groups
```

---

### Managing Requirements.txt with Poetry

```bash
# Export to requirements.txt (for Docker, etc.)
poetry export -f requirements.txt --output requirements.txt

# Without dev dependencies
poetry export -f requirements.txt --output requirements.txt --without dev

# With specific groups
poetry export -f requirements.txt --output requirements.txt --with test
```

---

## Project Structure Best Practices

```
data-pipeline/
├── .venv/                    # Virtual environment (Poetry creates)
├── src/
│   └── pipeline/
│       ├── __init__.py
│       ├── extractors/
│       ├── transformers/
│       └── loaders/
├── tests/
│   ├── unit/
│   ├── integration/
│   └── conftest.py
├── notebooks/                # Jupyter notebooks
├── data/
│   ├── raw/
│   ├── processed/
│   └── output/
├── .env                      # Environment variables (not in git!)
├── .gitignore
├── pyproject.toml           # Poetry dependencies
├── poetry.lock              # Locked versions
├── README.md
└── Dockerfile
```

---

## Dockerfile with Poetry

```dockerfile
FROM python:3.11-slim

WORKDIR /app

# Install Poetry
RUN pip install poetry

# Copy dependency files
COPY pyproject.toml poetry.lock ./

# Install dependencies
# Don't create virtual env (already in container)
RUN poetry config virtualenvs.create false \
    && poetry install --no-dev --no-interaction --no-ansi

# Copy application
COPY src/ ./src/

# Run application
CMD ["python", "-m", "src.pipeline.main"]
```

---

## 🎯 Best Practices Summary

### Performance

1. **Profile first** - Don't optimize without measuring
2. **Use built-ins** - They're C-optimized
3. **Async for I/O** - Network, disk operations
4. **Multiprocessing for CPU** - Heavy computations
5. **NumPy for numbers** - 50-100x faster than Python lists
6. **Cache expensive calls** - `@lru_cache` for repeated calculations
7. **Generators for large data** - Lazy evaluation, memory efficient

### Dependencies

1. **Use Poetry** - Modern, comprehensive tool
2. **Lock dependencies** - poetry.lock for reproducibility
3. **Separate environments** - One per project
4. **Group dependencies** - dev/test/prod separation
5. **Version constraints** - Use `^` for compatible versions
6. **Regular updates** - Keep dependencies current
7. **Document requirements** - README with setup instructions

---

## 🔗 Related Notes

- [[06. Python for Data Transformation|Chapter 6 - Data Transformation]]
- [[07. Testing Data Pipelines with Python|Chapter 7 - Testing]]
- [[10. Workflow Orchestration - Airflow, Prefect, Dagster|Chapter 10 - Orchestration]]
- [[README|Project Overview]]

---

## 📚 Key Takeaways

1. **Profile before optimizing** - Measure to find actual bottlenecks
2. **Async for I/O-bound** - Network and disk operations
3. **Multiprocessing for CPU-bound** - Heavy computations
4. **Use NumPy** - 50-100x faster for numerical operations
5. **Poetry is best** - Modern dependency management tool
6. **Lock files matter** - Ensure reproducible builds
7. **Separate environments** - One virtual environment per project
8. **Group dependencies** - dev/test/prod separation
9. **Don't premature optimize** - Readability first, optimize when needed
10. **Cache strategically** - `@lru_cache` for expensive repeated calls

---

**Last Updated:** 2025-10-16
**Status:** ✅ Complete
