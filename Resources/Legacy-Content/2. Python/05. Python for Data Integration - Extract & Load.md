---
title: Python for Data Integration - Extract & Load
date: 2025-10-16
tags: [python, data-integration, etl, apis, databases, cloud, data-engineering, integration]
status: active
learning_phase: "Integration (Extract/Load)"
source: https://de101.startdataengineering.com/py_el
---

**Source:** [DE101 Chapter 5 - Python for Data Integration](https://de101.startdataengineering.com/py_el)
**Created:** 2025-10-16
**Last Updated:** 2025-10-16
**Status:** ‚úÖ Complete

---

## Overview

**Python connects data pipelines** - It has libraries to read and write data from virtually any system. This makes Python the universal connector in modern data engineering, enabling integration across diverse technology stacks.

**Key Concept:** Python's extensive ecosystem provides tools to extract data from source systems and load it into target destinations, forming the "EL" in ELT pipelines.

---

## Six Categories of Data Systems

### System Category Overview

Python organizes data access through six main categories, each serving different integration needs:

| Category | Purpose | Common Tools | Use Case |
|----------|---------|--------------|----------|
| **Database Drivers** | Direct database connections | psycopg2, sqlite3, duckdb | Query and fetch data |
| **Cloud SDKs** | Cloud service integration | boto3 (AWS), gsutil (GCP) | Cloud storage operations |
| **APIs** | HTTP-based data access | requests, httpx | Third-party data sources |
| **File Formats** | File reading/writing | csv, json, parquet | Local/remote file processing |
| **SFTP/FTP** | Secure file transfer | paramiko, ftplib | External data distribution |
| **Queuing Systems** | Stream data handling | pykafka, kafka-python | Real-time data pipelines |

---

## 1. Database Drivers - Direct Database Access

### Core Concept

**Database drivers** establish connections to databases using credentials, enabling query execution and data retrieval programmatically.

### Common Database Drivers

```python
# PostgreSQL
import psycopg2

# SQLite (built-in)
import sqlite3

# DuckDB (analytical)
import duckdb

# MySQL
import mysql.connector

# SQL Server
import pyodbc
```

---

### PostgreSQL Example - psycopg2

```python
import psycopg2

# Establish connection
conn = psycopg2.connect(
    host="localhost",
    database="analytics",
    user="data_engineer",
    password="secure_password",
    port=5432
)

# Create cursor for executing queries
cursor = conn.cursor()

# Execute query
cursor.execute("""
    SELECT customer_id, name, total_spent
    FROM customers
    WHERE total_spent > 1000
    ORDER BY total_spent DESC
    LIMIT 10
""")

# Fetch results
results = cursor.fetchall()
for row in results:
    customer_id, name, total_spent = row
    print(f"{name} (ID: {customer_id}): ${total_spent:,.2f}")

# Clean up
cursor.close()
conn.close()
```

---

### SQLite Example - Built-in Database

```python
import sqlite3

# Connect to database (creates if doesn't exist)
conn = sqlite3.connect('customers.db')
cursor = conn.cursor()

# Create table
cursor.execute("""
    CREATE TABLE IF NOT EXISTS customers (
        customer_id INTEGER PRIMARY KEY,
        name TEXT NOT NULL,
        email TEXT UNIQUE,
        balance REAL DEFAULT 0.0
    )
""")

# Insert data
cursor.execute("""
    INSERT INTO customers (customer_id, name, email, balance)
    VALUES (?, ?, ?, ?)
""", (101, "Alice Johnson", "alice@example.com", 5420.50))

# Commit changes
conn.commit()

# Query data
cursor.execute("SELECT * FROM customers WHERE balance > 1000")
rows = cursor.fetchall()

# Use row_factory for dictionary-like access
conn.row_factory = sqlite3.Row
cursor = conn.cursor()
cursor.execute("SELECT * FROM customers")
for row in cursor:
    print(dict(row))

conn.close()
```

---

### DuckDB Example - Analytical Database

```python
import duckdb

# In-memory database
conn = duckdb.connect()

# Or file-based
# conn = duckdb.connect('analytics.duckdb')

# Direct query on CSV files
result = conn.execute("""
    SELECT
        customer_id,
        COUNT(*) as order_count,
        SUM(order_total) as total_spent
    FROM read_csv_auto('orders.csv')
    GROUP BY customer_id
    HAVING total_spent > 1000
    ORDER BY total_spent DESC
""").fetchall()

# Query Parquet files
result = conn.execute("""
    SELECT * FROM read_parquet('data/*.parquet')
    WHERE date >= '2024-01-01'
""").df()  # Returns pandas DataFrame

conn.close()
```

---

### Best Practices - Database Connections

```python
# ‚úÖ Use context managers (automatic cleanup)
import psycopg2

with psycopg2.connect(host="localhost", database="db") as conn:
    with conn.cursor() as cursor:
        cursor.execute("SELECT * FROM customers")
        results = cursor.fetchall()
# Connection automatically closed

# ‚úÖ Use parameterized queries (prevent SQL injection)
cursor.execute(
    "SELECT * FROM customers WHERE customer_id = %s",
    (customer_id,)
)

# ‚ùå NEVER do this (SQL injection vulnerability!)
cursor.execute(f"SELECT * FROM customers WHERE name = '{user_input}'")

# ‚úÖ Batch inserts for performance
data = [(1, "Alice"), (2, "Bob"), (3, "Carol")]
cursor.executemany(
    "INSERT INTO customers (id, name) VALUES (?, ?)",
    data
)

# ‚úÖ Use connection pooling for web applications
from psycopg2 import pool
connection_pool = pool.SimpleConnectionPool(1, 20, host="localhost", database="db")
```

---

## 2. Cloud SDKs - Cloud Service Integration

### Core Concept

**Cloud SDKs** (Software Development Kits) enable interaction with cloud services, primarily for storage operations in data pipelines.

---

### AWS SDK - boto3

```python
import boto3

# Initialize S3 client
s3 = boto3.client('s3',
    aws_access_key_id='YOUR_ACCESS_KEY',
    aws_secret_access_key='YOUR_SECRET_KEY',
    region_name='us-east-1'
)

# Upload file to S3
s3.upload_file(
    Filename='local_data.csv',
    Bucket='my-data-bucket',
    Key='raw/2024/01/data.csv'
)

# Download file from S3
s3.download_file(
    Bucket='my-data-bucket',
    Key='raw/2024/01/data.csv',
    Filename='downloaded_data.csv'
)

# List objects in bucket
response = s3.list_objects_v2(
    Bucket='my-data-bucket',
    Prefix='raw/2024/'
)
for obj in response.get('Contents', []):
    print(f"{obj['Key']} - {obj['Size']} bytes")

# Read CSV directly from S3 into pandas
import pandas as pd
obj = s3.get_object(Bucket='my-data-bucket', Key='data.csv')
df = pd.read_csv(obj['Body'])
```

---

### AWS S3 - Advanced Operations

```python
import boto3
from botocore.exceptions import ClientError

s3 = boto3.client('s3')

# Check if object exists
def object_exists(bucket, key):
    try:
        s3.head_object(Bucket=bucket, Key=key)
        return True
    except ClientError:
        return False

# Copy object within S3
s3.copy_object(
    CopySource={'Bucket': 'source-bucket', 'Key': 'source/file.csv'},
    Bucket='dest-bucket',
    Key='dest/file.csv'
)

# Delete object
s3.delete_object(Bucket='my-bucket', Key='old/file.csv')

# Generate presigned URL (temporary access)
url = s3.generate_presigned_url(
    'get_object',
    Params={'Bucket': 'my-bucket', 'Key': 'data.csv'},
    ExpiresIn=3600  # 1 hour
)

# Upload with metadata
s3.upload_file(
    'data.csv',
    'my-bucket',
    'data.csv',
    ExtraArgs={
        'Metadata': {
            'source': 'api',
            'upload_date': '2024-01-15'
        },
        'ServerSideEncryption': 'AES256'
    }
)
```

---

### GCP SDK - Google Cloud Storage

```python
from google.cloud import storage

# Initialize client (uses environment credentials)
client = storage.Client()

# Get bucket
bucket = client.get_bucket('my-gcs-bucket')

# Upload file
blob = bucket.blob('raw/data.csv')
blob.upload_from_filename('local_data.csv')

# Download file
blob = bucket.blob('raw/data.csv')
blob.download_to_filename('downloaded_data.csv')

# List blobs
blobs = bucket.list_blobs(prefix='raw/2024/')
for blob in blobs:
    print(f"{blob.name} - {blob.size} bytes")

# Read directly into memory
blob = bucket.blob('data.csv')
content = blob.download_as_string()

# Set metadata
blob.metadata = {'source': 'api', 'processed': 'false'}
blob.patch()
```

---

### Azure SDK - Azure Blob Storage

```python
from azure.storage.blob import BlobServiceClient

# Initialize client
connection_string = "DefaultEndpointsProtocol=https;..."
blob_service_client = BlobServiceClient.from_connection_string(connection_string)

# Get container client
container_client = blob_service_client.get_container_client('my-container')

# Upload file
with open('data.csv', 'rb') as data:
    blob_client = container_client.get_blob_client('raw/data.csv')
    blob_client.upload_blob(data, overwrite=True)

# Download file
blob_client = container_client.get_blob_client('raw/data.csv')
with open('downloaded.csv', 'wb') as download_file:
    download_file.write(blob_client.download_blob().readall())

# List blobs
blob_list = container_client.list_blobs(name_starts_with='raw/')
for blob in blob_list:
    print(f"{blob.name} - {blob.size} bytes")
```

---

## 3. APIs - HTTP-Based Data Access

### Core Concept

**APIs** (Application Programming Interfaces) expose data through HTTPS endpoints, returning information based on request parameters.

---

### Using requests Library

```python
import requests

# Basic GET request
url = "https://pokeapi.co/api/v2/pokemon/1"
response = requests.get(url)

# Check status
if response.status_code == 200:
    data = response.json()
    print(data['name'])  # "bulbasaur"
else:
    print(f"Error: {response.status_code}")

# GET with parameters
params = {
    'limit': 10,
    'offset': 0,
    'category': 'electronics'
}
response = requests.get('https://api.example.com/products', params=params)

# GET with headers (authentication)
headers = {
    'Authorization': 'Bearer YOUR_API_TOKEN',
    'Content-Type': 'application/json'
}
response = requests.get('https://api.example.com/data', headers=headers)

# POST request (submit data)
payload = {
    'customer_id': 101,
    'order_total': 299.99,
    'items': ['laptop', 'mouse']
}
response = requests.post(
    'https://api.example.com/orders',
    json=payload,
    headers=headers
)

# Handle response
if response.status_code == 201:
    order_id = response.json()['order_id']
    print(f"Order created: {order_id}")
```

---

### Error Handling and Retries

```python
import requests
from requests.adapters import HTTPAdapter
from requests.packages.urllib3.util.retry import Retry

# Configure retry strategy
retry_strategy = Retry(
    total=3,                    # Total retries
    backoff_factor=1,           # Wait 1, 2, 4 seconds
    status_forcelist=[429, 500, 502, 503, 504]
)

adapter = HTTPAdapter(max_retries=retry_strategy)
session = requests.Session()
session.mount("https://", adapter)

# Make request with retries
try:
    response = session.get('https://api.example.com/data', timeout=10)
    response.raise_for_status()  # Raises exception for 4xx/5xx
    data = response.json()
except requests.exceptions.RequestException as e:
    print(f"Request failed: {e}")

# Pagination handling
def fetch_all_pages(base_url, headers):
    all_data = []
    page = 1

    while True:
        response = requests.get(
            f"{base_url}?page={page}",
            headers=headers
        )
        data = response.json()

        if not data['results']:
            break

        all_data.extend(data['results'])
        page += 1

    return all_data
```

---

### Real-World API Example

```python
import requests
import time

class DataAPIClient:
    def __init__(self, api_key):
        self.base_url = "https://api.example.com/v1"
        self.headers = {
            'Authorization': f'Bearer {api_key}',
            'Content-Type': 'application/json'
        }

    def get_customers(self, limit=100):
        """Fetch customer data with pagination"""
        customers = []
        offset = 0

        while True:
            response = requests.get(
                f"{self.base_url}/customers",
                headers=self.headers,
                params={'limit': limit, 'offset': offset}
            )

            if response.status_code != 200:
                print(f"Error: {response.status_code}")
                break

            data = response.json()
            batch = data['customers']

            if not batch:
                break

            customers.extend(batch)
            offset += limit

            # Rate limiting - be nice to APIs!
            time.sleep(0.5)

        return customers

    def create_order(self, order_data):
        """Submit new order"""
        response = requests.post(
            f"{self.base_url}/orders",
            headers=self.headers,
            json=order_data
        )
        response.raise_for_status()
        return response.json()

# Usage
client = DataAPIClient('your_api_key_here')
customers = client.get_customers()
print(f"Fetched {len(customers)} customers")
```

---

## 4. File Formats - Reading and Writing Files

### Core Concept

Python provides libraries for reading and writing both standard and specialized file formats.

---

### CSV Files - Standard Library

```python
import csv

# Reading CSV
data_location = "./data/customer.csv"

# Method 1: List of lists
with open(data_location, 'r', newline='') as csvfile:
    csvreader = csv.reader(csvfile)
    header = next(csvreader)  # Skip header
    for row in csvreader:
        customer_id, name, email = row
        print(f"{name}: {email}")

# Method 2: Dictionary reader (better for named columns)
with open(data_location, 'r', newline='') as csvfile:
    csvreader = csv.DictReader(csvfile)
    for row in csvreader:
        print(f"{row['name']}: {row['email']}")

# Writing CSV
customers = [
    {'id': 101, 'name': 'Alice', 'balance': 5420.50},
    {'id': 102, 'name': 'Bob', 'balance': 3210.00}
]

with open('output.csv', 'w', newline='') as csvfile:
    fieldnames = ['id', 'name', 'balance']
    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)

    writer.writeheader()
    writer.writerows(customers)
```

---

### JSON Files - JavaScript Object Notation

```python
import json

# Reading JSON
with open('data.json', 'r') as f:
    data = json.load(f)

# Writing JSON
customers = [
    {'id': 101, 'name': 'Alice', 'balance': 5420.50},
    {'id': 102, 'name': 'Bob', 'balance': 3210.00}
]

with open('output.json', 'w') as f:
    json.dump(customers, f, indent=2)

# Pretty printing
print(json.dumps(customers, indent=2))

# Parsing JSON string
json_string = '{"name": "Alice", "age": 30}'
data = json.loads(json_string)

# Handle nested JSON
nested_data = {
    'customer': {
        'id': 101,
        'profile': {
            'name': 'Alice',
            'email': 'alice@example.com'
        },
        'orders': [
            {'order_id': 1, 'total': 99.99},
            {'order_id': 2, 'total': 149.99}
        ]
    }
}

with open('nested.json', 'w') as f:
    json.dump(nested_data, f, indent=2)
```

---

### Parquet Files - Columnar Storage

```python
import pandas as pd
import pyarrow.parquet as pq

# Write Parquet with pandas
df = pd.DataFrame({
    'customer_id': [101, 102, 103],
    'name': ['Alice', 'Bob', 'Carol'],
    'balance': [5420.50, 3210.00, 8750.25]
})

df.to_parquet('customers.parquet', compression='snappy')

# Read Parquet
df = pd.read_parquet('customers.parquet')

# Read with PyArrow (more control)
table = pq.read_table('customers.parquet')
df = table.to_pandas()

# Read specific columns
df = pd.read_parquet('customers.parquet', columns=['customer_id', 'balance'])

# Write with partitioning
df['date'] = pd.to_datetime(['2024-01-01', '2024-01-02', '2024-01-01'])
df.to_parquet(
    'partitioned/',
    partition_cols=['date'],
    compression='snappy'
)
```

---

### XML Files

```python
import xml.etree.ElementTree as ET

# Parse XML
tree = ET.parse('data.xml')
root = tree.getroot()

# Iterate elements
for customer in root.findall('customer'):
    customer_id = customer.get('id')
    name = customer.find('name').text
    email = customer.find('email').text
    print(f"{customer_id}: {name} - {email}")

# Create XML
root = ET.Element('customers')
customer = ET.SubElement(root, 'customer', id='101')
ET.SubElement(customer, 'name').text = 'Alice'
ET.SubElement(customer, 'email').text = 'alice@example.com'

tree = ET.ElementTree(root)
tree.write('output.xml', encoding='utf-8', xml_declaration=True)
```

---

### Excel Files

```python
import pandas as pd

# Read Excel
df = pd.read_excel('data.xlsx', sheet_name='Sheet1')

# Read multiple sheets
excel_file = pd.ExcelFile('data.xlsx')
sheet_names = excel_file.sheet_names

sheets = {}
for sheet in sheet_names:
    sheets[sheet] = pd.read_excel(excel_file, sheet_name=sheet)

# Write Excel
df.to_excel('output.xlsx', sheet_name='Customers', index=False)

# Write multiple sheets
with pd.ExcelWriter('output.xlsx', engine='xlsxwriter') as writer:
    df1.to_excel(writer, sheet_name='Customers', index=False)
    df2.to_excel(writer, sheet_name='Orders', index=False)
```

---

## 5. SFTP/FTP - Secure File Transfer

### Core Concept

SFTP/FTP protocols enable secure data access from external file servers where partners distribute data.

---

### SFTP with paramiko

```python
import paramiko

# Connect to SFTP server
transport = paramiko.Transport(('sftp.example.com', 22))
transport.connect(username='user', password='password')

sftp = paramiko.SFTPClient.from_transport(transport)

# List files
files = sftp.listdir('/remote/path')
for filename in files:
    print(filename)

# Download file
sftp.get('/remote/path/data.csv', 'local_data.csv')

# Upload file
sftp.put('local_data.csv', '/remote/path/uploaded.csv')

# Close connection
sftp.close()
transport.close()

# Using context manager (recommended)
with paramiko.SSHClient() as ssh:
    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
    ssh.connect('sftp.example.com', username='user', password='password')

    with ssh.open_sftp() as sftp:
        sftp.get('/remote/data.csv', 'local_data.csv')
```

---

### FTP with ftplib

```python
from ftplib import FTP

# Connect to FTP server
ftp = FTP('ftp.example.com')
ftp.login(user='username', passwd='password')

# List directory
files = ftp.nlst()
for filename in files:
    print(filename)

# Download file
with open('local_file.txt', 'wb') as f:
    ftp.retrbinary('RETR remote_file.txt', f.write)

# Upload file
with open('local_file.txt', 'rb') as f:
    ftp.storbinary('STOR remote_file.txt', f)

# Close connection
ftp.quit()
```

---

## 6. Queuing Systems - Stream Data

### Core Concept

Message queue platforms buffer and distribute data streams in real-time pipelines.

---

### Kafka with kafka-python

```python
from kafka import KafkaProducer, KafkaConsumer
import json

# Producer - Send messages
producer = KafkaProducer(
    bootstrap_servers=['localhost:9092'],
    value_serializer=lambda v: json.dumps(v).encode('utf-8')
)

# Send message
message = {
    'customer_id': 101,
    'event': 'purchase',
    'amount': 299.99,
    'timestamp': '2024-01-15T10:30:00'
}

producer.send('orders', value=message)
producer.flush()

# Consumer - Read messages
consumer = KafkaConsumer(
    'orders',
    bootstrap_servers=['localhost:9092'],
    auto_offset_reset='earliest',
    value_deserializer=lambda m: json.loads(m.decode('utf-8'))
)

for message in consumer:
    print(f"Received: {message.value}")

    # Process message
    customer_id = message.value['customer_id']
    amount = message.value['amount']

    # Your processing logic here
    print(f"Customer {customer_id} spent ${amount}")
```

---

## HTML Web Scraping - Bonus Category

### Beautiful Soup for HTML Parsing

```python
import requests
from bs4 import BeautifulSoup

# Fetch webpage
url = 'https://example.com'
response = requests.get(url)

# Parse HTML
soup = BeautifulSoup(response.content, 'html.parser')

# Find elements
title = soup.find('title').text
print(f"Page title: {title}")

# Find all links
links = soup.find_all('a')
for link in links:
    href = link.get('href')
    text = link.text
    print(f"{text}: {href}")

# Find by CSS class
products = soup.find_all('div', class_='product')
for product in products:
    name = product.find('h3').text
    price = product.find('span', class_='price').text
    print(f"{name}: {price}")

# Find by CSS selector
items = soup.select('.product .price')
for item in items:
    print(item.text)
```

---

## üéØ Best Practices

### Connection Management
1. **Use context managers** - Automatic resource cleanup
2. **Connection pooling** - Reuse connections in production
3. **Handle errors gracefully** - Network issues will happen
4. **Set timeouts** - Don't wait forever

### Security
1. **Never hardcode credentials** - Use environment variables
2. **Parameterized queries** - Prevent SQL injection
3. **HTTPS for APIs** - Encrypt data in transit
4. **Rotate API keys** - Regular security hygiene

### Performance
1. **Batch operations** - Reduce network round trips
2. **Pagination** - Don't fetch everything at once
3. **Compression** - Enable for large transfers
4. **Parallel processing** - Use threading/multiprocessing

### Error Handling
1. **Retry with backoff** - Handle transient failures
2. **Log failures** - Debug issues later
3. **Validate data** - Check before processing
4. **Monitor rate limits** - Respect API constraints

---

## üîó Related Notes

- [[04. Python Basics - Data Structures & Control Flow|Previous: Chapter 4 - Python Basics]]
- [[06. Python for Data Transformation|Next: Chapter 6 - Data Transformation]]
- [[README|Project Overview]]

---

## üìö Key Takeaways

1. **Python connects everything** - Libraries exist for virtually any system
2. **Six main categories** - Databases, Cloud, APIs, Files, SFTP, Queues
3. **requests for APIs** - Universal HTTP client
4. **boto3 for AWS** - Most common cloud SDK in data engineering
5. **Context managers** - Use `with` statements for automatic cleanup
6. **Security matters** - Never hardcode credentials, always use parameters
7. **Handle errors** - Network operations fail, plan for it

---

**Last Updated:** 2025-10-16
**Chapter Status:** ‚úÖ Complete
