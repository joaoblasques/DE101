---
title: Python for Data Transformation
date: 2025-10-16
tags: [python, data-transformation, pyspark, pandas, polars, sql, data-engineering, processing]
status: active
learning_phase: "Transformation (Processing)"
source: https://de101.startdataengineering.com/py_transform
---

**Source:** [DE101 Chapter 6 - Python for Data Transformation](https://de101.startdataengineering.com/py_transform)
**Created:** 2025-10-16
**Last Updated:** 2025-10-16
**Status:** âœ… Complete

---

## Overview

**Python tells data processing engines what to do** - Libraries like Spark, Trino, DuckDB, and Polars enable transformation of data at scale. Python provides the interface; the engine performs the computation.

**Key Concept:** Python doesn't always process data itself. Often, it instructs specialized engines (Spark, databases) to perform transformations, leveraging their optimized processing capabilities.

---

## Three Categories of Data Processing

### Processing Category Overview

| Category | Examples | Data Location | Use Case |
|----------|----------|---------------|----------|
| **Python Standard Library** | csv, json, gzip | In-memory (Python) | Small datasets, simple operations |
| **DataFrame Libraries** | pandas, polars, Spark | In-memory or distributed | Tabular data, SQL-like operations |
| **SQL via Python** | psycopg2, sqlite3, duckdb | Database system | Large datasets, complex queries |

---

## 1. Python Standard Library - Native Processing

### Core Concept

Built-in Python modules process data using native data structures (lists, dicts, sets). Best for small to medium datasets that fit in memory.

---

### Common Operations with Standard Library

```python
import csv
from collections import defaultdict

# Read CSV data
customers = []
with open('customers.csv', 'r') as f:
    reader = csv.DictReader(f)
    for row in reader:
        customers.append(row)

# 1. REMOVING DUPLICATES
seen_ids = set()
unique_customers = []

for customer in customers:
    customer_id = customer['customer_id']
    if customer_id not in seen_ids:
        seen_ids.add(customer_id)
        unique_customers.append(customer)

print(f"Original: {len(customers)}, Unique: {len(unique_customers)}")

# 2. HANDLING MISSING VALUES (NULL)
for customer in unique_customers:
    # Replace empty values with defaults
    if not customer['Age']:
        customer['Age'] = 0
    if not customer['Purchase_Amount']:
        customer['Purchase_Amount'] = 0.0

# 3. REMOVING OUTLIERS (Filter)
cleaned_customers = [
    customer for customer in unique_customers
    if int(customer['Age']) <= 100
    and float(customer['Purchase_Amount']) <= 1000
]

# 4. DATA TRANSFORMATION
for customer in cleaned_customers:
    # Convert gender to binary
    customer['Gender_Binary'] = 0 if customer['Gender'] == 'Female' else 1

    # Split name into first and last
    name_parts = customer['Customer_Name'].split(' ', 1)
    customer['First_Name'] = name_parts[0]
    customer['Last_Name'] = name_parts[1] if len(name_parts) > 1 else ''

# 5. AGGREGATION - Total purchases by gender
gender_totals = defaultdict(float)
for customer in cleaned_customers:
    gender = customer['Gender']
    amount = float(customer['Purchase_Amount'])
    gender_totals[gender] += amount

for gender, total in gender_totals.items():
    print(f"{gender}: ${total:,.2f}")

# 6. GROUPING - Age brackets
age_groups = defaultdict(list)
for customer in cleaned_customers:
    age = int(customer['Age'])

    if 18 <= age <= 30:
        bracket = '18-30'
    elif 31 <= age <= 40:
        bracket = '31-40'
    elif 41 <= age <= 50:
        bracket = '41-50'
    elif 51 <= age <= 60:
        bracket = '51-60'
    elif 61 <= age <= 70:
        bracket = '61-70'
    else:
        bracket = 'Other'

    age_groups[bracket].append(customer)

# Calculate average purchase by age group
for bracket, group_customers in age_groups.items():
    if group_customers:
        avg_purchase = sum(float(c['Purchase_Amount']) for c in group_customers) / len(group_customers)
        print(f"{bracket}: ${avg_purchase:.2f} (n={len(group_customers)})")
```

---

### Advanced Standard Library Patterns

```python
from itertools import groupby
from operator import itemgetter

# Sorting and grouping with itertools
customers = sorted(customers, key=itemgetter('Gender', 'Age'))

for gender, group in groupby(customers, key=itemgetter('Gender')):
    group_list = list(group)
    print(f"{gender}: {len(group_list)} customers")

# Filtering with filter()
high_value = list(filter(
    lambda c: float(c['Purchase_Amount']) > 500,
    customers
))

# Mapping with map()
customer_ids = list(map(itemgetter('customer_id'), customers))

# Reducing with functools.reduce
from functools import reduce
total_revenue = reduce(
    lambda acc, c: acc + float(c['Purchase_Amount']),
    customers,
    0.0
)
```

---

### When to Use Standard Library

**âœ… Use When:**
- Dataset fits comfortably in memory (< 1GB)
- Simple transformations
- No external dependencies needed
- Quick scripts and prototypes

**âŒ Avoid When:**
- Data is large (> 1GB)
- Need SQL-like operations (joins, window functions)
- Require high performance
- Team is more familiar with SQL/DataFrames

---

## 2. DataFrame Libraries - Tabular Processing

### Core Concept

DataFrame libraries provide SQL-equivalent operations on tabular data through Python APIs. Some process in-memory (pandas, polars), others distributed (Spark).

---

## 2.1 PySpark - Distributed DataFrame Processing

### Why PySpark?

**PySpark advantages:**
1. **Distributed processing** - Handles datasets larger than memory
2. **Lazy evaluation** - Optimizes query execution
3. **Familiar API** - SQL-like operations in Python
4. **Scalability** - Runs on clusters
5. **Fault tolerance** - Recovers from failures

---

### PySpark Basics

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when, split, avg, count, sum as spark_sum

# Create Spark session
spark = SparkSession.builder \
    .appName("CustomerAnalysis") \
    .getOrCreate()

# Read CSV into DataFrame
df = spark.read.csv(
    'customers.csv',
    header=True,
    inferSchema=True
)

# Show schema
df.printSchema()
# root
#  |-- customer_id: integer
#  |-- Customer_Name: string
#  |-- Age: integer
#  |-- Gender: string
#  |-- Purchase_Amount: double

# Display first 5 rows
df.show(5)

# Get row count
print(f"Total rows: {df.count()}")
```

---

### PySpark Data Cleaning

```python
# 1. REMOVE DUPLICATES
unique_df = df.dropDuplicates(['customer_id'])

# 2. HANDLE MISSING VALUES
# Replace nulls with defaults
cleaned_df = unique_df.fillna({
    'Age': 0,
    'Purchase_Amount': 0.0
})

# Or drop rows with any null
cleaned_df = unique_df.dropna()

# 3. FILTER OUTLIERS
filtered_df = cleaned_df.filter(
    (col('Age') <= 100) &
    (col('Purchase_Amount') <= 1000)
)

# Alternative: SQL-style filter
filtered_df = cleaned_df.filter(
    "Age <= 100 AND Purchase_Amount <= 1000"
)
```

---

### PySpark Transformations

```python
from pyspark.sql.functions import when, split, col

# 1. CONDITIONAL TRANSFORMATION (Gender to binary)
transformed_df = filtered_df.withColumn(
    'Gender_Binary',
    when(col('Gender') == 'Female', 0).otherwise(1)
)

# 2. STRING OPERATIONS (Split name)
transformed_df = transformed_df \
    .withColumn('First_Name', split(col('Customer_Name'), ' ').getItem(0)) \
    .withColumn('Last_Name', split(col('Customer_Name'), ' ').getItem(1))

# 3. MULTIPLE TRANSFORMATIONS (Chaining)
final_df = df \
    .dropDuplicates(['customer_id']) \
    .fillna({'Age': 0, 'Purchase_Amount': 0.0}) \
    .filter((col('Age') <= 100) & (col('Purchase_Amount') <= 1000)) \
    .withColumn('Gender_Binary', when(col('Gender') == 'Female', 0).otherwise(1)) \
    .withColumn('First_Name', split(col('Customer_Name'), ' ').getItem(0))

final_df.show()
```

---

### PySpark Aggregations

```python
from pyspark.sql.functions import sum, avg, count, min, max

# 1. SIMPLE AGGREGATION
gender_totals = final_df.groupBy('Gender') \
    .agg(sum('Purchase_Amount').alias('total_purchases')) \
    .orderBy('total_purchases', ascending=False)

gender_totals.show()
# +------+----------------+
# |Gender|total_purchases |
# +------+----------------+
# |Male  |  125430.50     |
# |Female|  98765.25      |
# +------+----------------+

# 2. MULTIPLE AGGREGATIONS
gender_stats = final_df.groupBy('Gender').agg(
    count('*').alias('customer_count'),
    avg('Purchase_Amount').alias('avg_purchase'),
    sum('Purchase_Amount').alias('total_purchases'),
    min('Purchase_Amount').alias('min_purchase'),
    max('Purchase_Amount').alias('max_purchase')
)

gender_stats.show()

# 3. AGE BRACKET GROUPING
from pyspark.sql.functions import when

age_bracket_df = final_df.withColumn(
    'Age_Bracket',
    when((col('Age') >= 18) & (col('Age') <= 30), '18-30')
    .when((col('Age') >= 31) & (col('Age') <= 40), '31-40')
    .when((col('Age') >= 41) & (col('Age') <= 50), '41-50')
    .when((col('Age') >= 51) & (col('Age') <= 60), '51-60')
    .when((col('Age') >= 61) & (col('Age') <= 70), '61-70')
    .otherwise('Other')
)

bracket_stats = age_bracket_df.groupBy('Age_Bracket').agg(
    count('*').alias('count'),
    avg('Purchase_Amount').alias('avg_purchase')
).orderBy('Age_Bracket')

bracket_stats.show()
```

---

### PySpark Window Functions

```python
from pyspark.sql.window import Window
from pyspark.sql.functions import row_number, rank, dense_rank, lag, lead

# Define window specification
window_spec = Window.partitionBy('Gender').orderBy(col('Purchase_Amount').desc())

# Ranking within gender
ranked_df = final_df.withColumn(
    'rank_in_gender',
    row_number().over(window_spec)
)

# Top 3 customers per gender
top_customers = ranked_df.filter(col('rank_in_gender') <= 3)
top_customers.select('Gender', 'Customer_Name', 'Purchase_Amount', 'rank_in_gender').show()

# Running total within partition
running_total_window = Window.partitionBy('Gender').orderBy('customer_id')
df_with_running_total = final_df.withColumn(
    'running_total',
    spark_sum('Purchase_Amount').over(running_total_window)
)

# Previous and next values
lag_lead_window = Window.partitionBy('Gender').orderBy('Purchase_Amount')
comparison_df = final_df \
    .withColumn('prev_purchase', lag('Purchase_Amount', 1).over(lag_lead_window)) \
    .withColumn('next_purchase', lead('Purchase_Amount', 1).over(lag_lead_window))
```

---

### PySpark Joins

```python
# Read orders data
orders_df = spark.read.csv('orders.csv', header=True, inferSchema=True)

# Inner join
customer_orders = final_df.join(
    orders_df,
    final_df.customer_id == orders_df.customer_id,
    'inner'
).select(
    final_df['customer_id'],
    final_df['Customer_Name'],
    orders_df['order_id'],
    orders_df['order_total']
)

# Left join
all_customers_orders = final_df.join(
    orders_df,
    'customer_id',  # Join on column with same name
    'left'
)

# Aggregating after join
customer_order_summary = final_df.join(orders_df, 'customer_id', 'left') \
    .groupBy('customer_id', 'Customer_Name') \
    .agg(
        count('order_id').alias('order_count'),
        spark_sum('order_total').alias('total_spent')
    )
```

---

### PySpark SQL Interface

```python
# Register DataFrame as temporary view
final_df.createOrReplaceTempView('customers')
orders_df.createOrReplaceTempView('orders')

# Write SQL queries
result = spark.sql("""
    SELECT
        c.Gender,
        COUNT(*) as customer_count,
        AVG(c.Purchase_Amount) as avg_purchase,
        SUM(o.order_total) as total_orders
    FROM customers c
    LEFT JOIN orders o ON c.customer_id = o.customer_id
    WHERE c.Age BETWEEN 18 AND 65
    GROUP BY c.Gender
    HAVING total_orders > 10000
    ORDER BY total_orders DESC
""")

result.show()

# Complex SQL with window functions
result = spark.sql("""
    SELECT
        customer_id,
        Customer_Name,
        Purchase_Amount,
        ROW_NUMBER() OVER (PARTITION BY Gender ORDER BY Purchase_Amount DESC) as rank,
        AVG(Purchase_Amount) OVER (PARTITION BY Gender) as gender_avg
    FROM customers
""")
```

---

### PySpark Writing Output

```python
# Write to CSV
final_df.write.csv(
    'output/customers',
    header=True,
    mode='overwrite'
)

# Write to Parquet (recommended for big data)
final_df.write.parquet(
    'output/customers.parquet',
    mode='overwrite',
    compression='snappy'
)

# Write with partitioning
final_df.write.partitionBy('Gender').parquet(
    'output/customers_partitioned',
    mode='overwrite'
)

# Write to database
final_df.write.jdbc(
    url='jdbc:postgresql://localhost:5432/analytics',
    table='customers',
    mode='overwrite',
    properties={
        'user': 'data_engineer',
        'password': 'password',
        'driver': 'org.postgresql.Driver'
    }
)
```

---

## 2.2 Pandas - In-Memory DataFrame Processing

### Pandas Basics

```python
import pandas as pd

# Read CSV
df = pd.read_csv('customers.csv')

# Basic info
print(df.info())
print(df.describe())
print(df.head())

# Select columns
names = df['Customer_Name']
subset = df[['customer_id', 'Customer_Name', 'Purchase_Amount']]

# Filter rows
high_value = df[df['Purchase_Amount'] > 500]
adults = df[(df['Age'] >= 18) & (df['Age'] <= 65)]

# Remove duplicates
unique_df = df.drop_duplicates(subset=['customer_id'])

# Handle missing values
df_filled = df.fillna({'Age': 0, 'Purchase_Amount': 0.0})
df_dropped = df.dropna()

# Add columns
df['Gender_Binary'] = df['Gender'].map({'Female': 0, 'Male': 1})
df['Age_Bracket'] = pd.cut(
    df['Age'],
    bins=[0, 30, 40, 50, 60, 70, 100],
    labels=['0-30', '31-40', '41-50', '51-60', '61-70', '71+']
)

# Aggregations
gender_totals = df.groupby('Gender')['Purchase_Amount'].sum()
gender_stats = df.groupby('Gender').agg({
    'customer_id': 'count',
    'Purchase_Amount': ['sum', 'mean', 'min', 'max']
})

# Joins
orders_df = pd.read_csv('orders.csv')
merged = df.merge(orders_df, on='customer_id', how='left')

# Write output
df.to_csv('output.csv', index=False)
df.to_parquet('output.parquet')
```

---

### Pandas vs PySpark Comparison

| Feature | Pandas | PySpark |
|---------|--------|---------|
| **Data Size** | In-memory (< 10GB) | Distributed (TB-PB) |
| **Speed (small data)** | Faster | Slower (overhead) |
| **Speed (big data)** | Cannot handle | Much faster |
| **API Complexity** | Simpler | More verbose |
| **Lazy Evaluation** | No | Yes |
| **Distributed** | No | Yes |
| **Use Case** | EDA, small datasets | Production ETL |

**Rule of Thumb:**
- Data fits in RAM â†’ Use Pandas
- Data > RAM or needs to scale â†’ Use PySpark

---

## 2.3 Polars - Fast DataFrame Library

### Polars Overview

**Polars advantages:**
- Faster than pandas (written in Rust)
- Lazy evaluation (like Spark)
- Better memory efficiency
- Intuitive API

```python
import polars as pl

# Read CSV
df = pl.read_csv('customers.csv')

# Lazy evaluation
lazy_df = pl.scan_csv('customers.csv')

# Transformations (lazy)
result = lazy_df \
    .filter(pl.col('Age') <= 100) \
    .filter(pl.col('Purchase_Amount') <= 1000) \
    .with_columns([
        pl.when(pl.col('Gender') == 'Female')
          .then(0)
          .otherwise(1)
          .alias('Gender_Binary')
    ]) \
    .groupby('Gender') \
    .agg([
        pl.count().alias('count'),
        pl.col('Purchase_Amount').sum().alias('total'),
        pl.col('Purchase_Amount').mean().alias('avg')
    ])

# Execute (collect triggers computation)
result_df = result.collect()
print(result_df)

# Write output
result_df.write_csv('output.csv')
result_df.write_parquet('output.parquet')
```

---

## 3. SQL via Python - Database Processing

### Core Concept

Use database drivers to execute SQL queries through Python, offloading computation to the database system. Python orchestrates; database processes.

---

### DuckDB - Analytical SQL Database

```python
import duckdb

# Connect to database
conn = duckdb.connect('analytics.duckdb')

# Create table from CSV
conn.execute("""
    CREATE TABLE customers AS
    SELECT * FROM read_csv_auto('customers.csv')
""")

# SQL transformations
result = conn.execute("""
    WITH cleaned_customers AS (
        SELECT
            customer_id,
            Customer_Name,
            COALESCE(Age, 0) as Age,
            Gender,
            COALESCE(Purchase_Amount, 0.0) as Purchase_Amount
        FROM customers
        WHERE customer_id IS NOT NULL
    ),
    deduplicated AS (
        SELECT * FROM cleaned_customers
        QUALIFY ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY Purchase_Amount DESC) = 1
    ),
    transformed AS (
        SELECT
            *,
            CASE WHEN Gender = 'Female' THEN 0 ELSE 1 END as Gender_Binary,
            CASE
                WHEN Age BETWEEN 18 AND 30 THEN '18-30'
                WHEN Age BETWEEN 31 AND 40 THEN '31-40'
                WHEN Age BETWEEN 41 AND 50 THEN '41-50'
                WHEN Age BETWEEN 51 AND 60 THEN '51-60'
                WHEN Age BETWEEN 61 AND 70 THEN '61-70'
                ELSE 'Other'
            END as Age_Bracket
        FROM deduplicated
        WHERE Age <= 100 AND Purchase_Amount <= 1000
    )
    SELECT
        Gender,
        Age_Bracket,
        COUNT(*) as customer_count,
        SUM(Purchase_Amount) as total_purchases,
        AVG(Purchase_Amount) as avg_purchase
    FROM transformed
    GROUP BY Gender, Age_Bracket
    ORDER BY Gender, Age_Bracket
""").fetchdf()  # Returns pandas DataFrame

print(result)

# Export to Parquet
conn.execute("""
    COPY (SELECT * FROM transformed)
    TO 'output/customers.parquet' (FORMAT PARQUET)
""")

conn.close()
```

---

### PostgreSQL via psycopg2

```python
import psycopg2
import pandas as pd

# Connect
conn = psycopg2.connect(
    host="localhost",
    database="analytics",
    user="data_engineer",
    password="password"
)

# Create table
cursor = conn.cursor()
cursor.execute("""
    CREATE TABLE IF NOT EXISTS customer_summary (
        customer_id INTEGER PRIMARY KEY,
        name TEXT,
        total_orders INTEGER,
        total_spent NUMERIC(10, 2),
        avg_order_value NUMERIC(10, 2)
    )
""")

# Complex transformation in SQL
cursor.execute("""
    INSERT INTO customer_summary
    SELECT
        c.customer_id,
        c.name,
        COUNT(o.order_id) as total_orders,
        SUM(o.order_total) as total_spent,
        AVG(o.order_total) as avg_order_value
    FROM customers c
    LEFT JOIN orders o ON c.customer_id = o.customer_id
    WHERE c.active = true
    GROUP BY c.customer_id, c.name
    HAVING COUNT(o.order_id) > 0
""")

conn.commit()

# Read results into pandas
df = pd.read_sql_query("""
    SELECT *
    FROM customer_summary
    WHERE total_spent > 1000
    ORDER BY total_spent DESC
""", conn)

cursor.close()
conn.close()
```

---

## Transformation Pattern Comparison

### Same Analysis - Three Approaches

**Task:** Calculate total purchases by gender for customers aged 18-65

#### Approach 1: Standard Library

```python
from collections import defaultdict
import csv

totals = defaultdict(float)
with open('customers.csv') as f:
    for row in csv.DictReader(f):
        age = int(row['Age'])
        if 18 <= age <= 65:
            gender = row['Gender']
            amount = float(row['Purchase_Amount'])
            totals[gender] += amount
```

#### Approach 2: PySpark

```python
from pyspark.sql.functions import sum

result = df \
    .filter((col('Age') >= 18) & (col('Age') <= 65)) \
    .groupBy('Gender') \
    .agg(sum('Purchase_Amount').alias('total')) \
    .collect()
```

#### Approach 3: SQL (DuckDB)

```python
result = conn.execute("""
    SELECT Gender, SUM(Purchase_Amount) as total
    FROM customers
    WHERE Age BETWEEN 18 AND 65
    GROUP BY Gender
""").fetchall()
```

---

## ðŸŽ¯ Best Practices

### Choosing the Right Tool

**Use Standard Library when:**
- Dataset < 100MB
- Simple operations
- No dependencies preferred
- Quick scripts

**Use Pandas when:**
- Dataset < 10GB (fits in RAM)
- Exploratory data analysis
- Complex transformations
- Team familiar with pandas

**Use PySpark when:**
- Dataset > 10GB
- Production ETL pipelines
- Need to scale horizontally
- Distributed processing required

**Use SQL via Python when:**
- Complex analytical queries
- Data already in database
- Leveraging database optimizations
- Team strong in SQL

### Performance Optimization

1. **Filter early** - Reduce data volume ASAP
2. **Avoid collect()** - Don't bring all data to driver (Spark)
3. **Partition wisely** - Balance partition count
4. **Cache intermediate results** - Reused DataFrames
5. **Broadcast small tables** - Optimize joins (Spark)
6. **Columnar formats** - Use Parquet, not CSV

### Code Organization

```python
# âœ… Good: Modular, testable
def clean_customer_data(df):
    return df \
        .dropDuplicates(['customer_id']) \
        .fillna({'Age': 0, 'Purchase_Amount': 0.0}) \
        .filter((col('Age') <= 100) & (col('Purchase_Amount') <= 1000))

def transform_customer_data(df):
    return df \
        .withColumn('Gender_Binary', when(col('Gender') == 'Female', 0).otherwise(1))

# Main pipeline
cleaned_df = clean_customer_data(raw_df)
transformed_df = transform_customer_data(cleaned_df)

# âŒ Bad: Monolithic, hard to test
result = raw_df.dropDuplicates(['customer_id']).fillna({'Age': 0}).filter(col('Age') <= 100).withColumn('Gender_Binary', when(col('Gender') == 'Female', 0).otherwise(1))  # Too long!
```

---

## ðŸ”— Related Notes

- [[05. Python for Data Integration - Extract & Load|Previous: Chapter 5 - Data Integration]]
- [[04. Python Basics - Data Structures & Control Flow|Chapter 4 - Python Basics]]
- [[README|Project Overview]]

---

## ðŸ“š Key Takeaways

1. **Three processing models** - Standard library, DataFrames, SQL via Python
2. **PySpark for big data** - Distributed processing, lazy evaluation, scalable
3. **Pandas for exploration** - Fast for small data, rich API, in-memory
4. **SQL via Python** - Leverage database engines, familiar syntax
5. **Choose based on data size** - Match tool to data volume
6. **Filter early** - Reduce data volume before processing
7. **DataFrame operations â‰ˆ SQL** - Similar concepts, different syntax
8. **Lazy evaluation wins** - PySpark and Polars optimize execution

---

**Last Updated:** 2025-10-16
**Chapter Status:** âœ… Complete
